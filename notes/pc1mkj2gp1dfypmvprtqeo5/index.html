<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Feature Engineering</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Feature Engineering"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/pc1mkj2gp1dfypmvprtqeo5/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="3/27/2023"/><meta property="article:modified_time" content="4/20/2023"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/pc1mkj2gp1dfypmvprtqeo5/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="feature-engineering">Feature Engineering<a aria-hidden="true" class="anchor-heading icon-link" href="#feature-engineering"></a></h1>
<p>Quick <a href="https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/A%20Short%20Guide%20for%20Feature%20Engineering%20and%20Feature%20Selection.md#451-recursive-feature-elimination">guide</a> for feature engineer and selection.</p>
<h1 id="feature-engineering-1">Feature engineering<a aria-hidden="true" class="anchor-heading icon-link" href="#feature-engineering-1"></a></h1>
<p><strong>TL;DR</strong></p>
<ul>
<li>transformation of current features (taking squares, log etc)</li>
<li>apply interactions (multplication and ratio of two other features). When you make recipies it does make sense to add the ratio of the amounts of two ingredients</li>
<li>group transforms (you get features that aggregate information across multiple rows grouped by some category, e.g. the average income of a person's state of residence)</li>
<li>building-up and breaking-down features (ID 123-45-6789, addresses '8241 Kaggle Ln., Goose City, NV')</li>
<li>K-means (use cluster as a categorical feature)</li>
<li>cluster distane feature</li>
<li>rescaling features (<code>preprocessing</code> module in scikit-learn )</li>
<li>PCA</li>
<li>target encoding for categorical features</li>
<li>one-hot encoding (ok if you do not have many different values in a single category)</li>
</ul>
<p><strong>Feature importance</strong></p>
<p>Rank features using a feature utility metric called "mutual information". The <strong>mutual information (MI)</strong> between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?</p>
<p>Mutual information = entropy (amount of information learned).</p>
<ul>
<li>MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.</li>
<li>It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.</li>
<li>The actual usefulness of a feature depends on the model you use it with. </li>
</ul>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> mutual_info_regression
</code></pre>
<p><strong>Interaction plots</strong> using the hue parameter in seaborn. These plots shows you interactions between pairwise features.</p>
<p><strong>General tips:</strong></p>
<ul>
<li>Linear models learn sums and differences naturally, but can't learn anything more complex.</li>
<li>Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.</li>
<li>Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.</li>
<li>Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.</li>
<li>Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.</li>
</ul>
<h2 id="k-means">K-means<a aria-hidden="true" class="anchor-heading icon-link" href="#k-means"></a></h2>
<p>The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a "divide and conquer" strategy.</p>
<p><strong>K means is sensitive to scale</strong>. Features with larger values will be weighted more heavily.</p>
<p><strong>Cluster distance feature!</strong>
The k-means algorithm offers an alternative way of creating features. Instead of labelling each feature with the nearest cluster centroid, it can measure the distance from a point to all the centroids and return those distances as features.</p>
<h2 id="pca">PCA<a aria-hidden="true" class="anchor-heading icon-link" href="#pca"></a></h2>
<p>Clustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data.</p>
<p>PCA is typically applied to standardized data.</p>
<p>The whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.</p>
<p>PCA makes this precise through each component's percent of explained variance.</p>
<p>PCA is a linear dimensionality reduction technique.</p>
<p>PCA use-cases:</p>
<ul>
<li>Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.</li>
<li>Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.</li>
<li>Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.</li>
<li>Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.</li>
</ul>
<p><strong>PCA tips:</strong></p>
<ul>
<li>PCA only works with numeric features, like continuous quantities or counts.</li>
<li>PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.</li>
<li>Consider removing or constraining outliers, since they can have an undue influence on the results.</li>
</ul>
<h2 id="categorical-features">Categorical features<a aria-hidden="true" class="anchor-heading icon-link" href="#categorical-features"></a></h2>
<p>We need to encode categorical features into numerical features.</p>
<ul>
<li>group aggregation strategy (use aggregate of the y values to encode the categorical features) e.g. mean encoding</li>
</ul>
<p>This strategy is also known as <strong>target encoding</strong>. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.</p>
<p>Problem with aggregate encoding:</p>
<ol>
<li>missing categories in the test set need to be imputed somehow</li>
<li>rare categories would have uncertain calculated aggregated statistics</li>
</ol>
<p><strong>Smoothing</strong> in encoding idea.</p>
<p>The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.</p>
<pre><code>encoding = weight * in_category + (1 - weight) * overall
</code></pre>
<pre><code>weight = n / (n + m)
</code></pre>
<p>Use <strong>m-estimate</strong> to calculate the weight.  The parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span> determines the "smoothing factor". Larger values of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span> put more weight on the overall estimate, the smoother it is.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> category_encoders <span class="token keyword">import</span> MEstimateEncoder <span class="token comment"># scikit-learn-contrib package</span>
</code></pre>
<p><strong>Use Cases for Target Encoding</strong></p>
<ul>
<li>High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.</li>
<li>Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.</li>
</ul>
<p><strong>Tip</strong>
When  using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#feature-engineering" title="Feature engineering">Feature engineering</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#k-means" title="K-means">K-means</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pca" title="PCA">PCA</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#categorical-features" title="Categorical features">Categorical features</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"pc1mkj2gp1dfypmvprtqeo5","title":"Feature Engineering","desc":"","updated":1681988682289,"created":1679935795985,"custom":{},"fname":"machine learning.Feature Engineering","type":"note","vault":{"fsPath":"vault"},"contentHash":"a3a51cdd7a815b5afe7186eb2742385f","links":[],"anchors":{"feature-engineering":{"type":"header","text":"Feature engineering","value":"feature-engineering","line":12,"column":0,"depth":1},"k-means":{"type":"header","text":"K-means","value":"k-means","line":52,"column":0,"depth":2},"pca":{"type":"header","text":"PCA","value":"pca","line":63,"column":0,"depth":2},"categorical-features":{"type":"header","text":"Categorical features","value":"categorical-features","line":87,"column":0,"depth":2}},"children":[],"parent":"2b5bwf46z6v132wu7xghvrp","data":{}},"body":"\u003ch1 id=\"feature-engineering\"\u003eFeature Engineering\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#feature-engineering\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eQuick \u003ca href=\"https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/A%20Short%20Guide%20for%20Feature%20Engineering%20and%20Feature%20Selection.md#451-recursive-feature-elimination\"\u003eguide\u003c/a\u003e for feature engineer and selection.\u003c/p\u003e\n\u003ch1 id=\"feature-engineering-1\"\u003eFeature engineering\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#feature-engineering-1\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etransformation of current features (taking squares, log etc)\u003c/li\u003e\n\u003cli\u003eapply interactions (multplication and ratio of two other features). When you make recipies it does make sense to add the ratio of the amounts of two ingredients\u003c/li\u003e\n\u003cli\u003egroup transforms (you get features that aggregate information across multiple rows grouped by some category, e.g. the average income of a person's state of residence)\u003c/li\u003e\n\u003cli\u003ebuilding-up and breaking-down features (ID 123-45-6789, addresses '8241 Kaggle Ln., Goose City, NV')\u003c/li\u003e\n\u003cli\u003eK-means (use cluster as a categorical feature)\u003c/li\u003e\n\u003cli\u003ecluster distane feature\u003c/li\u003e\n\u003cli\u003erescaling features (\u003ccode\u003epreprocessing\u003c/code\u003e module in scikit-learn )\u003c/li\u003e\n\u003cli\u003ePCA\u003c/li\u003e\n\u003cli\u003etarget encoding for categorical features\u003c/li\u003e\n\u003cli\u003eone-hot encoding (ok if you do not have many different values in a single category)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFeature importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRank features using a feature utility metric called \"mutual information\". The \u003cstrong\u003emutual information (MI)\u003c/strong\u003e between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?\u003c/p\u003e\n\u003cp\u003eMutual information = entropy (amount of information learned).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\u003c/li\u003e\n\u003cli\u003eIt's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\u003c/li\u003e\n\u003cli\u003eThe actual usefulness of a feature depends on the model you use it with. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e sklearn\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efeature_selection \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e mutual_info_regression\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eInteraction plots\u003c/strong\u003e using the hue parameter in seaborn. These plots shows you interactions between pairwise features.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGeneral tips:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinear models learn sums and differences naturally, but can't learn anything more complex.\u003c/li\u003e\n\u003cli\u003eRatios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\u003c/li\u003e\n\u003cli\u003eLinear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\u003c/li\u003e\n\u003cli\u003eTree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\u003c/li\u003e\n\u003cli\u003eCounts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"k-means\"\u003eK-means\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#k-means\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eK means is sensitive to scale\u003c/strong\u003e. Features with larger values will be weighted more heavily.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCluster distance feature!\u003c/strong\u003e\nThe k-means algorithm offers an alternative way of creating features. Instead of labelling each feature with the nearest cluster centroid, it can measure the distance from a point to all the centroids and return those distances as features.\u003c/p\u003e\n\u003ch2 id=\"pca\"\u003ePCA\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pca\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eClustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data.\u003c/p\u003e\n\u003cp\u003ePCA is typically applied to standardized data.\u003c/p\u003e\n\u003cp\u003eThe whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\u003c/p\u003e\n\u003cp\u003ePCA makes this precise through each component's percent of explained variance.\u003c/p\u003e\n\u003cp\u003ePCA is a linear dimensionality reduction technique.\u003c/p\u003e\n\u003cp\u003ePCA use-cases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\u003c/li\u003e\n\u003cli\u003eAnomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\u003c/li\u003e\n\u003cli\u003eNoise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\u003c/li\u003e\n\u003cli\u003eDecorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePCA tips:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePCA only works with numeric features, like continuous quantities or counts.\u003c/li\u003e\n\u003cli\u003ePCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\u003c/li\u003e\n\u003cli\u003eConsider removing or constraining outliers, since they can have an undue influence on the results.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"categorical-features\"\u003eCategorical features\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#categorical-features\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe need to encode categorical features into numerical features.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egroup aggregation strategy (use aggregate of the y values to encode the categorical features) e.g. mean encoding\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis strategy is also known as \u003cstrong\u003etarget encoding\u003c/strong\u003e. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\u003c/p\u003e\n\u003cp\u003eProblem with aggregate encoding:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003emissing categories in the test set need to be imputed somehow\u003c/li\u003e\n\u003cli\u003erare categories would have uncertain calculated aggregated statistics\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eSmoothing\u003c/strong\u003e in encoding idea.\u003c/p\u003e\n\u003cp\u003eThe idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eencoding = weight * in_category + (1 - weight) * overall\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eweight = n / (n + m)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse \u003cstrong\u003em-estimate\u003c/strong\u003e to calculate the weight.  The parameter \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e determines the \"smoothing factor\". Larger values of \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e put more weight on the overall estimate, the smoother it is.\u003c/p\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e category_encoders \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e MEstimateEncoder \u003cspan class=\"token comment\"\u003e# scikit-learn-contrib package\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eUse Cases for Target Encoding\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHigh-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\u003c/li\u003e\n\u003cli\u003eDomain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e\nWhen  using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"pc1mkj2gp1dfypmvprtqeo5"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>