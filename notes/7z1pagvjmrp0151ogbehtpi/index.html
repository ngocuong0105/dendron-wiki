<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>MIT - Introduction to Deep Learning</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="MIT - Introduction to Deep Learning"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/7z1pagvjmrp0151ogbehtpi/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="4/14/2025"/><meta property="article:modified_time" content="4/15/2025"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/7z1pagvjmrp0151ogbehtpi/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-28855ac016325484.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="mit---introduction-to-deep-learning">MIT - Introduction to Deep Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#mit---introduction-to-deep-learning"></a></h1>
<h1 id="6s191--mit-introduction-to-deep-learning">6.S191 | MIT Introduction to Deep Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#6s191--mit-introduction-to-deep-learning"></a></h1>
<ul>
<li><a href="https://introtodeeplearning.com/">Official Website</a></li>
<li><a href="https://github.com/MITDeepLearning/introtodeeplearning/tree/master">GitHub Repo</a></li>
</ul>
<h1 id="key-words">Key words<a aria-hidden="true" class="anchor-heading icon-link" href="#key-words"></a></h1>
<p>perceptron,feed forward neural network, activation function, weights and bias, sigmoid, relu, hyperbolic, gradient descent, stochastic gradient descent, backpropagation, chain rule, learning rates, adam, adaptive learning rates, overshoot or stuck at local minimum, overfitting, regularization, dropout, early stopping, sequence modeling, rnn, hidden states in rnn?, backpropagation through time, embedding = vectorization, exploding/vanishing gradient, transformer, attention is all you need, </p>
<h1 id="lecture-1-introduction">Lecture 1: Introduction<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-1-introduction"></a></h1>
<p>Deep Fake vide of Obama. MIT created 2 minutes deep video of OObama saying Welcome to MIT class. In 2020 ot costed 15K $</p>
<p>Intelligence - the ability process information in order to inform some future decision or action</p>
<p>Artificial Intelligence - make computers be able to learn to apply this process.</p>
<p>Machine Learning is a subset of Artificial Intelligence.</p>
<p>Make computers learn and execute tasks from the given data.</p>
<h2 id="why-deep-learning">Why Deep Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#why-deep-learning"></a></h2>
<p>Classical ML works by defining features. For example in image detection we would start defining lines, edges, curves ,eyes, noses, face. We need to define the features from low level to high level. We can't detect faces directly. We built composite features. <strong>Feature Engineering</strong>.</p>
<p>DL automates the process of <strong>feature engineering</strong>. DL has been around for a long time (decades). Why it became popular now?</p>
<ul>
<li>More data</li>
<li>Compute power</li>
<li>Libraries like tensorflow, pytorch</li>
</ul>
<h2 id="the-perceptron-forward-propagation">The perceptron: Forward propagation<a aria-hidden="true" class="anchor-heading icon-link" href="#the-perceptron-forward-propagation"></a></h2>
<ul>
<li>single neuron</li>
<li>input vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span></li>
<li>Linear sum using weights and a bias term</li>
<li>non-linear activation function: sigmoid (good for probabilities), ReLu (piecewise linear, non linear at 0), hyperbolic function</li>
</ul>
<p>The point of the activation function is to introduce a non-linearity because real data in real world is heavily non-linear.</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><msup><mi>x</mi><mi>T</mi></msup><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = g(w_{0} + x^{T}w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span> </p>
<p><img src="/dendron-wiki/./assets/images/perceptron_simplified.png" alt="alt text"></p>
<p><strong>dot product, add bias, apply non-linearity</strong></p>
<h2 id="layer">Layer<a aria-hidden="true" class="anchor-heading icon-link" href="#layer"></a></h2>
<p><img src="/dendron-wiki/./assets/images/one_hidden_layer.png" alt="alt text"></p>
<h2 id="deep-network">Deep network<a aria-hidden="true" class="anchor-heading icon-link" href="#deep-network"></a></h2>
<p>Has many hidden layers</p>
<h2 id="loss">Loss<a aria-hidden="true" class="anchor-heading icon-link" href="#loss"></a></h2>
<p>Empirical Loss</p>
<p>Loss function = Cost function = Objective function</p>
<p>Cross entropy loss, difference between probabilities. For Binary predictions.</p>
<p>Mean Squared Errors, difference between us functions. For real number predictions.</p>
<p>Our goal is to find a network that minimizes the loss on the given dataset.</p>
<p>Goal is to find all weights.</p>
<h2 id="loss-optimization">Loss optimization<a aria-hidden="true" class="anchor-heading icon-link" href="#loss-optimization"></a></h2>
<p><img src="/dendron-wiki/./assets/images/loss_optimization.png" alt="alt text"></p>
<h2 id="gradient-descent">Gradient Descent<a aria-hidden="true" class="anchor-heading icon-link" href="#gradient-descent"></a></h2>
<p>Randomly initialize our weights. Randomly pick a point in our landscape (loss function). Compute the gradient and take the opposite direction of the gradient. Note this is <strong>local optimization</strong>. We go with a small step opposite to the gradient direction. Choosing learning rate = step size.</p>
<p><img src="/dendron-wiki/./assets/images/gradient_descent_2.png" alt="alt text"></p>
<p>How do we compute the gradient? the process of computing the gradient is called <strong>backpropagation</strong>.</p>
<p>Derivatives, chain rule</p>
<p>Neural networks are extremely complex functions with complex loss landscapes.</p>
<h2 id="learning-rate">Learning rate<a aria-hidden="true" class="anchor-heading icon-link" href="#learning-rate"></a></h2>
<p>You don't want to set it too small, because you will be stuck in local minimum.</p>
<p>You don't want it to be too large as you will overshoot and diverge.</p>
<h3 id="adaptive-learning-rates">Adaptive Learning Rates<a aria-hidden="true" class="anchor-heading icon-link" href="#adaptive-learning-rates"></a></h3>
<p>Change the learning rate depending on the landscape</p>
<ul>
<li>how fast you are learning</li>
<li>how steep</li>
</ul>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent<a aria-hidden="true" class="anchor-heading icon-link" href="#stochastic-gradient-descent"></a></h2>
<p>GD computes the gradient over the entire dataset which can be computationally expensive.</p>
<p>SGD chooses a subset of the data to estimate the gradient</p>
<p>Mini batch Gradient Descent (choose a batch of B data points) to calculate the gradient</p>
<p>Larger batches means you can trust your gradient more and you can use larger learning rate.</p>
<p>If you use 32 data points you can parallelize gradient computation over 32 processors.</p>
<h2 id="overfitting">Overfitting<a aria-hidden="true" class="anchor-heading icon-link" href="#overfitting"></a></h2>
<h2 id="regularization">Regularization<a aria-hidden="true" class="anchor-heading icon-link" href="#regularization"></a></h2>
<p>To avoid overfitting.</p>
<h3 id="dropout">Dropout<a aria-hidden="true" class="anchor-heading icon-link" href="#dropout"></a></h3>
<p>sets some activation neurons to 0. Forces the network to learn a different pathway. Very power technique as it makes the model that does not rely too much on a fixed set of weights.</p>
<p>Dropout nodes would not have any update, no gradient to compute.</p>
<h3 id="early-stopping">Early stopping<a aria-hidden="true" class="anchor-heading icon-link" href="#early-stopping"></a></h3>
<p>Stop the training model early.</p>
<p>Training loss always go down.</p>
<p><img src="/dendron-wiki/./asstes/images/early_stopping.png" alt="alt text"></p>
<p><strong>In practice you can start plotting this curve and decide when to early stop!</strong></p>
<p>Ideal Difference between train and test dataset is to be 0. Then you will not know when to stop. This usually happens in Large Language Models. The dataset is so big that the model itself finds it hard to memorize. So the difference between train and test will be almost always 0.</p>
<p><strong>Language models usually does not the classical overfitting problems.</strong></p>
<h1 id="lecture-2-recurrent-neural-networks-transformers-and-attention">Lecture 2: Recurrent Neural Networks, Transformers, and Attention<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-2-recurrent-neural-networks-transformers-and-attention"></a></h1>
<h2 id="deep-sequence-modeling">Deep Sequence Modeling<a aria-hidden="true" class="anchor-heading icon-link" href="#deep-sequence-modeling"></a></h2>
<p>How do we model time series or sequential data in Neural Networks?</p>
<p><strong>Naive</strong>. We can have our standard feed-forward neural network and put input output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_0,y_0),(x_1,y_2)...(x_t,y_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">...</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p><img src="/dendron-wiki/./assets/images/naive_sequence_modeling.png" alt="alt text"></p>
<p>This approach does not take the sequence info, taking past history into the future.</p>
<h2 id="rnn---recurrent-neural-networks">RNN - Recurrent Neural Networks<a aria-hidden="true" class="anchor-heading icon-link" href="#rnn---recurrent-neural-networks"></a></h2>
<p>We will pass sequentially <strong>hidden state</strong> of the network. Hidden state is a vector that is passed.</p>
<p><img src="/dendron-wiki/./assets/images/rnn_from_scratch.png" alt="alt text"></p>
<p><img src="/dendron-wiki/./assets/images/rnn_computational_graph.png" alt="alt text"></p>
<h2 id="sequence-modeling-requirements">Sequence Modeling Requirements<a aria-hidden="true" class="anchor-heading icon-link" href="#sequence-modeling-requirements"></a></h2>
<ul>
<li>Handle <strong>variable-length</strong> sequences. Sentences can be short or long. It is not fixed like image where it had high vs width pixels length</li>
<li>Handle <strong>long-term</strong> dependency. You can short or long term dependency. Something at the very beginning might dictate the end of the sequence</li>
<li>Maintain information about <strong>order</strong></li>
<li><strong>Share parameters</strong> across sequence</li>
</ul>
<p>RNNs can handle the above requirements - though it needs a few upgrades. RNN are in the core of Sequence Modeling</p>
<p>These requirements show why Sequence modeling is rich and complex.</p>
<h2 id="example-predict-the-next-word">Example. Predict the next word.<a aria-hidden="true" class="anchor-heading icon-link" href="#example-predict-the-next-word"></a></h2>
<ol>
<li>We need to represent language to a neural network. We need to vectorize the words as numbers.</li>
</ol>
<ul>
<li>Encode language to neural network</li>
<li>Embedding: transforming words to vectors</li>
<li>Vocabulary -> Indexing -> Embedding (one-hot-embedding)</li>
</ul>
<p>EMBEDDING = VECTORIZATION of words</p>
<p><img src="/dendron-wiki/./assets/images/embedding.png" alt="alt text"></p>
<p>learned embedding = lower dimensional representation of language</p>
<h2 id="backpropagation-through-time">Backpropagation through time.<a aria-hidden="true" class="anchor-heading icon-link" href="#backpropagation-through-time"></a></h2>
<p>Carry partial derivatives from errors late in the sequence to the very beginning. You will have to multiply matrices multiple times and multiply derivatives many times.</p>
<p><img src="/dendron-wiki/./assets/images/back_through_time.png" alt="alt text"></p>
<ul>
<li>if there are many values > 1 you might have <strong>exploding gradient</strong></li>
<li>if there are many values &#x3C; 1 you might have <strong>vanishing gradient</strong></li>
</ul>
<p>This problem exists in very deep feed forward neural networks. Having many layers mean you will multiply the partial derivatives many time.</p>
<h3 id="unstable-gradients-makes-it-hard-to-lear-long-term-dependencies">Unstable gradients makes it hard to lear long term dependencies.<a aria-hidden="true" class="anchor-heading icon-link" href="#unstable-gradients-makes-it-hard-to-lear-long-term-dependencies"></a></h3>
<p>We cannot pass information from late time steps into initial time steps to promote/update the weights.</p>
<p>Current research upgrades RNN-s to be able to tackle this.</p>
<h2 id="lstm-gru">LSTM, GRU<a aria-hidden="true" class="anchor-heading icon-link" href="#lstm-gru"></a></h2>
<p>Ides: use <strong>gates</strong> to selectively add and remove information passed through hidden states. LSTM (long-short term memory) network uses gate cells to pass information throughout many time steps</p>
<h2 id="limitations-of-rnns">Limitations of RNNs<a aria-hidden="true" class="anchor-heading icon-link" href="#limitations-of-rnns"></a></h2>
<ul>
<li>slow, no parallelization - need to compute derivatives sequentially</li>
<li>rnn state (hidden state <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>) is of fixed size. Encoding bottleneck - we have cap on the information we can keep.</li>
<li>not long term memory</li>
</ul>
<p>time-step by time-step processing.. brings this bottleneck</p>
<p>Can we eliminate the need for recurrence? Squash all inputs into one vector and put it into one network. The naive approach does not work because it does not carry any time information. We've destroyed any notion of order.</p>
<p><strong>Idea:</strong> Can we define a way to identify the import parts of a sequence and model out the </p>
<p>Paper: <a href="https://arxiv.org/pdf/1706.03762">Attention is all you need</a></p>
<p>This landmark paper defines what is a transformer.</p>
<p><strong>GPT:</strong> The T stands for transformer.</p>
<p>Attention: when we look at an image we do not go pixel by pixel and look which are the parts that we attend to.</p>
<p>Finding the most important pixels is just a simple search.s</p>
<h3 id="transformers-core-idea">Transformers core idea<a aria-hidden="true" class="anchor-heading icon-link" href="#transformers-core-idea"></a></h3>
<p>Goal: Search and attend to the most important features in an input.</p>
<ol>
<li>Encode input (positional encoding) - didn't go into many details into that</li>
<li>Compute query, key, value. Use three weight matrices to get each of those. {key:value} are features</li>
<li>Compute similarity between query and all keys (dot product divided by scaling)</li>
<li>Compute <strong>attention weighting</strong></li>
<li>Extract values from important features</li>
</ol>
<p>Attention is the building block of transformers.</p>
<p><img src="/dendron-wiki/./assets/images/transformers.png" alt="alt text"></p>
<p>Self-attention to handle sequence modeling</p>
<p>Self-attention is the basis of many larne language models.</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#6s191--mit-introduction-to-deep-learning" title="6.S191 | MIT Introduction to Deep Learning">6.S191 | MIT Introduction to Deep Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#key-words" title="Key words">Key words</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-1-introduction" title="Lecture 1: Introduction">Lecture 1: Introduction</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#why-deep-learning" title="Why Deep Learning">Why Deep Learning</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#the-perceptron-forward-propagation" title="The perceptron: Forward propagation">The perceptron: Forward propagation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#layer" title="Layer">Layer</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#deep-network" title="Deep network">Deep network</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#loss" title="Loss">Loss</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#loss-optimization" title="Loss optimization">Loss optimization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#gradient-descent" title="Gradient Descent">Gradient Descent</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#learning-rate" title="Learning rate">Learning rate</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#adaptive-learning-rates" title="Adaptive Learning Rates">Adaptive Learning Rates</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#stochastic-gradient-descent" title="Stochastic Gradient Descent">Stochastic Gradient Descent</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#overfitting" title="Overfitting">Overfitting</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#regularization" title="Regularization">Regularization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dropout" title="Dropout">Dropout</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#early-stopping" title="Early stopping">Early stopping</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-2-recurrent-neural-networks-transformers-and-attention" title="Lecture 2: Recurrent Neural Networks, Transformers, and Attention">Lecture 2: Recurrent Neural Networks, Transformers, and Attention</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#deep-sequence-modeling" title="Deep Sequence Modeling">Deep Sequence Modeling</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#rnn---recurrent-neural-networks" title="RNN - Recurrent Neural Networks">RNN - Recurrent Neural Networks</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#sequence-modeling-requirements" title="Sequence Modeling Requirements">Sequence Modeling Requirements</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#example-predict-the-next-word" title="Example. Predict the next word.">Example. Predict the next word.</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#backpropagation-through-time" title="Backpropagation through time.">Backpropagation through time.</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#unstable-gradients-makes-it-hard-to-lear-long-term-dependencies" title="Unstable gradients makes it hard to lear long term dependencies.">Unstable gradients makes it hard to lear long term dependencies.</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lstm-gru" title="LSTM, GRU">LSTM, GRU</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#limitations-of-rnns" title="Limitations of RNNs">Limitations of RNNs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#transformers-core-idea" title="Transformers core idea">Transformers core idea</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"7z1pagvjmrp0151ogbehtpi","title":"MIT - Introduction to Deep Learning","desc":"","updated":1744741205507,"created":1744659193293,"custom":{},"fname":"deep learning.MIT - Introduction to Deep Learning","type":"note","vault":{"fsPath":"vault"},"contentHash":"08f83d769f758a932bbf5889c3c7594c","links":[],"anchors":{"6s191--mit-introduction-to-deep-learning":{"type":"header","text":"6.S191 | MIT Introduction to Deep Learning","value":"6s191--mit-introduction-to-deep-learning","line":7,"column":0,"depth":1},"key-words":{"type":"header","text":"Key words","value":"key-words","line":12,"column":0,"depth":1},"lecture-1-introduction":{"type":"header","text":"Lecture 1: Introduction","value":"lecture-1-introduction","line":15,"column":0,"depth":1},"why-deep-learning":{"type":"header","text":"Why Deep Learning","value":"why-deep-learning","line":28,"column":0,"depth":2},"the-perceptron-forward-propagation":{"type":"header","text":"The perceptron: Forward propagation","value":"the-perceptron-forward-propagation","line":37,"column":0,"depth":2},"layer":{"type":"header","text":"Layer","value":"layer","line":54,"column":0,"depth":2},"deep-network":{"type":"header","text":"Deep network","value":"deep-network","line":59,"column":0,"depth":2},"loss":{"type":"header","text":"Loss","value":"loss","line":63,"column":0,"depth":2},"loss-optimization":{"type":"header","text":"Loss optimization","value":"loss-optimization","line":80,"column":0,"depth":2},"gradient-descent":{"type":"header","text":"Gradient Descent","value":"gradient-descent","line":84,"column":0,"depth":2},"learning-rate":{"type":"header","text":"Learning rate","value":"learning-rate","line":98,"column":0,"depth":2},"adaptive-learning-rates":{"type":"header","text":"Adaptive Learning Rates","value":"adaptive-learning-rates","line":105,"column":0,"depth":3},"stochastic-gradient-descent":{"type":"header","text":"Stochastic Gradient Descent","value":"stochastic-gradient-descent","line":112,"column":0,"depth":2},"overfitting":{"type":"header","text":"Overfitting","value":"overfitting","line":127,"column":0,"depth":2},"regularization":{"type":"header","text":"Regularization","value":"regularization","line":130,"column":0,"depth":2},"dropout":{"type":"header","text":"Dropout","value":"dropout","line":134,"column":0,"depth":3},"early-stopping":{"type":"header","text":"Early stopping","value":"early-stopping","line":141,"column":0,"depth":3},"lecture-2-recurrent-neural-networks-transformers-and-attention":{"type":"header","text":"Lecture 2: Recurrent Neural Networks, Transformers, and Attention","value":"lecture-2-recurrent-neural-networks-transformers-and-attention","line":157,"column":0,"depth":1},"deep-sequence-modeling":{"type":"header","text":"Deep Sequence Modeling","value":"deep-sequence-modeling","line":160,"column":0,"depth":2},"rnn---recurrent-neural-networks":{"type":"header","text":"RNN - Recurrent Neural Networks","value":"rnn---recurrent-neural-networks","line":171,"column":0,"depth":2},"sequence-modeling-requirements":{"type":"header","text":"Sequence Modeling Requirements","value":"sequence-modeling-requirements","line":181,"column":0,"depth":2},"example-predict-the-next-word":{"type":"header","text":"Example. Predict the next word.","value":"example-predict-the-next-word","line":192,"column":0,"depth":2},"backpropagation-through-time":{"type":"header","text":"Backpropagation through time.","value":"backpropagation-through-time","line":206,"column":0,"depth":2},"unstable-gradients-makes-it-hard-to-lear-long-term-dependencies":{"type":"header","text":"Unstable gradients makes it hard to lear long term dependencies.","value":"unstable-gradients-makes-it-hard-to-lear-long-term-dependencies","line":222,"column":0,"depth":3},"lstm-gru":{"type":"header","text":"LSTM, GRU","value":"lstm-gru","line":229,"column":0,"depth":2},"limitations-of-rnns":{"type":"header","text":"Limitations of RNNs","value":"limitations-of-rnns","line":234,"column":0,"depth":2},"transformers-core-idea":{"type":"header","text":"Transformers core idea","value":"transformers-core-idea","line":259,"column":0,"depth":3}},"children":[],"parent":"ro9bbyftsutm88mxw6r16p5","data":{}},"body":"\u003ch1 id=\"mit---introduction-to-deep-learning\"\u003eMIT - Introduction to Deep Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#mit---introduction-to-deep-learning\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"6s191--mit-introduction-to-deep-learning\"\u003e6.S191 | MIT Introduction to Deep Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#6s191--mit-introduction-to-deep-learning\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://introtodeeplearning.com/\"\u003eOfficial Website\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/MITDeepLearning/introtodeeplearning/tree/master\"\u003eGitHub Repo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"key-words\"\u003eKey words\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#key-words\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eperceptron,feed forward neural network, activation function, weights and bias, sigmoid, relu, hyperbolic, gradient descent, stochastic gradient descent, backpropagation, chain rule, learning rates, adam, adaptive learning rates, overshoot or stuck at local minimum, overfitting, regularization, dropout, early stopping, sequence modeling, rnn, hidden states in rnn?, backpropagation through time, embedding = vectorization, exploding/vanishing gradient, transformer, attention is all you need, \u003c/p\u003e\n\u003ch1 id=\"lecture-1-introduction\"\u003eLecture 1: Introduction\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-1-introduction\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eDeep Fake vide of Obama. MIT created 2 minutes deep video of OObama saying Welcome to MIT class. In 2020 ot costed 15K $\u003c/p\u003e\n\u003cp\u003eIntelligence - the ability process information in order to inform some future decision or action\u003c/p\u003e\n\u003cp\u003eArtificial Intelligence - make computers be able to learn to apply this process.\u003c/p\u003e\n\u003cp\u003eMachine Learning is a subset of Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eMake computers learn and execute tasks from the given data.\u003c/p\u003e\n\u003ch2 id=\"why-deep-learning\"\u003eWhy Deep Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#why-deep-learning\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eClassical ML works by defining features. For example in image detection we would start defining lines, edges, curves ,eyes, noses, face. We need to define the features from low level to high level. We can't detect faces directly. We built composite features. \u003cstrong\u003eFeature Engineering\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eDL automates the process of \u003cstrong\u003efeature engineering\u003c/strong\u003e. DL has been around for a long time (decades). Why it became popular now?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMore data\u003c/li\u003e\n\u003cli\u003eCompute power\u003c/li\u003e\n\u003cli\u003eLibraries like tensorflow, pytorch\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"the-perceptron-forward-propagation\"\u003eThe perceptron: Forward propagation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#the-perceptron-forward-propagation\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003esingle neuron\u003c/li\u003e\n\u003cli\u003einput vector \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eLinear sum using weights and a bias term\u003c/li\u003e\n\u003cli\u003enon-linear activation function: sigmoid (good for probabilities), ReLu (piecewise linear, non linear at 0), hyperbolic function\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe point of the activation function is to introduce a non-linearity because real data in real world is heavily non-linear.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y} = g(w_{0} + x^{T}w)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6944em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.1944em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1944em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eg\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e \u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/perceptron_simplified.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003edot product, add bias, apply non-linearity\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"layer\"\u003eLayer\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#layer\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/one_hidden_layer.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch2 id=\"deep-network\"\u003eDeep network\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#deep-network\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eHas many hidden layers\u003c/p\u003e\n\u003ch2 id=\"loss\"\u003eLoss\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#loss\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eEmpirical Loss\u003c/p\u003e\n\u003cp\u003eLoss function = Cost function = Objective function\u003c/p\u003e\n\u003cp\u003eCross entropy loss, difference between probabilities. For Binary predictions.\u003c/p\u003e\n\u003cp\u003eMean Squared Errors, difference between us functions. For real number predictions.\u003c/p\u003e\n\u003cp\u003eOur goal is to find a network that minimizes the loss on the given dataset.\u003c/p\u003e\n\u003cp\u003eGoal is to find all weights.\u003c/p\u003e\n\u003ch2 id=\"loss-optimization\"\u003eLoss optimization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#loss-optimization\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/loss_optimization.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch2 id=\"gradient-descent\"\u003eGradient Descent\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gradient-descent\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eRandomly initialize our weights. Randomly pick a point in our landscape (loss function). Compute the gradient and take the opposite direction of the gradient. Note this is \u003cstrong\u003elocal optimization\u003c/strong\u003e. We go with a small step opposite to the gradient direction. Choosing learning rate = step size.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/gradient_descent_2.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003eHow do we compute the gradient? the process of computing the gradient is called \u003cstrong\u003ebackpropagation\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eDerivatives, chain rule\u003c/p\u003e\n\u003cp\u003eNeural networks are extremely complex functions with complex loss landscapes.\u003c/p\u003e\n\u003ch2 id=\"learning-rate\"\u003eLearning rate\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#learning-rate\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eYou don't want to set it too small, because you will be stuck in local minimum.\u003c/p\u003e\n\u003cp\u003eYou don't want it to be too large as you will overshoot and diverge.\u003c/p\u003e\n\u003ch3 id=\"adaptive-learning-rates\"\u003eAdaptive Learning Rates\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#adaptive-learning-rates\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eChange the learning rate depending on the landscape\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehow fast you are learning\u003c/li\u003e\n\u003cli\u003ehow steep\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"stochastic-gradient-descent\"\u003eStochastic Gradient Descent\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#stochastic-gradient-descent\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eGD computes the gradient over the entire dataset which can be computationally expensive.\u003c/p\u003e\n\u003cp\u003eSGD chooses a subset of the data to estimate the gradient\u003c/p\u003e\n\u003cp\u003eMini batch Gradient Descent (choose a batch of B data points) to calculate the gradient\u003c/p\u003e\n\u003cp\u003eLarger batches means you can trust your gradient more and you can use larger learning rate.\u003c/p\u003e\n\u003cp\u003eIf you use 32 data points you can parallelize gradient computation over 32 processors.\u003c/p\u003e\n\u003ch2 id=\"overfitting\"\u003eOverfitting\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#overfitting\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch2 id=\"regularization\"\u003eRegularization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regularization\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTo avoid overfitting.\u003c/p\u003e\n\u003ch3 id=\"dropout\"\u003eDropout\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dropout\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003esets some activation neurons to 0. Forces the network to learn a different pathway. Very power technique as it makes the model that does not rely too much on a fixed set of weights.\u003c/p\u003e\n\u003cp\u003eDropout nodes would not have any update, no gradient to compute.\u003c/p\u003e\n\u003ch3 id=\"early-stopping\"\u003eEarly stopping\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#early-stopping\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eStop the training model early.\u003c/p\u003e\n\u003cp\u003eTraining loss always go down.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./asstes/images/early_stopping.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn practice you can start plotting this curve and decide when to early stop!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIdeal Difference between train and test dataset is to be 0. Then you will not know when to stop. This usually happens in Large Language Models. The dataset is so big that the model itself finds it hard to memorize. So the difference between train and test will be almost always 0.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage models usually does not the classical overfitting problems.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"lecture-2-recurrent-neural-networks-transformers-and-attention\"\u003eLecture 2: Recurrent Neural Networks, Transformers, and Attention\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-2-recurrent-neural-networks-transformers-and-attention\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"deep-sequence-modeling\"\u003eDeep Sequence Modeling\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#deep-sequence-modeling\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eHow do we model time series or sequential data in Neural Networks?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNaive\u003c/strong\u003e. We can have our standard feed-forward neural network and put input output \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e(x_0,y_0),(x_1,y_2)...(x_t,y_t)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mord\"\u003e...\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/naive_sequence_modeling.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003eThis approach does not take the sequence info, taking past history into the future.\u003c/p\u003e\n\u003ch2 id=\"rnn---recurrent-neural-networks\"\u003eRNN - Recurrent Neural Networks\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#rnn---recurrent-neural-networks\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe will pass sequentially \u003cstrong\u003ehidden state\u003c/strong\u003e of the network. Hidden state is a vector that is passed.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/rnn_from_scratch.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/rnn_computational_graph.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch2 id=\"sequence-modeling-requirements\"\u003eSequence Modeling Requirements\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#sequence-modeling-requirements\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHandle \u003cstrong\u003evariable-length\u003c/strong\u003e sequences. Sentences can be short or long. It is not fixed like image where it had high vs width pixels length\u003c/li\u003e\n\u003cli\u003eHandle \u003cstrong\u003elong-term\u003c/strong\u003e dependency. You can short or long term dependency. Something at the very beginning might dictate the end of the sequence\u003c/li\u003e\n\u003cli\u003eMaintain information about \u003cstrong\u003eorder\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShare parameters\u003c/strong\u003e across sequence\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRNNs can handle the above requirements - though it needs a few upgrades. RNN are in the core of Sequence Modeling\u003c/p\u003e\n\u003cp\u003eThese requirements show why Sequence modeling is rich and complex.\u003c/p\u003e\n\u003ch2 id=\"example-predict-the-next-word\"\u003eExample. Predict the next word.\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-predict-the-next-word\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWe need to represent language to a neural network. We need to vectorize the words as numbers.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eEncode language to neural network\u003c/li\u003e\n\u003cli\u003eEmbedding: transforming words to vectors\u003c/li\u003e\n\u003cli\u003eVocabulary -\u003e Indexing -\u003e Embedding (one-hot-embedding)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEMBEDDING = VECTORIZATION of words\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/embedding.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003elearned embedding = lower dimensional representation of language\u003c/p\u003e\n\u003ch2 id=\"backpropagation-through-time\"\u003eBackpropagation through time.\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#backpropagation-through-time\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eCarry partial derivatives from errors late in the sequence to the very beginning. You will have to multiply matrices multiple times and multiply derivatives many times.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/back_through_time.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eif there are many values \u003e 1 you might have \u003cstrong\u003eexploding gradient\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eif there are many values \u0026#x3C; 1 you might have \u003cstrong\u003evanishing gradient\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis problem exists in very deep feed forward neural networks. Having many layers mean you will multiply the partial derivatives many time.\u003c/p\u003e\n\u003ch3 id=\"unstable-gradients-makes-it-hard-to-lear-long-term-dependencies\"\u003eUnstable gradients makes it hard to lear long term dependencies.\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#unstable-gradients-makes-it-hard-to-lear-long-term-dependencies\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWe cannot pass information from late time steps into initial time steps to promote/update the weights.\u003c/p\u003e\n\u003cp\u003eCurrent research upgrades RNN-s to be able to tackle this.\u003c/p\u003e\n\u003ch2 id=\"lstm-gru\"\u003eLSTM, GRU\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lstm-gru\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIdes: use \u003cstrong\u003egates\u003c/strong\u003e to selectively add and remove information passed through hidden states. LSTM (long-short term memory) network uses gate cells to pass information throughout many time steps\u003c/p\u003e\n\u003ch2 id=\"limitations-of-rnns\"\u003eLimitations of RNNs\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#limitations-of-rnns\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eslow, no parallelization - need to compute derivatives sequentially\u003c/li\u003e\n\u003cli\u003ernn state (hidden state \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eh_{t}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eh\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003eâ€‹\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e) is of fixed size. Encoding bottleneck - we have cap on the information we can keep.\u003c/li\u003e\n\u003cli\u003enot long term memory\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003etime-step by time-step processing.. brings this bottleneck\u003c/p\u003e\n\u003cp\u003eCan we eliminate the need for recurrence? Squash all inputs into one vector and put it into one network. The naive approach does not work because it does not carry any time information. We've destroyed any notion of order.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIdea:\u003c/strong\u003e Can we define a way to identify the import parts of a sequence and model out the \u003c/p\u003e\n\u003cp\u003ePaper: \u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003eAttention is all you need\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis landmark paper defines what is a transformer.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGPT:\u003c/strong\u003e The T stands for transformer.\u003c/p\u003e\n\u003cp\u003eAttention: when we look at an image we do not go pixel by pixel and look which are the parts that we attend to.\u003c/p\u003e\n\u003cp\u003eFinding the most important pixels is just a simple search.s\u003c/p\u003e\n\u003ch3 id=\"transformers-core-idea\"\u003eTransformers core idea\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#transformers-core-idea\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eGoal: Search and attend to the most important features in an input.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEncode input (positional encoding) - didn't go into many details into that\u003c/li\u003e\n\u003cli\u003eCompute query, key, value. Use three weight matrices to get each of those. {key:value} are features\u003c/li\u003e\n\u003cli\u003eCompute similarity between query and all keys (dot product divided by scaling)\u003c/li\u003e\n\u003cli\u003eCompute \u003cstrong\u003eattention weighting\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eExtract values from important features\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAttention is the building block of transformers.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/transformers.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003eSelf-attention to handle sequence modeling\u003c/p\u003e\n\u003cp\u003eSelf-attention is the basis of many larne language models.\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1744237493819,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"ea936af1aea818f3052610daac103a63","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","v77wdzobzcackzimz6a7crv"],"parent":null,"data":{},"body":"\nWelcome to my Knowledge Base! Here I write about my perception of life, document exciting things I've learned, debate (with myself) on controversial topics. If you know me you will not be surprised to find out that I write mostly about engineering and maths. Other topics I'm interested in are economics, politics, business, chess and poker."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"7z1pagvjmrp0151ogbehtpi"},"buildId":"hUMuUtBSCkU7YSeQn-rle","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>