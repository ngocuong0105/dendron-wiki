<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Improving Deep Neural Netwo</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Improving Deep Neural Netwo"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/ccipbaguot0vjojpl58x55h/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="6/7/2023"/><meta property="article:modified_time" content="6/8/2023"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/ccipbaguot0vjojpl58x55h/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="improving-deep-neural-netwo">Improving Deep Neural Netwo<a aria-hidden="true" class="anchor-heading icon-link" href="#improving-deep-neural-netwo"></a></h1>
<p><a href="https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning#syllabus">Andrew Ng's course</a></p>
<h1 id="ml-setup">ML Setup<a aria-hidden="true" class="anchor-heading icon-link" href="#ml-setup"></a></h1>
<p>Applied ML is a highly iterative process. Idea -> Code -> Experiment -> Idea -> Code -> Experiment -> ...</p>
<p>You need setup to that. You need to be able to run experiments quickly.</p>
<p>Train/Validation/Test sets split</p>
<p>Privious era was 60/20/20 or 70/30 (train/test split)</p>
<p>This is fine when you have 100-1000-10000 data points</p>
<p>But now when you have big data 1M+, then you might need less test set. You use the test set to evaluate of how well your model is doing on unseen data. Say you need 10,000 test data points for test. If you have 1M data points, then you can use 99/1 split.</p>
<p><strong>Aim to have yur validation and test set coming from the same distribution.</strong></p>
<h1 id="biasvariance">Bias/Variance<a aria-hidden="true" class="anchor-heading icon-link" href="#biasvariance"></a></h1>
<p>Say we have the task is to classify images of dogs. This is a task human are almot perfect (optimal error is neary 0 percent)</p>
<ul>
<li>
<p>Train set error: 1 %</p>
</li>
<li>
<p>Dev set error: 11 %
High variance/overfitting</p>
</li>
<li>
<p>Train set: 15%</p>
</li>
<li>
<p>Dev set error: 16%
High bias/underfitting, because these errors are much larger than the base error.</p>
</li>
</ul>
<h2 id="recipe">Recipe<a aria-hidden="true" class="anchor-heading icon-link" href="#recipe"></a></h2>
<ol>
<li>Ask yourself: "Does your model have high bias?" If yes, you need more complicated network (bigger network, train longer, different NN architecture). Try to get tird of this problem first.</li>
<li>What is my dev/validation performance. Am I overfitting? If yes, get more data, regularization, different NN architecture.</li>
</ol>
<p>In DL era, there is less talk about bias-variance tradeoff. </p>
<ul>
<li>Making a bigger network usually decreases bias and as long as you regularize, you would not hurt variance (Need more computational time).</li>
<li>Training with more data would usually decrease variance and not hurt bias.</li>
</ul>
<h1 id="regularization">Regularization<a aria-hidden="true" class="anchor-heading icon-link" href="#regularization"></a></h1>
<p>L1 model, makes the model more sparse. L2 model, makes the model more smooth.</p>
<p>L2 regularization results in weight decay. The weights would slowly decrease.</p>
<p>If you do not have regularization, then your Loss would decrease monotonically as you increase the number of iterations</p>
<h2 id="dropout-regularization">Dropout regularization<a aria-hidden="true" class="anchor-heading icon-link" href="#dropout-regularization"></a></h2>
<p>Randomly eliminate a random set of nodes in each layer. It randomly shuts down some neurons in each iteration. The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. </p>
<p><img src="/dendron-wiki/assets/images/dropout.png"></p>
<p>Drop out would decrese the expected values of the activations of each node. So you need to increase the weights to compensate for that. So you need to increase the weights by a factor of 1/(1-p) where p is the probability of dropout. <strong>Inverted dropout</strong></p>
<p><img src="/dendron-wiki/assets/images/inverted_dropout.png"></p>
<p>The layers with more neurons would have higher dropout rate.</p>
<p>At test time, you do not use dropout. You use the whole network.</p>
<p>Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. Dropout roughly doubles the number of iterations required to converge.</p>
<p>Say node A in layer L depends on nodes B,C,D in layaer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span>. Dropping nodes in layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">L-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span> would make node A spread out evenly its dependence on nodes B,C,D. It would not put all the weight in one of those.</p>
<h1 id="normalizing-data">Normalizing data<a aria-hidden="true" class="anchor-heading icon-link" href="#normalizing-data"></a></h1>
<p>substract mean + divide by <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span></p>
<p>Why do we need normalizing?</p>
<p>If x1 and x2 are in different scales, then the weights <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">w1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">1</span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">w2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord">2</span></span></span></span></span> would be very different the cost function would be elongated in one direction (stretched cost function). So the gradient descent would take longer to converge.</p>
<p><img src="/dendron-wiki/assets/images/opt_we.png"></p>
<h1 id="vanishing--exploding-gradients">Vanishing / Exploding gradients<a aria-hidden="true" class="anchor-heading icon-link" href="#vanishing--exploding-gradients"></a></h1>
<p>Problem of the DEEP networks.</p>
<p>Say we have an indetity activation function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">g(z) = z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>b</mi><mi>l</mi></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b^{l}=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span> for all laeyers. Then:</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi>W</mi><mi>L</mi></msup><msup><mi>W</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msup><mi>W</mi><mn>1</mn></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">\hat{y} = W^{L}W^{L-1}...W^{1}X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span>. Then the derivatives could increase exponentially or decrease exponentially. </p>
<h1 id="weight-initialization">Weight Initialization<a aria-hidden="true" class="anchor-heading icon-link" href="#weight-initialization"></a></h1>
<p>This can be important to tackle the vanishing/exploding gradients problem.</p>
<p>$z = w_1x_1 +...+w_nx_n $</p>
<p>The larger <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>, the smaller <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> you want to have.</p>
<p>For Relu activation people often initialize with <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi><mi mathvariant="normal">.</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>n</mi><mo stretchy="false">(</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>n</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mo stretchy="false">(</mo><mn>2</mn><mi mathvariant="normal">/</mi><msup><mi>n</mi><mrow><mo stretchy="false">[</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">W = np.random.randn(shape) * np.sqrt(2/n^{[l-1]})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">p</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">d</span><span class="mord mathnormal">o</span><span class="mord mathnormal">m</span><span class="mord">.</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">an</span><span class="mord mathnormal">d</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">ha</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">p</span><span class="mord">.</span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord">2/</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<h2 id="gradient-checking">Gradient Checking<a aria-hidden="true" class="anchor-heading icon-link" href="#gradient-checking"></a></h2>
<p>Gradient estimation:</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>J</mi></mrow><mrow><mi>d</mi><mi>θ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo><mo>−</mo><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>−</mo><mi>ϵ</mi><mo stretchy="false">)</mo></mrow><mrow><mn>2</mn><mi>ϵ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{dJ}{d\theta} = \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">dJ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">ϵ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">ϵ</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> is better than <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>+</mo><mi>ϵ</mi><mo stretchy="false">)</mo><mo>−</mo><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mi>ϵ</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{J(\theta + \epsilon) - J(\theta)}{\epsilon}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϵ</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p><img src="/dendron-wiki/assets/images/grad_est.png"></p>
<p>Using these gradient estimation formulas, you can check if your backpropagation is correct.</p>
<p><img src="/dendron-wiki/assets/images/grad_check.png"></p>
<p><a href="https://arxiv.org/abs/1502.01852s">He et.al 2015</a> initialization</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">initialize_parameters_he</span><span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """</span>
    
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    parameters <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    L <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>layers_dims<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token comment"># integer representing the number of layers</span>
     
    <span class="token keyword">for</span> l <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> L <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        parameters<span class="token punctuation">[</span><span class="token string">'W'</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>layers_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> layers_dims<span class="token punctuation">[</span>l<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">*</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span>layers_dims<span class="token punctuation">[</span>l<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># small weights to start with, normal distribution is better than uniform to avoid the extremes, most points in the center where the derivative is larger - see sigmoid function.</span>
        parameters<span class="token punctuation">[</span><span class="token string">'b'</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>layers_dims<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> parameters

</code></pre>
<h1 id="optimization-algorithms">Optimization Algorithms<a aria-hidden="true" class="anchor-heading icon-link" href="#optimization-algorithms"></a></h1></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ml-setup" title="ML Setup">ML Setup</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#biasvariance" title="Bias/Variance">Bias/Variance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#recipe" title="Recipe">Recipe</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#regularization" title="Regularization">Regularization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dropout-regularization" title="Dropout regularization">Dropout regularization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#normalizing-data" title="Normalizing data">Normalizing data</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#vanishing--exploding-gradients" title="Vanishing / Exploding gradients">Vanishing / Exploding gradients</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#weight-initialization" title="Weight Initialization">Weight Initialization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#gradient-checking" title="Gradient Checking">Gradient Checking</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#optimization-algorithms" title="Optimization Algorithms">Optimization Algorithms</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"ccipbaguot0vjojpl58x55h","title":"Improving Deep Neural Netwo","desc":"","updated":1686241869638,"created":1686139445590,"custom":{},"fname":"deep learning.Coursera.Improving Deep Neural Networks","type":"note","vault":{"fsPath":"vault"},"contentHash":"dba6f355e73b18752f3d36947755b3fa","links":[],"anchors":{"ml-setup":{"type":"header","text":"ML Setup","value":"ml-setup","line":10,"column":0,"depth":1},"biasvariance":{"type":"header","text":"Bias/Variance","value":"biasvariance","line":27,"column":0,"depth":1},"recipe":{"type":"header","text":"Recipe","value":"recipe","line":39,"column":0,"depth":2},"regularization":{"type":"header","text":"Regularization","value":"regularization","line":48,"column":0,"depth":1},"dropout-regularization":{"type":"header","text":"Dropout regularization","value":"dropout-regularization","line":55,"column":0,"depth":2},"normalizing-data":{"type":"header","text":"Normalizing data","value":"normalizing-data","line":75,"column":0,"depth":1},"vanishing--exploding-gradients":{"type":"header","text":"Vanishing / Exploding gradients","value":"vanishing--exploding-gradients","line":85,"column":0,"depth":1},"weight-initialization":{"type":"header","text":"Weight Initialization","value":"weight-initialization","line":93,"column":0,"depth":1},"gradient-checking":{"type":"header","text":"Gradient Checking","value":"gradient-checking","line":103,"column":0,"depth":2},"optimization-algorithms":{"type":"header","text":"Optimization Algorithms","value":"optimization-algorithms","line":146,"column":0,"depth":1}},"children":[],"parent":"xfq7qpcvdkdjakeed07yyru","data":{}},"body":"\u003ch1 id=\"improving-deep-neural-netwo\"\u003eImproving Deep Neural Netwo\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#improving-deep-neural-netwo\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning#syllabus\"\u003eAndrew Ng's course\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"ml-setup\"\u003eML Setup\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ml-setup\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eApplied ML is a highly iterative process. Idea -\u003e Code -\u003e Experiment -\u003e Idea -\u003e Code -\u003e Experiment -\u003e ...\u003c/p\u003e\n\u003cp\u003eYou need setup to that. You need to be able to run experiments quickly.\u003c/p\u003e\n\u003cp\u003eTrain/Validation/Test sets split\u003c/p\u003e\n\u003cp\u003ePrivious era was 60/20/20 or 70/30 (train/test split)\u003c/p\u003e\n\u003cp\u003eThis is fine when you have 100-1000-10000 data points\u003c/p\u003e\n\u003cp\u003eBut now when you have big data 1M+, then you might need less test set. You use the test set to evaluate of how well your model is doing on unseen data. Say you need 10,000 test data points for test. If you have 1M data points, then you can use 99/1 split.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAim to have yur validation and test set coming from the same distribution.\u003c/strong\u003e\u003c/p\u003e\n\u003ch1 id=\"biasvariance\"\u003eBias/Variance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#biasvariance\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eSay we have the task is to classify images of dogs. This is a task human are almot perfect (optimal error is neary 0 percent)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTrain set error: 1 %\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDev set error: 11 %\nHigh variance/overfitting\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain set: 15%\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDev set error: 16%\nHigh bias/underfitting, because these errors are much larger than the base error.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"recipe\"\u003eRecipe\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#recipe\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAsk yourself: \"Does your model have high bias?\" If yes, you need more complicated network (bigger network, train longer, different NN architecture). Try to get tird of this problem first.\u003c/li\u003e\n\u003cli\u003eWhat is my dev/validation performance. Am I overfitting? If yes, get more data, regularization, different NN architecture.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn DL era, there is less talk about bias-variance tradeoff. \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMaking a bigger network usually decreases bias and as long as you regularize, you would not hurt variance (Need more computational time).\u003c/li\u003e\n\u003cli\u003eTraining with more data would usually decrease variance and not hurt bias.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"regularization\"\u003eRegularization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regularization\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eL1 model, makes the model more sparse. L2 model, makes the model more smooth.\u003c/p\u003e\n\u003cp\u003eL2 regularization results in weight decay. The weights would slowly decrease.\u003c/p\u003e\n\u003cp\u003eIf you do not have regularization, then your Loss would decrease monotonically as you increase the number of iterations\u003c/p\u003e\n\u003ch2 id=\"dropout-regularization\"\u003eDropout regularization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dropout-regularization\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eRandomly eliminate a random set of nodes in each layer. It randomly shuts down some neurons in each iteration. The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/dropout.png\"\u003e\u003c/p\u003e\n\u003cp\u003eDrop out would decrese the expected values of the activations of each node. So you need to increase the weights to compensate for that. So you need to increase the weights by a factor of 1/(1-p) where p is the probability of dropout. \u003cstrong\u003eInverted dropout\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/inverted_dropout.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe layers with more neurons would have higher dropout rate.\u003c/p\u003e\n\u003cp\u003eAt test time, you do not use dropout. You use the whole network.\u003c/p\u003e\n\u003cp\u003eDropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. Dropout roughly doubles the number of iterations required to converge.\u003c/p\u003e\n\u003cp\u003eSay node A in layer L depends on nodes B,C,D in layaer \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL-1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. Dropping nodes in layer \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL-1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e would make node A spread out evenly its dependence on nodes B,C,D. It would not put all the weight in one of those.\u003c/p\u003e\n\u003ch1 id=\"normalizing-data\"\u003eNormalizing data\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#normalizing-data\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003esubstract mean + divide by \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\sigma\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eWhy do we need normalizing?\u003c/p\u003e\n\u003cp\u003eIf x1 and x2 are in different scales, then the weights \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e and \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mord\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e would be very different the cost function would be elongated in one direction (stretched cost function). So the gradient descent would take longer to converge.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/opt_we.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"vanishing--exploding-gradients\"\u003eVanishing / Exploding gradients\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#vanishing--exploding-gradients\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eProblem of the DEEP networks.\u003c/p\u003e\n\u003cp\u003eSay we have an indetity activation function \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ez\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eg(z) = z\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eg\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e and \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eb^{l}=0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8491em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8491em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\"\u003el\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e for all laeyers. Then:\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/msup\u003e\u003cmsup\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y} = W^{L}W^{L-1}...W^{1}X\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6944em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.1944em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1944em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8413em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eL\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eL\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e...\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. Then the derivatives could increase exponentially or decrease exponentially. \u003c/p\u003e\n\u003ch1 id=\"weight-initialization\"\u003eWeight Initialization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#weight-initialization\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eThis can be important to tackle the vanishing/exploding gradients problem.\u003c/p\u003e\n\u003cp\u003e$z = w_1x_1 +...+w_nx_n $\u003c/p\u003e\n\u003cp\u003eThe larger \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e, the smaller \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e you want to have.\u003c/p\u003e\n\u003cp\u003eFor Relu activation people often initialize with \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003eq\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmi mathvariant=\"normal\"\u003e/\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eW = np.random.randn(shape) * np.sqrt(2/n^{[l-1]})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\"\u003eW\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"mord\"\u003e.\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ean\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003cspan class=\"mord\"\u003e.\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ean\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003es\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eha\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e∗\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.138em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"mord\"\u003e.\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003es\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eq\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e2/\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.888em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen mtight\"\u003e[\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\"\u003el\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003ch2 id=\"gradient-checking\"\u003eGradient Checking\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gradient-checking\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eGradient estimation:\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\frac{dJ}{d\\theta} = \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2251em;vertical-align:-0.345em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8801em;\"\u003e\u003cspan style=\"top:-2.655em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ed\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.394em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.09618em;\"\u003edJ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.345em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.355em;vertical-align:-0.345em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.01em;\"\u003e\u003cspan style=\"top:-2.655em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eϵ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.485em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e+\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eϵ\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eϵ\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.345em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e is better than \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.355em;vertical-align:-0.345em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.01em;\"\u003e\u003cspan style=\"top:-2.655em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eϵ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.485em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e+\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eϵ\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.345em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/grad_est.png\"\u003e\u003c/p\u003e\n\u003cp\u003eUsing these gradient estimation formulas, you can check if your backpropagation is correct.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/grad_check.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1502.01852s\"\u003eHe et.al 2015\u003c/a\u003e initialization\u003c/p\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003einitialize_parameters_he\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003elayers_dims\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token triple-quoted-string string\"\u003e\"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\u003c/span\u003e\n    \n    np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    parameters \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\n    L \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token builtin\"\u003elen\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003elayers_dims\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token operator\"\u003e-\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# integer representing the number of layers\u003c/span\u003e\n     \n    \u003cspan class=\"token keyword\"\u003efor\u003c/span\u003e l \u003cspan class=\"token keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"token builtin\"\u003erange\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e L \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n        parameters\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token string\"\u003e'W'\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token builtin\"\u003estr\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003el\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandn\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003elayers_dims\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e layers_dims\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token operator\"\u003e*\u003c/span\u003enp\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003esqrt\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token operator\"\u003e/\u003c/span\u003elayers_dims\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# small weights to start with, normal distribution is better than uniform to avoid the extremes, most points in the center where the derivative is larger - see sigmoid function.\u003c/span\u003e\n        parameters\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token string\"\u003e'b'\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token builtin\"\u003estr\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003el\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ezeros\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003elayers_dims\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003el\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e parameters\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"optimization-algorithms\"\u003eOptimization Algorithms\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#optimization-algorithms\"\u003e\u003c/a\u003e\u003c/h1\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"ccipbaguot0vjojpl58x55h"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>