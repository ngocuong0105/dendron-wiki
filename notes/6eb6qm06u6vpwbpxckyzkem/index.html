<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Stanford CS230: Deep Learning</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Stanford CS230: Deep Learning"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/6eb6qm06u6vpwbpxckyzkem/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="5/22/2023"/><meta property="article:modified_time" content="6/8/2023"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/6eb6qm06u6vpwbpxckyzkem/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="stanford-cs230-deep-learning">Stanford CS230: Deep Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#stanford-cs230-deep-learning"></a></h1>
<p><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb">Stanford DL course</a></p>
<p><a href="http://cs230.stanford.edu/syllabus/">Syllabus</a></p>
<p>This course is youtube videos + coursera videos. Youtube has deeper insights and coursera is for practice and fundamentals.</p>
<h1 id="key-messages">Key messages<a aria-hidden="true" class="anchor-heading icon-link" href="#key-messages"></a></h1>
<ul>
<li>In deep learning, feature learning replaces feature engineering</li>
<li>Traditional ML performance <strong>platues</strong> at some point and cannnot utilize more data. For NN we have not reached that point yet.</li>
<li>ML,DL is all about approximating/learning a function.</li>
<li>Model = Architecture + Parameters</li>
<li>Gradient evaluated at particular point = slope of the tangent at particular point</li>
<li>Gradient checking (e.g if the signs changes often) is a good way to see if your optimization of the loss function is divergent.</li>
<li>Backprogation is just an application of the chain rule.</li>
<li>Backpropagation is used to calculate the gradient of the loss function with respect to the parameters. </li>
<li>ReLU is the most common activation function</li>
<li>Sigmoid is almost never used for hidden layers, tanh is almost always better choice</li>
<li>NN require random initialization</li>
<li>Aim to have yur validation and test set coming from the same distribution.</li>
<li>L2 regularization is also called weight decay <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>α</mi><mi>λ</mi></mrow><mi>n</mi></mfrac></mstyle><mo stretchy="false">)</mo><mi>w</mi><mo>−</mo><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">w = (1-\dfrac{\alpha\lambda}{n})w - other terms</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal">λ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">m</span><span class="mord mathnormal">s</span></span></span></span></span></li>
<li>Sanity checks for your NN:
<ul>
<li>cost is decreasing as number of iterations increase (if using dropout, might not be the case)</li>
<li>gradient checking  </li>
</ul>
</li>
<li>Regularization is used to reduce overfitting
<ul>
<li>L1, L2</li>
<li>Dropout</li>
<li>Data augmentation (add more data, rotate image, add some noise, flip image, blur image )</li>
<li>Early stopping</li>
</ul>
</li>
<li>Orthogonalisation idea one task at a time. (First focus on minimizing the cost function (overfit!). Then focus on regularization.)</li>
<li>Normalize inputs for NN to converge more quickly</li>
<li>Careful choice of weights initialization can solve vanishing/exploding gradients</li>
<li>Gradient tracking</li>
<li><a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a></li>
</ul>
<h1 id="lecture-1-deep-learning-overview">Lecture 1 Deep Learning overview<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-1-deep-learning-overview"></a></h1>
<p>Deep Learning and Neural Network are almost the same thing. DL is the popular brand word.</p>
<p>Traditional ML performance <strong>platues</strong> at some point and cannnot utilize more data. For NN we have not reached that point yet.</p>
<p><img src="/dendron-wiki/assets/images/ML_DL_performance.png"></p>
<p><strong>Fundamental courses:</strong></p>
<ul>
<li>CS229 (ML most mathematical)</li>
<li>CS299A (Applied ML least mathematical and easiest)</li>
<li>CS230 (a bit in between, focuses on DL)</li>
</ul>
<p>Finish C1M1, C1M2 (chap 1 module 2).</p>
<h1 id="lecture-2-deep-learning-intuition">Lecture 2: Deep Learning Intuition<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-2-deep-learning-intuition"></a></h1>
<p>ML,DL is all about approximating/learning a function.</p>
<p><strong>Model = Architecture + Parameters</strong></p>
<p>The goal of ML/DL is to find the best parameters for the model.</p>
<p>Example:</p>
<p><img src="/dendron-wiki/assets/images/lg_nn.png"></p>
<p><strong>Each hidden layer encodes information from previous layer. E.g in convolutional neural network, each layer encodes more and more complex features. </strong></p>
<p><img src="/dendron-wiki/assets/images/cnn_encoding.png"></p>
<h2 id="case-study-1-day-and-night-classification">Case study 1: Day and Night classification<a aria-hidden="true" class="anchor-heading icon-link" href="#case-study-1-day-and-night-classification"></a></h2>
<p><strong>Deep Learning Project choices:</strong></p>
<p><img src="/dendron-wiki/assets/images/dl_choices.png"></p>
<p>The problem of clasifying a cat needed 10,000 images. Is the next problem easier or harder? Depending  on the task you want to solve you would know based on past projets how many data point you need.</p>
<ul>
<li>train/test split 80/20 is good for 10k images. If I had 1M images I would choose 98/2 split. Test data is to gauge how well the model is doing on real unseen data. You ask yourself how many data points I need to tell my model is doing good (dawn, sunset, sunrise, evening, morning)</li>
<li>bias you want balanced dataset in train and test</li>
<li>resolution of images (the smaller the better for computation. 32x32 is better than 400x400) Choose the smallest resolution that human can have perfect performance. If you had unlimited computational power you would choose the highest resolution.</li>
</ul>
<h2 id="case-study-2-face-verification">Case study 2: Face verification<a aria-hidden="true" class="anchor-heading icon-link" href="#case-study-2-face-verification"></a></h2>
<ul>
<li>architecture: encode each image using a DL and then compute distance functions</li>
<li>In face verification, we have used an encoder network to learn a lower
dimensional representation (called “encoding”) for a set of data by
training the network to focus on non-noisy signals.</li>
<li><strong>Triplet loss</strong> is a loss function where an (anchor) input is compared to a
positive input and a negative input. The distance from the anchor input to
the positive input is minimized, whereas the distance from the anchor input
to the negative input is maximized.</li>
</ul>
<p><img src="/dendron-wiki/assets/images/face_verify1.png"></p>
<p><img src="/dendron-wiki/assets/images/face_verify2.png"></p>
<p><img src="/dendron-wiki/assets/images/face_verify3.png"></p>
<h2 id="case-study-3-art-generation">Case study 3: Art generation<a aria-hidden="true" class="anchor-heading icon-link" href="#case-study-3-art-generation"></a></h2>
<ul>
<li>Do not train model, just optimize the cost function by changing pixels (goal is to generate image)
Kian Katanforoosh</li>
<li>In the neural style transfer algorithm proposed by Gatys et al., you optimize
image pixels rather than model parameters. Model parameters are
pretrained and non-trainable.</li>
<li>You leverage the “knowledge” of a pretrained model to extract the content
of a content image and the style of a style image</li>
<li>ImageNet</li>
<li>Take gradient of loss function with respect to the pixels <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>.</li>
</ul>
<p><img src="/dendron-wiki/assets/images/art_gen1.png"></p>
<p><img src="/dendron-wiki/assets/images/art_gen2.png"></p>
<p><img src="/dendron-wiki/assets/images/art_gen3.png"></p>
<h2 id="case-study-4-trigger-word-detection-alexa">Case study 4: Trigger word detection (Alexa)<a aria-hidden="true" class="anchor-heading icon-link" href="#case-study-4-trigger-word-detection-alexa"></a></h2>
<ul>
<li>Your data collection strategy is critical to the success of your project. (If
applicable) Don’t hesitate to get out of the building.</li>
<li>You can gain insights on your labelling strategy by using a human experiment</li>
</ul>
<p><img src="/dendron-wiki/assets/images/trigger_word.png"></p>
<p><img src="/dendron-wiki/assets/images/data_collection.png"></p>
<h1 id="lecture-3---full-cycle-deep-learning-projects">Lecture 3 - Full-Cycle Deep Learning Projects<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-3---full-cycle-deep-learning-projects"></a></h1>
<ol>
<li>Select problem (e.g. supervised learning)</li>
<li>Get data (need to be very strategic in that)</li>
<li>Design model</li>
<li>Train model (iterative process with point 2 and 3)</li>
<li>Test model</li>
<li>Deploy</li>
<li>Maintain</li>
</ol>
<p>Five points when selecting a project:</p>
<ul>
<li>interest</li>
<li>data availability</li>
<li>domain knowledge</li>
<li>feasibility</li>
<li>Usefulness</li>
</ul>
<p>First goal when starting to build a ML system is to get a baseline as fast as possible. Then iterate on that. That is steps 1-5 should be done within a couple of days.</p>
<p>ML developement is an iterative process. Before you start working on it it is difficult to know what are the hard problems you need to tackle</p>
<p><strong>Tips:</strong></p>
<ul>
<li>keep clear notes on experiments runs</li>
<li>generally when you have to make choice between multiple options - go with the simpler one first</li>
<li>in real projects often data change over time. Non-ML models are more robust, ML models often need to be retrained.</li>
</ul>
<p>edge deployment vs cloud deployment</p>
<h1 id="lecture-4---adversarial-attacks--gans">Lecture 4 - Adversarial Attacks / GANs<a aria-hidden="true" class="anchor-heading icon-link" href="#lecture-4---adversarial-attacks--gans"></a></h1>
<p><strong>Discovery: Several ML models, including state-of-the-art neural networks are vulnerable to adversarial examples.</strong></p>
<p>[Szegedy et al. (2013): Intriguing properties of neural networks]</p>
<p>[Ian J. Goodfellow, Jonathon Shlens &#x26; Christian Szegedy (2015): Explaining and harnessing adversarial examples]</p>
<h2 id="attacking-a-network-with-adversarial-examples">Attacking a network with adversarial examples<a aria-hidden="true" class="anchor-heading icon-link" href="#attacking-a-network-with-adversarial-examples"></a></h2>
<p>What are examples of Adversarial attacks?</p>
<p>Given a network pretrained on ImageNet, find an input image that is not a iguana but will be classified as iguana.</p>
<p>Attack:</p>
<ol>
<li>Start with some image that is not iguana.</li>
<li>Use pretrained NN and output a vector of probabilities (cat, dog, iguana..)</li>
<li>Compute Loss.</li>
<li>Take gradient of loss wrt to the pixels. (backpropagation, need to have access to the model, parameters, layers etc.)</li>
<li>Change the image pixels.</li>
</ol>
<p><img src="/dendron-wiki/assets/images/adversarial_examples.png"></p>
<p>You can see that chaning with very little the pixels in the image you can fool the network that the cat is an iguana.</p>
<p><img src="/dendron-wiki/assets/images/space_images.png"></p>
<h2 id="defense-against-a-network-with-adversarial-examples">Defense against a network with adversarial examples<a aria-hidden="true" class="anchor-heading icon-link" href="#defense-against-a-network-with-adversarial-examples"></a></h2>
<ol>
<li>Add a SafetyNet - one more NN that will detect adversarial examples. (but it can be fooled as well) [Yuan et al. (2017): Adversarial Examples: Attacks and Defenses for Deep Learning]</li>
<li>Train on corrrectly labelled adversarial examples. Generate adversarial examples and train on them.</li>
</ol>
<h2 id="why-are-nn-vulnarable-to-adversarial-examples">Why are NN vulnarable to adversarial examples?<a aria-hidden="true" class="anchor-heading icon-link" href="#why-are-nn-vulnarable-to-adversarial-examples"></a></h2>
<p><a href="https://arxiv.org/pdf/1712.07107.pdf">Adversarial Examples: Attacks and Defenses for Deep Learning</a></p>
<p>You'd think that NN are vulnaraalbe because they are too complex and overfit the data. But it turns out that the linearity part of the network is the problem.</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mi>w</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xw+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p>
<p>It is easy to create <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo lspace="0em" rspace="0em">∗</mo></msup><mo>=</mo><mi>x</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">x^{*}=x+\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∗</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span></span> which would produce massively different y.</p>
<p><strong>Fast Gradient Sign Method</strong> is a method to create quickly adversarial examples.</p>
<h2 id="gan---generative-adversarial-networks">GAN - Generative Adversarial Networks<a aria-hidden="true" class="anchor-heading icon-link" href="#gan---generative-adversarial-networks"></a></h2>
<p>The word adversarial is used in different meaning here. In GANs it means that there are two networks that are competing with each other.</p>
<p>GAN-s are networks that generate images that mimic the distribution of the real images/data.</p>
<h3 id="gd-game">G/D game<a aria-hidden="true" class="anchor-heading icon-link" href="#gd-game"></a></h3>
<p>Generator (is what we want to train and generate fake images),</p>
<p>Disriminator</p>
<p>G wants to fool D. D catches fake images. In the beginning it would be very good at catching fake images. But G will learn to generate better images. G and D are trained together.</p>
<p>Min-max trick of chaning the loss function</p>
<p>Saturating cost vs non-saturating cost </p>
<p>If D does not improve G cannot improve. You can see D as an upper bound on G.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">for</span> num_iterations<span class="token punctuation">:</span>
    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>k_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        update D
    update G
</code></pre>
<p>Tips for training GAN-s:</p>
<ul>
<li>modify loss function</li>
<li>keep D up-to-date with respect to G (k update for G and 1 update for D)</li>
<li>Virtual Batchnorm</li>
<li>one-sided label smoothing</li>
</ul>
<p>CycleGAN</p>
<p>image with horses, generate same image with zebra.</p>
<p>Loss function contains all losses from normal gans + cycle loss. </p>
<p><img src="/dendron-wiki/assets/images/cycle_gan.png"></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#key-messages" title="Key messages">Key messages</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-1-deep-learning-overview" title="Lecture 1 Deep Learning overview">Lecture 1 Deep Learning overview</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-2-deep-learning-intuition" title="Lecture 2: Deep Learning Intuition">Lecture 2: Deep Learning Intuition</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#case-study-1-day-and-night-classification" title="Case study 1: Day and Night classification">Case study 1: Day and Night classification</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#case-study-2-face-verification" title="Case study 2: Face verification">Case study 2: Face verification</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#case-study-3-art-generation" title="Case study 3: Art generation">Case study 3: Art generation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#case-study-4-trigger-word-detection-alexa" title="Case study 4: Trigger word detection (Alexa)">Case study 4: Trigger word detection (Alexa)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-3---full-cycle-deep-learning-projects" title="Lecture 3 - Full-Cycle Deep Learning Projects">Lecture 3 - Full-Cycle Deep Learning Projects</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lecture-4---adversarial-attacks--gans" title="Lecture 4 - Adversarial Attacks / GANs">Lecture 4 - Adversarial Attacks / GANs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#attacking-a-network-with-adversarial-examples" title="Attacking a network with adversarial examples">Attacking a network with adversarial examples</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#defense-against-a-network-with-adversarial-examples" title="Defense against a network with adversarial examples">Defense against a network with adversarial examples</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#why-are-nn-vulnarable-to-adversarial-examples" title="Why are NN vulnarable to adversarial examples?">Why are NN vulnarable to adversarial examples?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#gan---generative-adversarial-networks" title="GAN - Generative Adversarial Networks">GAN - Generative Adversarial Networks</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#gd-game" title="G/D game">G/D game</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"6eb6qm06u6vpwbpxckyzkem","title":"Stanford CS230: Deep Learning","desc":"","updated":1686217953062,"created":1684778311163,"custom":{},"fname":"deep learning.Stanford CS230: Deep Learning","type":"note","vault":{"fsPath":"vault"},"contentHash":"f8a9dea04282c1a176db03ac088318b4","links":[],"anchors":{"key-messages":{"type":"header","text":"Key messages","value":"key-messages","line":14,"column":0,"depth":1},"lecture-1-deep-learning-overview":{"type":"header","text":"Lecture 1 Deep Learning overview","value":"lecture-1-deep-learning-overview","line":43,"column":0,"depth":1},"lecture-2-deep-learning-intuition":{"type":"header","text":"Lecture 2: Deep Learning Intuition","value":"lecture-2-deep-learning-intuition","line":60,"column":0,"depth":1},"case-study-1-day-and-night-classification":{"type":"header","text":"Case study 1: Day and Night classification","value":"case-study-1-day-and-night-classification","line":77,"column":0,"depth":2},"case-study-2-face-verification":{"type":"header","text":"Case study 2: Face verification","value":"case-study-2-face-verification","line":92,"column":0,"depth":2},"case-study-3-art-generation":{"type":"header","text":"Case study 3: Art generation","value":"case-study-3-art-generation","line":110,"column":0,"depth":2},"case-study-4-trigger-word-detection-alexa":{"type":"header","text":"Case study 4: Trigger word detection (Alexa)","value":"case-study-4-trigger-word-detection-alexa","line":127,"column":0,"depth":2},"lecture-3---full-cycle-deep-learning-projects":{"type":"header","text":"Lecture 3 - Full-Cycle Deep Learning Projects","value":"lecture-3---full-cycle-deep-learning-projects","line":138,"column":0,"depth":1},"lecture-4---adversarial-attacks--gans":{"type":"header","text":"Lecture 4 - Adversarial Attacks / GANs","value":"lecture-4---adversarial-attacks--gans","line":166,"column":0,"depth":1},"attacking-a-network-with-adversarial-examples":{"type":"header","text":"Attacking a network with adversarial examples","value":"attacking-a-network-with-adversarial-examples","line":174,"column":0,"depth":2},"defense-against-a-network-with-adversarial-examples":{"type":"header","text":"Defense against a network with adversarial examples","value":"defense-against-a-network-with-adversarial-examples","line":194,"column":0,"depth":2},"why-are-nn-vulnarable-to-adversarial-examples":{"type":"header","text":"Why are NN vulnarable to adversarial examples?","value":"why-are-nn-vulnarable-to-adversarial-examples","line":200,"column":0,"depth":2},"gan---generative-adversarial-networks":{"type":"header","text":"GAN - Generative Adversarial Networks","value":"gan---generative-adversarial-networks","line":212,"column":0,"depth":2},"gd-game":{"type":"header","text":"G/D game","value":"gd-game","line":218,"column":0,"depth":3}},"children":[],"parent":"ro9bbyftsutm88mxw6r16p5","data":{}},"body":"\u003ch1 id=\"stanford-cs230-deep-learning\"\u003eStanford CS230: Deep Learning\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#stanford-cs230-deep-learning\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb\"\u003eStanford DL course\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://cs230.stanford.edu/syllabus/\"\u003eSyllabus\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis course is youtube videos + coursera videos. Youtube has deeper insights and coursera is for practice and fundamentals.\u003c/p\u003e\n\u003ch1 id=\"key-messages\"\u003eKey messages\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#key-messages\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eIn deep learning, feature learning replaces feature engineering\u003c/li\u003e\n\u003cli\u003eTraditional ML performance \u003cstrong\u003eplatues\u003c/strong\u003e at some point and cannnot utilize more data. For NN we have not reached that point yet.\u003c/li\u003e\n\u003cli\u003eML,DL is all about approximating/learning a function.\u003c/li\u003e\n\u003cli\u003eModel = Architecture + Parameters\u003c/li\u003e\n\u003cli\u003eGradient evaluated at particular point = slope of the tangent at particular point\u003c/li\u003e\n\u003cli\u003eGradient checking (e.g if the signs changes often) is a good way to see if your optimization of the loss function is divergent.\u003c/li\u003e\n\u003cli\u003eBackprogation is just an application of the chain rule.\u003c/li\u003e\n\u003cli\u003eBackpropagation is used to calculate the gradient of the loss function with respect to the parameters. \u003c/li\u003e\n\u003cli\u003eReLU is the most common activation function\u003c/li\u003e\n\u003cli\u003eSigmoid is almost never used for hidden layers, tanh is almost always better choice\u003c/li\u003e\n\u003cli\u003eNN require random initialization\u003c/li\u003e\n\u003cli\u003eAim to have yur validation and test set coming from the same distribution.\u003c/li\u003e\n\u003cli\u003eL2 regularization is also called weight decay \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmstyle displaystyle=\"true\" scriptlevel=\"0\"\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003eλ\u003c/mi\u003e\u003c/mrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mfrac\u003e\u003c/mstyle\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew = (1-\\dfrac{\\alpha\\lambda}{n})w - other terms\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:2.0574em;vertical-align:-0.686em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mopen nulldelimiter\"\u003e\u003c/span\u003e\u003cspan class=\"mfrac\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.3714em;\"\u003e\u003cspan style=\"top:-2.314em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.23em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.677em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eλ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.686em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose nulldelimiter\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eh\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eer\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eer\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003es\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eSanity checks for your NN:\n\u003cul\u003e\n\u003cli\u003ecost is decreasing as number of iterations increase (if using dropout, might not be the case)\u003c/li\u003e\n\u003cli\u003egradient checking  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRegularization is used to reduce overfitting\n\u003cul\u003e\n\u003cli\u003eL1, L2\u003c/li\u003e\n\u003cli\u003eDropout\u003c/li\u003e\n\u003cli\u003eData augmentation (add more data, rotate image, add some noise, flip image, blur image )\u003c/li\u003e\n\u003cli\u003eEarly stopping\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOrthogonalisation idea one task at a time. (First focus on minimizing the cost function (overfit!). Then focus on regularization.)\u003c/li\u003e\n\u003cli\u003eNormalize inputs for NN to converge more quickly\u003c/li\u003e\n\u003cli\u003eCareful choice of weights initialization can solve vanishing/exploding gradients\u003c/li\u003e\n\u003cli\u003eGradient tracking\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1502.01852\"\u003ehttps://arxiv.org/abs/1502.01852\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"lecture-1-deep-learning-overview\"\u003eLecture 1 Deep Learning overview\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-1-deep-learning-overview\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eDeep Learning and Neural Network are almost the same thing. DL is the popular brand word.\u003c/p\u003e\n\u003cp\u003eTraditional ML performance \u003cstrong\u003eplatues\u003c/strong\u003e at some point and cannnot utilize more data. For NN we have not reached that point yet.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/ML_DL_performance.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFundamental courses:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCS229 (ML most mathematical)\u003c/li\u003e\n\u003cli\u003eCS299A (Applied ML least mathematical and easiest)\u003c/li\u003e\n\u003cli\u003eCS230 (a bit in between, focuses on DL)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinish C1M1, C1M2 (chap 1 module 2).\u003c/p\u003e\n\u003ch1 id=\"lecture-2-deep-learning-intuition\"\u003eLecture 2: Deep Learning Intuition\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-2-deep-learning-intuition\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eML,DL is all about approximating/learning a function.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel = Architecture + Parameters\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe goal of ML/DL is to find the best parameters for the model.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/lg_nn.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEach hidden layer encodes information from previous layer. E.g in convolutional neural network, each layer encodes more and more complex features. \u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/cnn_encoding.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"case-study-1-day-and-night-classification\"\u003eCase study 1: Day and Night classification\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#case-study-1-day-and-night-classification\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDeep Learning Project choices:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/dl_choices.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe problem of clasifying a cat needed 10,000 images. Is the next problem easier or harder? Depending  on the task you want to solve you would know based on past projets how many data point you need.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etrain/test split 80/20 is good for 10k images. If I had 1M images I would choose 98/2 split. Test data is to gauge how well the model is doing on real unseen data. You ask yourself how many data points I need to tell my model is doing good (dawn, sunset, sunrise, evening, morning)\u003c/li\u003e\n\u003cli\u003ebias you want balanced dataset in train and test\u003c/li\u003e\n\u003cli\u003eresolution of images (the smaller the better for computation. 32x32 is better than 400x400) Choose the smallest resolution that human can have perfect performance. If you had unlimited computational power you would choose the highest resolution.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"case-study-2-face-verification\"\u003eCase study 2: Face verification\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#case-study-2-face-verification\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003earchitecture: encode each image using a DL and then compute distance functions\u003c/li\u003e\n\u003cli\u003eIn face verification, we have used an encoder network to learn a lower\ndimensional representation (called “encoding”) for a set of data by\ntraining the network to focus on non-noisy signals.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTriplet loss\u003c/strong\u003e is a loss function where an (anchor) input is compared to a\npositive input and a negative input. The distance from the anchor input to\nthe positive input is minimized, whereas the distance from the anchor input\nto the negative input is maximized.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/face_verify1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/face_verify2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/face_verify3.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"case-study-3-art-generation\"\u003eCase study 3: Art generation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#case-study-3-art-generation\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDo not train model, just optimize the cost function by changing pixels (goal is to generate image)\nKian Katanforoosh\u003c/li\u003e\n\u003cli\u003eIn the neural style transfer algorithm proposed by Gatys et al., you optimize\nimage pixels rather than model parameters. Model parameters are\npretrained and non-trainable.\u003c/li\u003e\n\u003cli\u003eYou leverage the “knowledge” of a pretrained model to extract the content\nof a content image and the style of a style image\u003c/li\u003e\n\u003cli\u003eImageNet\u003c/li\u003e\n\u003cli\u003eTake gradient of loss function with respect to the pixels \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/art_gen1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/art_gen2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/art_gen3.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"case-study-4-trigger-word-detection-alexa\"\u003eCase study 4: Trigger word detection (Alexa)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#case-study-4-trigger-word-detection-alexa\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eYour data collection strategy is critical to the success of your project. (If\napplicable) Don’t hesitate to get out of the building.\u003c/li\u003e\n\u003cli\u003eYou can gain insights on your labelling strategy by using a human experiment\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/trigger_word.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/data_collection.png\"\u003e\u003c/p\u003e\n\u003ch1 id=\"lecture-3---full-cycle-deep-learning-projects\"\u003eLecture 3 - Full-Cycle Deep Learning Projects\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-3---full-cycle-deep-learning-projects\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eSelect problem (e.g. supervised learning)\u003c/li\u003e\n\u003cli\u003eGet data (need to be very strategic in that)\u003c/li\u003e\n\u003cli\u003eDesign model\u003c/li\u003e\n\u003cli\u003eTrain model (iterative process with point 2 and 3)\u003c/li\u003e\n\u003cli\u003eTest model\u003c/li\u003e\n\u003cli\u003eDeploy\u003c/li\u003e\n\u003cli\u003eMaintain\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFive points when selecting a project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einterest\u003c/li\u003e\n\u003cli\u003edata availability\u003c/li\u003e\n\u003cli\u003edomain knowledge\u003c/li\u003e\n\u003cli\u003efeasibility\u003c/li\u003e\n\u003cli\u003eUsefulness\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFirst goal when starting to build a ML system is to get a baseline as fast as possible. Then iterate on that. That is steps 1-5 should be done within a couple of days.\u003c/p\u003e\n\u003cp\u003eML developement is an iterative process. Before you start working on it it is difficult to know what are the hard problems you need to tackle\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTips:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ekeep clear notes on experiments runs\u003c/li\u003e\n\u003cli\u003egenerally when you have to make choice between multiple options - go with the simpler one first\u003c/li\u003e\n\u003cli\u003ein real projects often data change over time. Non-ML models are more robust, ML models often need to be retrained.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eedge deployment vs cloud deployment\u003c/p\u003e\n\u003ch1 id=\"lecture-4---adversarial-attacks--gans\"\u003eLecture 4 - Adversarial Attacks / GANs\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lecture-4---adversarial-attacks--gans\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eDiscovery: Several ML models, including state-of-the-art neural networks are vulnerable to adversarial examples.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e[Szegedy et al. (2013): Intriguing properties of neural networks]\u003c/p\u003e\n\u003cp\u003e[Ian J. Goodfellow, Jonathon Shlens \u0026#x26; Christian Szegedy (2015): Explaining and harnessing adversarial examples]\u003c/p\u003e\n\u003ch2 id=\"attacking-a-network-with-adversarial-examples\"\u003eAttacking a network with adversarial examples\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#attacking-a-network-with-adversarial-examples\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWhat are examples of Adversarial attacks?\u003c/p\u003e\n\u003cp\u003eGiven a network pretrained on ImageNet, find an input image that is not a iguana but will be classified as iguana.\u003c/p\u003e\n\u003cp\u003eAttack:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStart with some image that is not iguana.\u003c/li\u003e\n\u003cli\u003eUse pretrained NN and output a vector of probabilities (cat, dog, iguana..)\u003c/li\u003e\n\u003cli\u003eCompute Loss.\u003c/li\u003e\n\u003cli\u003eTake gradient of loss wrt to the pixels. (backpropagation, need to have access to the model, parameters, layers etc.)\u003c/li\u003e\n\u003cli\u003eChange the image pixels.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/adversarial_examples.png\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can see that chaning with very little the pixels in the image you can fool the network that the cat is an iguana.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/space_images.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"defense-against-a-network-with-adversarial-examples\"\u003eDefense against a network with adversarial examples\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#defense-against-a-network-with-adversarial-examples\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdd a SafetyNet - one more NN that will detect adversarial examples. (but it can be fooled as well) [Yuan et al. (2017): Adversarial Examples: Attacks and Defenses for Deep Learning]\u003c/li\u003e\n\u003cli\u003eTrain on corrrectly labelled adversarial examples. Generate adversarial examples and train on them.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"why-are-nn-vulnarable-to-adversarial-examples\"\u003eWhy are NN vulnarable to adversarial examples?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#why-are-nn-vulnarable-to-adversarial-examples\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1712.07107.pdf\"\u003eAdversarial Examples: Attacks and Defenses for Deep Learning\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou'd think that NN are vulnaraalbe because they are too complex and overfit the data. But it turns out that the linearity part of the network is the problem.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey = xw+b\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eIt is easy to create \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmo lspace=\"0em\" rspace=\"0em\"\u003e∗\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eϵ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex^{*}=x+\\epsilon\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6887em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6887em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e∗\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eϵ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e which would produce massively different y.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFast Gradient Sign Method\u003c/strong\u003e is a method to create quickly adversarial examples.\u003c/p\u003e\n\u003ch2 id=\"gan---generative-adversarial-networks\"\u003eGAN - Generative Adversarial Networks\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gan---generative-adversarial-networks\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe word adversarial is used in different meaning here. In GANs it means that there are two networks that are competing with each other.\u003c/p\u003e\n\u003cp\u003eGAN-s are networks that generate images that mimic the distribution of the real images/data.\u003c/p\u003e\n\u003ch3 id=\"gd-game\"\u003eG/D game\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#gd-game\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eGenerator (is what we want to train and generate fake images),\u003c/p\u003e\n\u003cp\u003eDisriminator\u003c/p\u003e\n\u003cp\u003eG wants to fool D. D catches fake images. In the beginning it would be very good at catching fake images. But G will learn to generate better images. G and D are trained together.\u003c/p\u003e\n\u003cp\u003eMin-max trick of chaning the loss function\u003c/p\u003e\n\u003cp\u003eSaturating cost vs non-saturating cost \u003c/p\u003e\n\u003cp\u003eIf D does not improve G cannot improve. You can see D as an upper bound on G.\u003c/p\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003efor\u003c/span\u003e num_iterations\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003efor\u003c/span\u003e k \u003cspan class=\"token keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"token builtin\"\u003erange\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ek_steps\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n        update D\n    update G\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTips for training GAN-s:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emodify loss function\u003c/li\u003e\n\u003cli\u003ekeep D up-to-date with respect to G (k update for G and 1 update for D)\u003c/li\u003e\n\u003cli\u003eVirtual Batchnorm\u003c/li\u003e\n\u003cli\u003eone-sided label smoothing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCycleGAN\u003c/p\u003e\n\u003cp\u003eimage with horses, generate same image with zebra.\u003c/p\u003e\n\u003cp\u003eLoss function contains all losses from normal gans + cycle loss. \u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/cycle_gan.png\"\u003e\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"6eb6qm06u6vpwbpxckyzkem"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>