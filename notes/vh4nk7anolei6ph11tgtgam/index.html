<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>2025-04-15</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="2025-04-15"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/vh4nk7anolei6ph11tgtgam/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="4/15/2025"/><meta property="article:modified_time" content="4/16/2025"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/vh4nk7anolei6ph11tgtgam/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-28855ac016325484.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="2025-04-15">2025-04-15<a aria-hidden="true" class="anchor-heading icon-link" href="#2025-04-15"></a></h1>
<h1 id="ai-industry-developments">AI Industry Developments<a aria-hidden="true" class="anchor-heading icon-link" href="#ai-industry-developments"></a></h1>
<p><a class="color-tag" style="--tag-color: #fff4f2;" href="/dendron-wiki/notes/jcbadkzec97t7sa62s5q49c">#AI</a></p>
<ul>
<li>Most LLMs are really just a fancy autocomplete. Given a set of words (tokens)
they are able to predict what the next word most likely is. They use billion of parameters to do this.</li>
<li>In 2022 LLM practitioners, realized that if they ask the LLM to show its thought process, and use it as prompts, LLMs
can do much better in reasoning tasks <strong>chain of thought prompting</strong>.</li>
<li>Open AI o1 wrapped this idea into a model and this model showed much better reasoning capabilities. <strong>chain of thoughts (CoTs)</strong>.
This new “reasoning model” goes through extensive internal reasoning in order to decompose problems, brainstorm ideas, check its work, iterate, etc.  Think of it like an internal monologue.</li>
<li>[<strong>IMPORTANT</strong>] Reasoning models are best when we have verifiable tasks. That is why they are good for coding since we can verify if a code is correct by writing tests.</li>
<li>One day LLMs reasoning models will be good at solving math problems too. Solutions are currently verifiable only by human, but once we are able to automate this process then models trained with chain of thoughts idea + self-verification will be able to solve hard math problems.</li>
</ul>
<h2 id="reasoning-models">Reasoning Models<a aria-hidden="true" class="anchor-heading icon-link" href="#reasoning-models"></a></h2>
<p>As of today best reasoning models are o3 (OpenAI), Sonnet 3.7 (Anthropic backed by Amazon)</p>
<p>Deep Research </p>
<p>Operators, Computer-Using Agent. This is fucking awesome. It opens the browser and starts looking at the screen, moves the mouse and clicks...</p>
<p>AI Agent, Assistant, Bot</p>
<ul>
<li>Agents perform complex operations, can reason on their own, and proactively make decisions. It has a "self" and is able to "think". Examples: Operators and Deep Research Products</li>
<li>Assistants assist human when working on tasks. Best example is the vanilla chatGPT, VsCode Co-pilot</li>
<li>Bot perform simple repetitive tasks, web scrappers, autocomplete, dial-call operators, classic call center support automation</li>
</ul>
<p><a href="https://openai.com/index/learning-to-reason-with-llms/">o1 release, CoT</a></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ai-industry-developments" title="AI Industry Developments">AI Industry Developments</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#reasoning-models" title="Reasoning Models">Reasoning Models</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"vh4nk7anolei6ph11tgtgam","title":"2025-04-15","desc":"","updated":1744836760355,"created":1744712302323,"traitIds":["journalNote"],"custom":{},"fname":"daily.journal.2025.04.15","type":"note","vault":{"fsPath":"vault"},"contentHash":"b50dcdf1606ee3ad2aac88c6660fb727","links":[{"type":"wiki","from":{"fname":"daily.journal.2025.04.15","id":"vh4nk7anolei6ph11tgtgam","vaultName":"vault"},"value":"tags.AI","alias":"#AI","position":{"start":{"line":3,"column":1,"offset":28},"end":{"line":3,"column":4,"offset":31},"indent":[]},"xvault":false,"to":{"fname":"tags.AI"}}],"anchors":{"ai-industry-developments":{"type":"header","text":"AI Industry Developments","value":"ai-industry-developments","line":9,"column":0,"depth":1},"reasoning-models":{"type":"header","text":"Reasoning Models","value":"reasoning-models","line":22,"column":0,"depth":2}},"children":[],"parent":"qygeaemip9zraaj3bxufhxz","data":{}},"body":"\u003ch1 id=\"2025-04-15\"\u003e2025-04-15\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#2025-04-15\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"ai-industry-developments\"\u003eAI Industry Developments\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ai-industry-developments\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca class=\"color-tag\" style=\"--tag-color: #fff4f2;\" href=\"/dendron-wiki/notes/jcbadkzec97t7sa62s5q49c\"\u003e#AI\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMost LLMs are really just a fancy autocomplete. Given a set of words (tokens)\nthey are able to predict what the next word most likely is. They use billion of parameters to do this.\u003c/li\u003e\n\u003cli\u003eIn 2022 LLM practitioners, realized that if they ask the LLM to show its thought process, and use it as prompts, LLMs\ncan do much better in reasoning tasks \u003cstrong\u003echain of thought prompting\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eOpen AI o1 wrapped this idea into a model and this model showed much better reasoning capabilities. \u003cstrong\u003echain of thoughts (CoTs)\u003c/strong\u003e.\nThis new “reasoning model” goes through extensive internal reasoning in order to decompose problems, brainstorm ideas, check its work, iterate, etc.  Think of it like an internal monologue.\u003c/li\u003e\n\u003cli\u003e[\u003cstrong\u003eIMPORTANT\u003c/strong\u003e] Reasoning models are best when we have verifiable tasks. That is why they are good for coding since we can verify if a code is correct by writing tests.\u003c/li\u003e\n\u003cli\u003eOne day LLMs reasoning models will be good at solving math problems too. Solutions are currently verifiable only by human, but once we are able to automate this process then models trained with chain of thoughts idea + self-verification will be able to solve hard math problems.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"reasoning-models\"\u003eReasoning Models\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reasoning-models\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAs of today best reasoning models are o3 (OpenAI), Sonnet 3.7 (Anthropic backed by Amazon)\u003c/p\u003e\n\u003cp\u003eDeep Research \u003c/p\u003e\n\u003cp\u003eOperators, Computer-Using Agent. This is fucking awesome. It opens the browser and starts looking at the screen, moves the mouse and clicks...\u003c/p\u003e\n\u003cp\u003eAI Agent, Assistant, Bot\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAgents perform complex operations, can reason on their own, and proactively make decisions. It has a \"self\" and is able to \"think\". Examples: Operators and Deep Research Products\u003c/li\u003e\n\u003cli\u003eAssistants assist human when working on tasks. Best example is the vanilla chatGPT, VsCode Co-pilot\u003c/li\u003e\n\u003cli\u003eBot perform simple repetitive tasks, web scrappers, autocomplete, dial-call operators, classic call center support automation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://openai.com/index/learning-to-reason-with-llms/\"\u003eo1 release, CoT\u003c/a\u003e\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1744237493819,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"ea936af1aea818f3052610daac103a63","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","v77wdzobzcackzimz6a7crv"],"parent":null,"data":{},"body":"\nWelcome to my Knowledge Base! Here I write about my perception of life, document exciting things I've learned, debate (with myself) on controversial topics. If you know me you will not be surprised to find out that I write mostly about engineering and maths. Other topics I'm interested in are economics, politics, business, chess and poker."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"vh4nk7anolei6ph11tgtgam"},"buildId":"hUMuUtBSCkU7YSeQn-rle","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>