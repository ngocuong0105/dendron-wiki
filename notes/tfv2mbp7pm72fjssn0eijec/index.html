<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Imbalanced dataset</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Imbalanced dataset"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/tfv2mbp7pm72fjssn0eijec/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="4/5/2023"/><meta property="article:modified_time" content="5/17/2023"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/tfv2mbp7pm72fjssn0eijec/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="imbalanced-dataset">Imbalanced dataset<a aria-hidden="true" class="anchor-heading icon-link" href="#imbalanced-dataset"></a></h1>
<h1 id="implications">Implications<a aria-hidden="true" class="anchor-heading icon-link" href="#implications"></a></h1>
<ol>
<li>If the train and test sets are both imbalanced with the same imbalance ratio, you do not need to balance your train data.</li>
<li>If the train set is imbalanced and the test set is balanced, balancing your train data using oversampling would improve your model.</li>
<li>Undersample the majority class when working with an imbalanced dataset only if you want to train faster. Bear in mind that your model score would decrease.</li>
<li>If you decide to change the balance ratio of your train set, then MAE and MSE errors would be almost constant, whereas F1 scores, precision, and recall can vary up to 2%.</li>
<li>Use F1 score <strong>macro-averaged</strong> to give equal weight to all classes in your data.</li>
<li>The default F1 score metric in scikit-learn gives big weight to the majority class, resulting in overoptimistic predictions when working with an imbalanced dataset.</li>
</ol>
<p>Common pitfalls</p>
<ul>
<li>Never test on the oversampled or undersampled dataset.</li>
<li>If we want to implement cross validation, remember to oversample or undersample your training data during cross-validation, not before!</li>
<li>Don't use <strong>accuracy score</strong> as a metric with imbalanced datasets (will be usually high and misleading), instead use f1-score, precision/recall score or confusion matrix</li>
</ul>
<p>Imbalanced dataset techniques:</p>
<ul>
<li>random under-sampling = get small subsample of the overrepresented class. Main risk is of great infromation loss.</li>
<li>SMOTE (synthetic minority over-sampling technique)</li>
<li>cost-sensitive loss function</li>
</ul>
<h1 id="what-is-the-imbalanced-dataset-problem">What is the imbalanced dataset problem?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-the-imbalanced-dataset-problem"></a></h1>
<p>Without loss of generality, we assume that the minority or rare class is the positive class, and the majority class is the negative class. If we apply most traditional (cost-insensitive) classifiers on the dataset, they will likely to predict everything as negative (the majority class). This was often regarded as a problem in learning from highly imbalanced datasets.</p>
<p>Creating a traditional-cost-insensitive classifier has two assumptions:</p>
<ul>
<li>goal is to maximize accuracy/minimize loss</li>
<li>train and test datasets have the same distributions</li>
</ul>
<p><strong>Under these assumtions, predicting everything negative is the correct thing to do.</strong></p>
<p>Thus, the imbalanced class problem becomes meaningful only if one or both of the two assumptions above are not true:</p>
<ul>
<li>the cost of different types of error is not the same/missclasification cost is not equal</li>
<li>the train and test data are different</li>
</ul>
<h2 id="when-changing-the-cost-function-works">When changing the cost-function works<a aria-hidden="true" class="anchor-heading icon-link" href="#when-changing-the-cost-function-works"></a></h2>
<p>In the case when the misclassification cost is not equal, it is usually more expensive to misclassify a minority (positive) example into the majority (negative) class, than a majority example into the minority class (otherwise it is more plausible to predict everything as negative). That is, FN > FP. In this case you should have a loss function which penalizes FN more than FP.</p>
<h2 id="when-overundersampling-works">When over/undersampling works<a aria-hidden="true" class="anchor-heading icon-link" href="#when-overundersampling-works"></a></h2>
<p>In case the class distributions of training and test datasets are different (for example, if the training data is highly imbalanced but the test data is more balanced), an obvious approach is to sample the training data such that its class distribution is the same as the test data (by oversampling the minority class and/or undersampling the majority class).</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#implications" title="Implications">Implications</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-is-the-imbalanced-dataset-problem" title="What is the imbalanced dataset problem?">What is the imbalanced dataset problem?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#when-changing-the-cost-function-works" title="When changing the cost-function works">When changing the cost-function works</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#when-overundersampling-works" title="When over/undersampling works">When over/undersampling works</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"tfv2mbp7pm72fjssn0eijec","title":"Imbalanced dataset","desc":"","updated":1684317942478,"created":1680683206762,"custom":{},"fname":"machine learning.Imbalanced dataset","type":"note","vault":{"fsPath":"vault"},"contentHash":"b82582d7368c8334cd4672f7106a21af","links":[],"anchors":{"implications":{"type":"header","text":"Implications","value":"implications","line":8,"column":0,"depth":1},"what-is-the-imbalanced-dataset-problem":{"type":"header","text":"What is the imbalanced dataset problem?","value":"what-is-the-imbalanced-dataset-problem","line":30,"column":0,"depth":1},"when-changing-the-cost-function-works":{"type":"header","text":"When changing the cost-function works","value":"when-changing-the-cost-function-works","line":44,"column":0,"depth":2},"when-overundersampling-works":{"type":"header","text":"When over/undersampling works","value":"when-overundersampling-works","line":48,"column":0,"depth":2}},"children":[],"parent":"2b5bwf46z6v132wu7xghvrp","data":{}},"body":"\u003ch1 id=\"imbalanced-dataset\"\u003eImbalanced dataset\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#imbalanced-dataset\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"implications\"\u003eImplications\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#implications\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eIf the train and test sets are both imbalanced with the same imbalance ratio, you do not need to balance your train data.\u003c/li\u003e\n\u003cli\u003eIf the train set is imbalanced and the test set is balanced, balancing your train data using oversampling would improve your model.\u003c/li\u003e\n\u003cli\u003eUndersample the majority class when working with an imbalanced dataset only if you want to train faster. Bear in mind that your model score would decrease.\u003c/li\u003e\n\u003cli\u003eIf you decide to change the balance ratio of your train set, then MAE and MSE errors would be almost constant, whereas F1 scores, precision, and recall can vary up to 2%.\u003c/li\u003e\n\u003cli\u003eUse F1 score \u003cstrong\u003emacro-averaged\u003c/strong\u003e to give equal weight to all classes in your data.\u003c/li\u003e\n\u003cli\u003eThe default F1 score metric in scikit-learn gives big weight to the majority class, resulting in overoptimistic predictions when working with an imbalanced dataset.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCommon pitfalls\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNever test on the oversampled or undersampled dataset.\u003c/li\u003e\n\u003cli\u003eIf we want to implement cross validation, remember to oversample or undersample your training data during cross-validation, not before!\u003c/li\u003e\n\u003cli\u003eDon't use \u003cstrong\u003eaccuracy score\u003c/strong\u003e as a metric with imbalanced datasets (will be usually high and misleading), instead use f1-score, precision/recall score or confusion matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImbalanced dataset techniques:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erandom under-sampling = get small subsample of the overrepresented class. Main risk is of great infromation loss.\u003c/li\u003e\n\u003cli\u003eSMOTE (synthetic minority over-sampling technique)\u003c/li\u003e\n\u003cli\u003ecost-sensitive loss function\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"what-is-the-imbalanced-dataset-problem\"\u003eWhat is the imbalanced dataset problem?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-is-the-imbalanced-dataset-problem\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eWithout loss of generality, we assume that the minority or rare class is the positive class, and the majority class is the negative class. If we apply most traditional (cost-insensitive) classifiers on the dataset, they will likely to predict everything as negative (the majority class). This was often regarded as a problem in learning from highly imbalanced datasets.\u003c/p\u003e\n\u003cp\u003eCreating a traditional-cost-insensitive classifier has two assumptions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egoal is to maximize accuracy/minimize loss\u003c/li\u003e\n\u003cli\u003etrain and test datasets have the same distributions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUnder these assumtions, predicting everything negative is the correct thing to do.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThus, the imbalanced class problem becomes meaningful only if one or both of the two assumptions above are not true:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe cost of different types of error is not the same/missclasification cost is not equal\u003c/li\u003e\n\u003cli\u003ethe train and test data are different\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"when-changing-the-cost-function-works\"\u003eWhen changing the cost-function works\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#when-changing-the-cost-function-works\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn the case when the misclassification cost is not equal, it is usually more expensive to misclassify a minority (positive) example into the majority (negative) class, than a majority example into the minority class (otherwise it is more plausible to predict everything as negative). That is, FN \u003e FP. In this case you should have a loss function which penalizes FN more than FP.\u003c/p\u003e\n\u003ch2 id=\"when-overundersampling-works\"\u003eWhen over/undersampling works\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#when-overundersampling-works\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn case the class distributions of training and test datasets are different (for example, if the training data is highly imbalanced but the test data is more balanced), an obvious approach is to sample the training data such that its class distribution is the same as the test data (by oversampling the minority class and/or undersampling the majority class).\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"tfv2mbp7pm72fjssn0eijec"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>