<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Kaggle Tricks</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Kaggle Tricks"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/ltvbyf1oqc8qx7khy58pby2/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="3/4/2023"/><meta property="article:modified_time" content="11/16/2025"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/ltvbyf1oqc8qx7khy58pby2/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="kaggle-tricks">Kaggle Tricks<a aria-hidden="true" class="anchor-heading icon-link" href="#kaggle-tricks"></a></h1>
<p><a href="https://www.kaggle.com/learn">Hands-on course in Kaggle</a></p>
<h1 id="all-is-in-the-right-scaling">All is in the right scaling<a aria-hidden="true" class="anchor-heading icon-link" href="#all-is-in-the-right-scaling"></a></h1>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># Make reproducible</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>

<span class="token comment"># Generate data</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">5</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> size<span class="token operator">=</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment"># Add outlier</span>
y<span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">30</span>   <span class="token comment"># Outlier at index 50</span>

plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Original'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<p><strong>Can you catch the outlier?</strong></p>
<p><img src="/dendron-wiki/./assets/images/catch_out.png" alt="alt text"></p>
<p><strong>Same as above but with  np.log(y)</strong></p>
<p><img src="/dendron-wiki/./assets/images/catched_out.png" alt="alt text"></p>
<h1 id="videos">Videos<a aria-hidden="true" class="anchor-heading icon-link" href="#videos"></a></h1>
<p><a href="https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/372629">Kaggle Grandmaster Interviews</a></p>
<hr>
<p><a href="https://www.youtube.com/watch?v=QGCvycOXs2M">Chris Deotte interview</a></p>
<p>There is alwasy a data property, barrier - breakthrough in the competition which makes the difference in the leaderboard. Find a feature which could change your modelling approach.</p>
<p>To do very very good you need to make a breakthrough. To have a breakthrough the secret is in the EDA. Make lots of plots, check correlation between things and eventually you get an intuition. It is all about the data.</p>
<p>Key components in Kaggle competitions:</p>
<ul>
<li>have a fast pipeline where you can run experiments (need to set a good local validation)</li>
<li>RIGID EDA, to get intuition of the data</li>
</ul>
<p>See something in the plots and you add a feature, split your model, modify the feature.</p>
<hr>
<p><a href="https://www.youtube.com/watch?v=gdyuCqmLMUg&#x26;t=944">Beyond Feature Selection</a></p>
<p><strong>Is more features better? should I just get more and more features?</strong></p>
<p>More features is generally better. As long as your features bring novel information it is good.</p>
<p>Decision trees have built in Feature Selection. for each split you choose the best split - it is a feature selection algorithm.</p>
<p><strong>So do we really need FS when training Decision Trees (LGBM, XGBOOST)</strong>. The answer is yes. You still need. It does happen your model on the sample data might pick up noisy feature. So you still need FS.</p>
<p>Feature selection can reduce the number of features and you can get your AUC/Score up by a few points.</p>
<p><code>scikit-learn</code></p>
<ul>
<li>Univariate analysis: 
<ul>
<li>variance threshold</li>
<li>F-test (ANOVA)</li>
<li>Forward selection in simple Ridge regression</li>
</ul>
</li>
<li>RFE (recursive feature elimination) a Kaggler says it works well</li>
</ul>
<p>There is some bias in the feature selection. If you do FS for Xgboost, then the features you et woud be good for the xgboost but might not be for Neaural Nets</p>
<p><code>Beyond scikit-learn</code></p>
<ul>
<li>Boruta
<ul>
<li>Wrapper</li>
<li>Loss based</li>
<li>Subset</li>
</ul>
</li>
<li>ReliefF
<ul>
<li>Filter method</li>
<li>Distance based</li>
<li>Ranking</li>
</ul>
</li>
</ul>
<p><strong>Characteristics of Feature Selection Algorithms</strong></p>
<ul>
<li>Feature space
<ul>
<li>Univariate F-Score</li>
<li>Multivariate (RFE, L1-based, Tree-based)</li>
</ul>
</li>
<li>Direction
<ul>
<li>Forward</li>
<li>Backward</li>
<li>Ranking: F-score, L1-based</li>
</ul>
</li>
</ul>
<p><strong>Most of the time in Kaggle competitons I spent 90% of the time on FS, FE, prepping the data.</strong></p>
<p><strong>Feature engineering is usually better that Feature Selection</strong></p>
<hr>
<p><a href="https://www.youtube.com/watch?v=VC8Jc9_lNoY&#x26;t=408s">CPMP talk</a></p>
<p>Target Engineering - transform the target and make predictions on it to match the competition metric.</p>
<p>SMAPE for example is assymetric metric. If you transfor the target with log, you get a metric which is almost like MAE.</p>
<p>Understand the problem that is being solved. Understand the metric. Understand the data.</p>
<p>How good a feature is depends on the competition metric!!! Feature importance, correlations, etc. are not that important.</p>
<p>Setup good Cross validation local setup. You CV score should correlate with the public LB.</p>
<p><strong>Track the Train-Test error GAP</strong> Large gap indicates overfit. When your CV score is increasing and the gap is not incrasing a lot, then you are not overfitting.</p>
<p>For xgboost/LGBM no need to worry about one hot encoding, or nan values. They handle it well.</p>
<hr>
<p><a href="https://www.slideshare.net/HJvanVeen/feature-engineering-72376750">Feature engineering deck</a></p>
<p>Categorical features:</p>
<ul>
<li>large cardinality creates sparse data</li>
<li>one hot encoding</li>
<li>hash encoding (deals with new varaibles, may introduce collisions), avoids extremely sparse data</li>
<li>label encoding (for every categorical variable give a unique numerical ID)</li>
<li>count encoding (sensitive to outliers, may add log transform, replace unseen varaibles with 1)</li>
<li>LabelCount encoding (rank categorical varaibles by count in the train set, no collisions)</li>
<li>target encoding - encode cat varaibles by their ratio of target (becareful to avoid overfit, nested CV)</li>
<li>use NN to create dense embeddings from categorical variables</li>
<li>give NaN valies an explicit encoding instead of ignoring  (use them only when train and test NANs are coused by the same, or your local CV proves it holds signal)</li>
<li>expansion encoding (create multiple categorical varaibles from a single variable)</li>
</ul>
<p>Numerical features:</p>
<ul>
<li>round numerical variables (form of lossy compression, retain most significant features of the data)</li>
<li>sometimes too much precision is just noise</li>
<li>binning (by quantiles, plot data to log for good interval binning)</li>
<li>scaling (standard Z, MinMax, Root, Log scaling)</li>
<li>impute missing (mean, median (robust to outliers), KNN,) add boolean column</li>
<li>interactions (substraction, addition, multiplication, divison) Ignore human intuition!, weird interactions can give significant improvement</li>
<li>create statistic on a row of data (number of NaNs, Number of 0s, Number of negative values, Mean, Max, Min, Skewness, etc)</li>
<li>temporal variables (dates, needs backtesting)</li>
<li>turn single features, like day_of_week into two coordinates on a circle</li>
<li>for TS, instead of total spend, encoed things like spend in last week, month, etc</li>
<li>hardcode categorical features like: date_3_days_before_holidays: 1</li>
<li>spacial variables (find closeness between big hubs)</li>
</ul>
<p>Neural Networks &#x26; DL</p>
<ul>
<li>NN claim end-to-end automatic feature engineering</li>
<li>FE dying field? No - moves the focus to architecture engineering</li>
<li>encode order of samples in dataset</li>
</ul>
<p><em>Applied Machine Learning is basically feature engineering, Andrew Ng</em></p>
<hr>
<p><a href="https://www.youtube.com/watch?v=XBJ2f68LuO4&#x26;t=38s">Winning gold is easy</a></p>
<p>A LOT OF TIPS AND TRICKS in this video.</p>
<p>If something does not work, does not stick to it.</p>
<p>to get a gold model you need to try crazy ideas. Most would not work, but if you get one good one you can get the gold.</p>
<p>Trick for feature engineering:</p>
<ul>
<li>add 1 noisy features</li>
<li>compute feature importance for all features</li>
<li>features which have higher rank than the noisy feature should generally be good features</li>
<li>if your noisy feature is too high in rank, your model is overfitting on noisy features and gives them high rank</li>
<li>you need to go through the top ranked features and see which of them your model is overfitting </li>
</ul>
<p><strong>Feature importance</strong></p>
<p>Detect overfitting features</p>
<ul>
<li>remove the feature and if CV is not hurt your are good</li>
<li>if removing the feature hurts CV, then use transformation on the feature</li>
</ul>
<p>Feature engineering is a bit of an art. Feature X and Feature Y might be good only if you use both (as they have interation).</p>
<p>If your model has many features (e.g xgboost) try using small colsample. Usually you would use 80-90%. But when having MANY features, this way your trees would be almost independent and not correlated.</p>
<p>Working with NN or linear models, when a feature you have NAN, add additional boolean column.</p>
<p>NN are making feature engineering implicitly.</p>
<hr>
<p><a href="https://www.youtube.com/watch?v=RtqtM1UJfZc&#x26;t">Giba talk, tips for feature engineering and selection</a></p>
<hr>
<p><a href="https://www.youtube.com/watch?v=A8oBphPOliM">Kaggle Grandmaster and startup</a></p>
<ul>
<li>CPU/RAM management is 1st priority</li>
<li>Work on a single model as long as you can (1 week for other models)</li>
<li>ML is just function approximation of the real world</li>
<li>Tree methods ARE sensitive to noisy/useless features (spend enough time in Feature selection...)</li>
<li>In kaggle you do not always have CV LB correlation (you have absolute noise in the LB, or the dataset is very small)</li>
<li>Every time the dataset is small - you can do 5 Fold CV, repeated with 20 different CV splits. Models bagged 20 times - 2000 models in total to train just 1 model.</li>
</ul>
<hr>
<p><a href="https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/394975">When averaging models in forecasting tasks help</a></p>
<h1 id="lb-cv-scores">LB CV Scores<a aria-hidden="true" class="anchor-heading icon-link" href="#lb-cv-scores"></a></h1>
<ul>
<li>Compute correlation between LB and CV scores!</li>
<li>Local CV setup to be as close as possible to the Kaggle API.</li>
</ul>
<h1 id="model-augmentation">Model augmentation<a aria-hidden="true" class="anchor-heading icon-link" href="#model-augmentation"></a></h1>
<ul>
<li>Bagging
<ul>
<li>same model trained on different folds</li>
<li>same model trained on different seeds (if data is very noisy)</li>
<li>same model trained on different features</li>
<li>same model trained on different hyperparameters</li>
<li>different models take weighted average/mean (careful for MAE usually it hurts)</li>
</ul>
</li>
<li>Stacking</li>
<li>Boosting: train on the errors</li>
<li>Use output of one model to be features of another model</li>
<li>Detrending</li>
</ul>
<h1 id="powerful-features">Powerful features<a aria-hidden="true" class="anchor-heading icon-link" href="#powerful-features"></a></h1>
<ul>
<li>
<p>unbiased feature
<code>featureX - groupby(FeatureY)[FeatureX].mean()</code></p>
</li>
<li>
<p>previous FCT error feature (difference between FCT and TARGET), kind of boosting method</p>
</li>
<li>
<p>rolling aggregates - mean, median, min, max, quantiles, std, kurtosis, skew</p>
</li>
<li>
<p>intercept and slope instead of mean aggregate, that is run linear regression on past samples within a window and put the intercept and slope as features</p>
</li>
<li>
<p>random feature trick</p>
</li>
<li>
<p>neighbors_target_mean_500: The mean TARGET value of the 500 closest neighbors of each row</p>
</li>
</ul>
<h1 id="adversarial-validation">Adversarial Validation<a aria-hidden="true" class="anchor-heading icon-link" href="#adversarial-validation"></a></h1>
<p>Adversarial Validation is a very clever and very simple way to let us know if our test data and our training data are similar; we combine our train and test data, labeling them with say a 0 for the training data and a 1 for the test data, mix them up, then see if we are able to correctly re-identify them using a binary classifier.</p>
<p>If we cannot correctly classify them, i.e. we obtain an area under the receiver operating characteristic curve (ROC) of 0.5 then they are indistinguishable and we are good to go.</p>
<p>However, if we can classify them (ROC > 0.5) then we have a problem, either with the whole dataset or more likely with some features in particular, which are probably from different distributions in the test and train datasets. If we have a problem, we can look at the feature that was most out of place. The problem may be that there were values that were only seen in, say, training data, but not in the test data. If the contribution to the ROC is very high from one feature, it may well be a good idea to remove that feature from the model.</p>
<p><a href="https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation">kaggle post</a></p>
<h1 id="check-for-covariate-shift-in-train-and-test">Check for covariate shift in train and test<a aria-hidden="true" class="anchor-heading icon-link" href="#check-for-covariate-shift-in-train-and-test"></a></h1>
<ul>
<li>adversarial validation</li>
<li>KS test for the features to see if they are from the same distribution</li>
</ul>
<h1 id="check-for-target-shift-in-train-and-test">Check for target shift in train and test<a aria-hidden="true" class="anchor-heading icon-link" href="#check-for-target-shift-in-train-and-test"></a></h1>
<p>Good to understand th dynamics of the target variable. If the target variable is changing over time, then you need to be careful with the train/test split. You need to make sure the train and test are from the same time period.</p>
<ul>
<li>KS Test</li>
<li>T-test for mean shift</li>
</ul>
<h1 id="hypothesis-testing--normality-tests">Hypothesis Testing / Normality tests<a aria-hidden="true" class="anchor-heading icon-link" href="#hypothesis-testing--normality-tests"></a></h1>
<p>I think KS and normality tests are very sensistive for large datasets. If you have 10k+ observations, then you would get a very low p-value for any test. So you need to be careful with the p-value.</p>
<p>Any small shifts would be detected and would reject the null hypothesis.</p>
<p>So these tests are useful for small sample sizes. For large sample sizes you'd use the CLT and the t-test.</p>
<h1 id="kolmogorov-smirnov-test">Kolmogorov-Smirnov test<a aria-hidden="true" class="anchor-heading icon-link" href="#kolmogorov-smirnov-test"></a></h1>
<ul>
<li>The two sample Kolmogorov-Smirnov test is a  nonparametric test that compares the cumulative distributions of two data sets(1,2).</li>
<li>The test is nonparametric. It does not assume that data are sampled from Gaussian distributions (or any other defined distributions).</li>
</ul>
<h2 id="one-sample-kolmogorov-smirnov-test">One sample Kolmogorov-Smirnov test<a aria-hidden="true" class="anchor-heading icon-link" href="#one-sample-kolmogorov-smirnov-test"></a></h2>
<p>It takes the difference between the empirical distribution of the sample and the theoretical distribution. Looks at the supremum which should converge to 0 almost surely.</p>
<h2 id="two-sample-kolmogorov-smirnov-test">Two sample Kolmogorov-Smirnov test<a aria-hidden="true" class="anchor-heading icon-link" href="#two-sample-kolmogorov-smirnov-test"></a></h2>
<p>It takes the difference between the empirical distributions of two samples. Looks at the supremum which should converge to 0 almost surely.</p>
<p>Kolmogorov distribution shows the rate of this convergence.</p>
<p><strong>In practice, the statistic requires a relatively large number of data points to properly reject the null hypothesis.</strong></p>
<p>1k + points should be ok with error less than 1% (see wiki page).</p>
<ul>
<li>caveat is that this test is very sensistive, if the median, varaince,shape changes it would have low p-value</li>
</ul>
<p><strong>Interpreting the P value</strong></p>
<p>The P value is the answer to this question:</p>
<p>If the two samples were randomly sampled from identical populations, what is the probability that the two cumulative frequency distributions would be as far apart as observed? More precisely, what is the chance that the value of the Komogorov-Smirnov D statistic would be as large or larger than observed?</p>
<p>If the P value is small, conclude that the two groups were sampled from populations with different distributions. The populations may differ in median, variability or the shape of the distribution. </p>
<h2 id="t-test">T-test<a aria-hidden="true" class="anchor-heading icon-link" href="#t-test"></a></h2>
<ul>
<li>parametric method, used when the samples satisfy the conditions of normality, equal variance and independence.</li>
</ul>
<p>This test assumes that the populations have identical variances by default.</p>
<h2 id="shapirowilk-test">Shapiro–Wilk test<a aria-hidden="true" class="anchor-heading icon-link" href="#shapirowilk-test"></a></h2>
<p>Normality test</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> shapiro
</code></pre>
<p>The null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed.</p>
<ul>
<li>Better than KS one sample test as there you need to know the parameters of the distribution i.e. mean and variance. Cannot standaradize the data, as you would be going into a circle. you can't standardize by using estimated parameters and test for standard normal; that's actually the same thing.</li>
<li>good with small sample sizes ~ 50 observations</li>
<li>has more power compared to other normality tests, meaning it can detect deviations from normality more effectively.</li>
<li>Not Sensitive to Outliers</li>
</ul>
<h1 id="likelihood-ratio-test">Likelihood-ratio test<a aria-hidden="true" class="anchor-heading icon-link" href="#likelihood-ratio-test"></a></h1>
<h1 id="change-point-detection-in-time-series">Change point detection in Time Series<a aria-hidden="true" class="anchor-heading icon-link" href="#change-point-detection-in-time-series"></a></h1>
<p>Change point detection marks the locations where the underlying properties (statistical characteristics e.g. mean and variance) of the time series shift abruptly.</p>
<ul>
<li>target engineering and prediction switching</li>
<li>target splitting, find its components</li>
</ul>
<h1 id="lgbm-xgbm--catboost">LGBM, XGBM , CATBOOST<a aria-hidden="true" class="anchor-heading icon-link" href="#lgbm-xgbm--catboost"></a></h1>
<ul>
<li>important hyperparams are usually: learning_rate, num_iterations, colsample, sample, max_depth, n_leaves, early_stopping,  device type</li>
<li>feature importance: use num_splits to select features, used as a guide for useful and overfitting features</li>
<li>plot correlation between features</li>
<li>get_split_value_histogram for each feature (lgbm): do that for the most important features</li>
<li>get_leaf_output</li>
<li>feature binning of continuous features (max_bin)</li>
</ul>
<h1 id="feature-selection">Feature Selection<a aria-hidden="true" class="anchor-heading icon-link" href="#feature-selection"></a></h1>
<p>Obviously you should only do this if it helps your CV.</p>
<p><strong>All tricks by Chris Deotte:</strong></p>
<ul>
<li>forward feature selection (using single or groups of features)</li>
<li>recursive feature elimination (using single or groups of features)</li>
<li>permutation importance</li>
<li>adversarial validation</li>
<li>correlation analysis</li>
<li>time consistency</li>
<li>train/test distribution analysis</li>
<li>random feature trick</li>
<li>remove low variance features</li>
</ul>
<p>One interesting trick called "time consistency" is to train a single model using a single feature (or small group of features) on the first month of train dataset and predict isFraud for the last month of train dataset. This evaluates whether a feature by itself is consistent over time. 95% were but we found 5% of columns hurt our models. They had training AUC around 0.60 and validation AUC 0.40. In other words some features found patterns in the present that did not exist in the future. Of course the possible of interactions complicates things but we double checked every test with other tests.</p>
<p>Whatever feature selection you do <strong>you should separete feature selection from model training.</strong> This avoid data leakage. First do model selection and then model training.</p>
<p><strong>Wrapper methods</strong>: these are model dependent</p>
<ul>
<li>forward selection</li>
<li>backward selection</li>
<li>recursive feature elimination (greedy optimization)</li>
</ul>
<p><strong>Filter methods</strong>: these are model independent, and involve preprocessing</p>
<ul>
<li>Pearson correlation</li>
<li>LDA - linear discriminant analysis</li>
<li>ANOVA</li>
<li>Chi-Square</li>
</ul>
<p>These methods does not remove multicollinearity.</p>
<p><strong>Embedded methods</strong>: regularization</p>
<ul>
<li>Ridge, Lasso</li>
</ul>
<h2 id="recursive-feature-elimination">Recursive Feature Elimination<a aria-hidden="true" class="anchor-heading icon-link" href="#recursive-feature-elimination"></a></h2>
<ol>
<li>Rank the features according to their importance</li>
<li>Remove one/or more features - the least important - and build a machine learning algorithm utilizing the remaining features.</li>
<li>Calculate a performance metric of your choice</li>
<li>If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.
Can run permutation importance on held out test set <a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html">scikit-learn</a></li>
</ol>
<p>Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.</p>
<h2 id="mdi">MDI<a aria-hidden="true" class="anchor-heading icon-link" href="#mdi"></a></h2>
<p>Tree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Log Loss or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.</p>
<p>Furthermore, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.</p>
<p>This problem stems from two limitations of impurity-based feature importances:</p>
<ul>
<li>impurity-based importances are biased towards high cardinality features;</li>
<li>impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).</li>
</ul>
<h2 id="permutation-importance">Permutation importance<a aria-hidden="true" class="anchor-heading icon-link" href="#permutation-importance"></a></h2>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>inspection <span class="token keyword">import</span> permutation_importance
</code></pre>
<p>Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is <strong>for a particular model.</strong></p>
<p>We measure the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature. A feature is “important” if shuffling its values increases the model <strong>error</strong>, because in this case the model relied on the feature for the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. </p>
<p>To highlight which features contribute the most to the generalization power of the inspected model permutation importance should be computed on validation set.</p>
<p>Sklearn permutation_importance takes as input a fitted model. Thus based on the selected features/weights during training it computes the importance of the features. These features importance show how good are these features for this particular model. </p>
<p><strong>Another take on feature importance is: to do permutation importance before model training.</strong> This should be done manually (I don't know about a library)
Advatange is that is more acurate, disadvantage is that you loose a lot of time in training.</p>
<p><strong>NB</strong>
When two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.</p>
<p>One way to handle this is to cluster features that are correlated and only keep one feature from each cluster.</p>
<h2 id="model-class-reliance">Model Class Reliance<a aria-hidden="true" class="anchor-heading icon-link" href="#model-class-reliance"></a></h2>
<p><a href="https://arxiv.org/pdf/1801.01489.pdf">Paper</a></p>
<h1 id="dynamic-label-smoothing">Dynamic label smoothing<a aria-hidden="true" class="anchor-heading icon-link" href="#dynamic-label-smoothing"></a></h1>
<h1 id="throw-complex-models-on-high-weight-data">Throw complex models on high weight data<a aria-hidden="true" class="anchor-heading icon-link" href="#throw-complex-models-on-high-weight-data"></a></h1>
<h1 id="2-submisssions--bet-both-on-heads-and-tails">2 submisssions = bet both on heads and tails<a aria-hidden="true" class="anchor-heading icon-link" href="#2-submisssions--bet-both-on-heads-and-tails"></a></h1>
<p><a href="https://www.kaggle.com/competitions/jane-street-market-prediction/discussion/224079">tricks on Jane Street comp</a></p>
<h1 id="random-forecast">Random forecast<a aria-hidden="true" class="anchor-heading icon-link" href="#random-forecast"></a></h1>
<ul>
<li>produce ranomized forecasts multiple time. This is your baseline on what noise is in the competition.</li>
</ul>
<h1 id="correlation-screening">Correlation Screening<a aria-hidden="true" class="anchor-heading icon-link" href="#correlation-screening"></a></h1>
<ul>
<li>compute correlation of each feature with the target</li>
</ul>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/dendron-wiki/notes/2b5bwf46z6v132wu7xghvrp">Machine Learning</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#all-is-in-the-right-scaling" title="All is in the right scaling">All is in the right scaling</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#videos" title="Videos">Videos</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lb-cv-scores" title="LB CV Scores">LB CV Scores</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#model-augmentation" title="Model augmentation">Model augmentation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#powerful-features" title="Powerful features">Powerful features</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#adversarial-validation" title="Adversarial Validation">Adversarial Validation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#check-for-covariate-shift-in-train-and-test" title="Check for covariate shift in train and test">Check for covariate shift in train and test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#check-for-target-shift-in-train-and-test" title="Check for target shift in train and test">Check for target shift in train and test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#hypothesis-testing--normality-tests" title="Hypothesis Testing / Normality tests">Hypothesis Testing / Normality tests</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#kolmogorov-smirnov-test" title="Kolmogorov-Smirnov test">Kolmogorov-Smirnov test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#one-sample-kolmogorov-smirnov-test" title="One sample Kolmogorov-Smirnov test">One sample Kolmogorov-Smirnov test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#two-sample-kolmogorov-smirnov-test" title="Two sample Kolmogorov-Smirnov test">Two sample Kolmogorov-Smirnov test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#t-test" title="T-test">T-test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#shapirowilk-test" title="Shapiro–Wilk test">Shapiro–Wilk test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#likelihood-ratio-test" title="Likelihood-ratio test">Likelihood-ratio test</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#change-point-detection-in-time-series" title="Change point detection in Time Series">Change point detection in Time Series</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lgbm-xgbm--catboost" title="LGBM, XGBM , CATBOOST">LGBM, XGBM , CATBOOST</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#feature-selection" title="Feature Selection">Feature Selection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#recursive-feature-elimination" title="Recursive Feature Elimination">Recursive Feature Elimination</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mdi" title="MDI">MDI</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#permutation-importance" title="Permutation importance">Permutation importance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#model-class-reliance" title="Model Class Reliance">Model Class Reliance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dynamic-label-smoothing" title="Dynamic label smoothing">Dynamic label smoothing</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#throw-complex-models-on-high-weight-data" title="Throw complex models on high weight data">Throw complex models on high weight data</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#2-submisssions--bet-both-on-heads-and-tails" title="2 submisssions = bet both on heads and tails">2 submisssions = bet both on heads and tails</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#random-forecast" title="Random forecast">Random forecast</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#correlation-screening" title="Correlation Screening">Correlation Screening</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"ltvbyf1oqc8qx7khy58pby2","title":"Kaggle Tricks","desc":"","updated":1763279864416,"created":1677947097369,"custom":{},"fname":"machine learning.Kaggle Tricks","type":"note","vault":{"fsPath":"vault"},"contentHash":"7d52bdd4ea4709d22f231fc78ba07b03","links":[{"from":{"fname":"machine learning","id":"2b5bwf46z6v132wu7xghvrp","vaultName":"vault"},"type":"backlink","position":{"start":{"line":7,"column":3,"offset":186},"end":{"line":7,"column":37,"offset":220},"indent":[]},"value":"machine learning.Kaggle tricks"}],"anchors":{"all-is-in-the-right-scaling":{"type":"header","text":"All is in the right scaling","value":"all-is-in-the-right-scaling","line":12,"column":0,"depth":1},"videos":{"type":"header","text":"Videos","value":"videos","line":45,"column":0,"depth":1},"lb-cv-scores":{"type":"header","text":"LB CV Scores","value":"lb-cv-scores","line":219,"column":0,"depth":1},"model-augmentation":{"type":"header","text":"Model augmentation","value":"model-augmentation","line":224,"column":0,"depth":1},"powerful-features":{"type":"header","text":"Powerful features","value":"powerful-features","line":237,"column":0,"depth":1},"adversarial-validation":{"type":"header","text":"Adversarial Validation","value":"adversarial-validation","line":252,"column":0,"depth":1},"check-for-covariate-shift-in-train-and-test":{"type":"header","text":"Check for covariate shift in train and test","value":"check-for-covariate-shift-in-train-and-test","line":265,"column":0,"depth":1},"check-for-target-shift-in-train-and-test":{"type":"header","text":"Check for target shift in train and test","value":"check-for-target-shift-in-train-and-test","line":270,"column":0,"depth":1},"hypothesis-testing--normality-tests":{"type":"header","text":"Hypothesis Testing / Normality tests","value":"hypothesis-testing--normality-tests","line":278,"column":0,"depth":1},"kolmogorov-smirnov-test":{"type":"header","text":"Kolmogorov-Smirnov test","value":"kolmogorov-smirnov-test","line":287,"column":0,"depth":1},"one-sample-kolmogorov-smirnov-test":{"type":"header","text":"One sample Kolmogorov-Smirnov test","value":"one-sample-kolmogorov-smirnov-test","line":293,"column":0,"depth":2},"two-sample-kolmogorov-smirnov-test":{"type":"header","text":"Two sample Kolmogorov-Smirnov test","value":"two-sample-kolmogorov-smirnov-test","line":297,"column":0,"depth":2},"t-test":{"type":"header","text":"T-test","value":"t-test","line":319,"column":0,"depth":2},"shapirowilk-test":{"type":"header","text":"Shapiro–Wilk test","value":"shapirowilk-test","line":327,"column":0,"depth":2},"likelihood-ratio-test":{"type":"header","text":"Likelihood-ratio test","value":"likelihood-ratio-test","line":344,"column":0,"depth":1},"change-point-detection-in-time-series":{"type":"header","text":"Change point detection in Time Series","value":"change-point-detection-in-time-series","line":347,"column":0,"depth":1},"lgbm-xgbm--catboost":{"type":"header","text":"LGBM, XGBM , CATBOOST","value":"lgbm-xgbm--catboost","line":355,"column":0,"depth":1},"feature-selection":{"type":"header","text":"Feature Selection","value":"feature-selection","line":366,"column":0,"depth":1},"recursive-feature-elimination":{"type":"header","text":"Recursive Feature Elimination","value":"recursive-feature-elimination","line":405,"column":0,"depth":2},"mdi":{"type":"header","text":"MDI","value":"mdi","line":416,"column":0,"depth":2},"permutation-importance":{"type":"header","text":"Permutation importance","value":"permutation-importance","line":426,"column":0,"depth":2},"model-class-reliance":{"type":"header","text":"Model Class Reliance","value":"model-class-reliance","line":450,"column":0,"depth":2},"dynamic-label-smoothing":{"type":"header","text":"Dynamic label smoothing","value":"dynamic-label-smoothing","line":454,"column":0,"depth":1},"throw-complex-models-on-high-weight-data":{"type":"header","text":"Throw complex models on high weight data","value":"throw-complex-models-on-high-weight-data","line":456,"column":0,"depth":1},"2-submisssions--bet-both-on-heads-and-tails":{"type":"header","text":"2 submisssions = bet both on heads and tails","value":"2-submisssions--bet-both-on-heads-and-tails","line":458,"column":0,"depth":1},"random-forecast":{"type":"header","text":"Random forecast","value":"random-forecast","line":463,"column":0,"depth":1},"correlation-screening":{"type":"header","text":"Correlation Screening","value":"correlation-screening","line":467,"column":0,"depth":1}},"children":[],"parent":"2b5bwf46z6v132wu7xghvrp","data":{}},"body":"\u003ch1 id=\"kaggle-tricks\"\u003eKaggle Tricks\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#kaggle-tricks\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/learn\"\u003eHands-on course in Kaggle\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"all-is-in-the-right-scaling\"\u003eAll is in the right scaling\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#all-is-in-the-right-scaling\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e matplotlib\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epyplot \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"token comment\"\u003e# Make reproducible\u003c/span\u003e\nnp\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e42\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Generate data\u003c/span\u003e\nx \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinspace\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e100\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e100\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e \u003cspan class=\"token operator\"\u003e*\u003c/span\u003e x \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e5\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e size\u003cspan class=\"token operator\"\u003e=\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eshape\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Add outlier\u003c/span\u003e\ny\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e50\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+=\u003c/span\u003e \u003cspan class=\"token number\"\u003e30\u003c/span\u003e   \u003cspan class=\"token comment\"\u003e# Outlier at index 50\u003c/span\u003e\n\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efigure\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003efigsize\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003esubplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003escatter\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e alpha\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'data'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etitle\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Original'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003exlabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'x'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eylabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'y'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elegend\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCan you catch the outlier?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/catch_out.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSame as above but with  np.log(y)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/catched_out.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch1 id=\"videos\"\u003eVideos\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#videos\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/372629\"\u003eKaggle Grandmaster Interviews\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=QGCvycOXs2M\"\u003eChris Deotte interview\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThere is alwasy a data property, barrier - breakthrough in the competition which makes the difference in the leaderboard. Find a feature which could change your modelling approach.\u003c/p\u003e\n\u003cp\u003eTo do very very good you need to make a breakthrough. To have a breakthrough the secret is in the EDA. Make lots of plots, check correlation between things and eventually you get an intuition. It is all about the data.\u003c/p\u003e\n\u003cp\u003eKey components in Kaggle competitions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehave a fast pipeline where you can run experiments (need to set a good local validation)\u003c/li\u003e\n\u003cli\u003eRIGID EDA, to get intuition of the data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee something in the plots and you add a feature, split your model, modify the feature.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=gdyuCqmLMUg\u0026#x26;t=944\"\u003eBeyond Feature Selection\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIs more features better? should I just get more and more features?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMore features is generally better. As long as your features bring novel information it is good.\u003c/p\u003e\n\u003cp\u003eDecision trees have built in Feature Selection. for each split you choose the best split - it is a feature selection algorithm.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSo do we really need FS when training Decision Trees (LGBM, XGBOOST)\u003c/strong\u003e. The answer is yes. You still need. It does happen your model on the sample data might pick up noisy feature. So you still need FS.\u003c/p\u003e\n\u003cp\u003eFeature selection can reduce the number of features and you can get your AUC/Score up by a few points.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003escikit-learn\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnivariate analysis: \n\u003cul\u003e\n\u003cli\u003evariance threshold\u003c/li\u003e\n\u003cli\u003eF-test (ANOVA)\u003c/li\u003e\n\u003cli\u003eForward selection in simple Ridge regression\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRFE (recursive feature elimination) a Kaggler says it works well\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere is some bias in the feature selection. If you do FS for Xgboost, then the features you et woud be good for the xgboost but might not be for Neaural Nets\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eBeyond scikit-learn\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBoruta\n\u003cul\u003e\n\u003cli\u003eWrapper\u003c/li\u003e\n\u003cli\u003eLoss based\u003c/li\u003e\n\u003cli\u003eSubset\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eReliefF\n\u003cul\u003e\n\u003cli\u003eFilter method\u003c/li\u003e\n\u003cli\u003eDistance based\u003c/li\u003e\n\u003cli\u003eRanking\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCharacteristics of Feature Selection Algorithms\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFeature space\n\u003cul\u003e\n\u003cli\u003eUnivariate F-Score\u003c/li\u003e\n\u003cli\u003eMultivariate (RFE, L1-based, Tree-based)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDirection\n\u003cul\u003e\n\u003cli\u003eForward\u003c/li\u003e\n\u003cli\u003eBackward\u003c/li\u003e\n\u003cli\u003eRanking: F-score, L1-based\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMost of the time in Kaggle competitons I spent 90% of the time on FS, FE, prepping the data.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFeature engineering is usually better that Feature Selection\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=VC8Jc9_lNoY\u0026#x26;t=408s\"\u003eCPMP talk\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTarget Engineering - transform the target and make predictions on it to match the competition metric.\u003c/p\u003e\n\u003cp\u003eSMAPE for example is assymetric metric. If you transfor the target with log, you get a metric which is almost like MAE.\u003c/p\u003e\n\u003cp\u003eUnderstand the problem that is being solved. Understand the metric. Understand the data.\u003c/p\u003e\n\u003cp\u003eHow good a feature is depends on the competition metric!!! Feature importance, correlations, etc. are not that important.\u003c/p\u003e\n\u003cp\u003eSetup good Cross validation local setup. You CV score should correlate with the public LB.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTrack the Train-Test error GAP\u003c/strong\u003e Large gap indicates overfit. When your CV score is increasing and the gap is not incrasing a lot, then you are not overfitting.\u003c/p\u003e\n\u003cp\u003eFor xgboost/LGBM no need to worry about one hot encoding, or nan values. They handle it well.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.slideshare.net/HJvanVeen/feature-engineering-72376750\"\u003eFeature engineering deck\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCategorical features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elarge cardinality creates sparse data\u003c/li\u003e\n\u003cli\u003eone hot encoding\u003c/li\u003e\n\u003cli\u003ehash encoding (deals with new varaibles, may introduce collisions), avoids extremely sparse data\u003c/li\u003e\n\u003cli\u003elabel encoding (for every categorical variable give a unique numerical ID)\u003c/li\u003e\n\u003cli\u003ecount encoding (sensitive to outliers, may add log transform, replace unseen varaibles with 1)\u003c/li\u003e\n\u003cli\u003eLabelCount encoding (rank categorical varaibles by count in the train set, no collisions)\u003c/li\u003e\n\u003cli\u003etarget encoding - encode cat varaibles by their ratio of target (becareful to avoid overfit, nested CV)\u003c/li\u003e\n\u003cli\u003euse NN to create dense embeddings from categorical variables\u003c/li\u003e\n\u003cli\u003egive NaN valies an explicit encoding instead of ignoring  (use them only when train and test NANs are coused by the same, or your local CV proves it holds signal)\u003c/li\u003e\n\u003cli\u003eexpansion encoding (create multiple categorical varaibles from a single variable)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNumerical features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eround numerical variables (form of lossy compression, retain most significant features of the data)\u003c/li\u003e\n\u003cli\u003esometimes too much precision is just noise\u003c/li\u003e\n\u003cli\u003ebinning (by quantiles, plot data to log for good interval binning)\u003c/li\u003e\n\u003cli\u003escaling (standard Z, MinMax, Root, Log scaling)\u003c/li\u003e\n\u003cli\u003eimpute missing (mean, median (robust to outliers), KNN,) add boolean column\u003c/li\u003e\n\u003cli\u003einteractions (substraction, addition, multiplication, divison) Ignore human intuition!, weird interactions can give significant improvement\u003c/li\u003e\n\u003cli\u003ecreate statistic on a row of data (number of NaNs, Number of 0s, Number of negative values, Mean, Max, Min, Skewness, etc)\u003c/li\u003e\n\u003cli\u003etemporal variables (dates, needs backtesting)\u003c/li\u003e\n\u003cli\u003eturn single features, like day_of_week into two coordinates on a circle\u003c/li\u003e\n\u003cli\u003efor TS, instead of total spend, encoed things like spend in last week, month, etc\u003c/li\u003e\n\u003cli\u003ehardcode categorical features like: date_3_days_before_holidays: 1\u003c/li\u003e\n\u003cli\u003espacial variables (find closeness between big hubs)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNeural Networks \u0026#x26; DL\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNN claim end-to-end automatic feature engineering\u003c/li\u003e\n\u003cli\u003eFE dying field? No - moves the focus to architecture engineering\u003c/li\u003e\n\u003cli\u003eencode order of samples in dataset\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eApplied Machine Learning is basically feature engineering, Andrew Ng\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=XBJ2f68LuO4\u0026#x26;t=38s\"\u003eWinning gold is easy\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA LOT OF TIPS AND TRICKS in this video.\u003c/p\u003e\n\u003cp\u003eIf something does not work, does not stick to it.\u003c/p\u003e\n\u003cp\u003eto get a gold model you need to try crazy ideas. Most would not work, but if you get one good one you can get the gold.\u003c/p\u003e\n\u003cp\u003eTrick for feature engineering:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eadd 1 noisy features\u003c/li\u003e\n\u003cli\u003ecompute feature importance for all features\u003c/li\u003e\n\u003cli\u003efeatures which have higher rank than the noisy feature should generally be good features\u003c/li\u003e\n\u003cli\u003eif your noisy feature is too high in rank, your model is overfitting on noisy features and gives them high rank\u003c/li\u003e\n\u003cli\u003eyou need to go through the top ranked features and see which of them your model is overfitting \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFeature importance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDetect overfitting features\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eremove the feature and if CV is not hurt your are good\u003c/li\u003e\n\u003cli\u003eif removing the feature hurts CV, then use transformation on the feature\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFeature engineering is a bit of an art. Feature X and Feature Y might be good only if you use both (as they have interation).\u003c/p\u003e\n\u003cp\u003eIf your model has many features (e.g xgboost) try using small colsample. Usually you would use 80-90%. But when having MANY features, this way your trees would be almost independent and not correlated.\u003c/p\u003e\n\u003cp\u003eWorking with NN or linear models, when a feature you have NAN, add additional boolean column.\u003c/p\u003e\n\u003cp\u003eNN are making feature engineering implicitly.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=RtqtM1UJfZc\u0026#x26;t\"\u003eGiba talk, tips for feature engineering and selection\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=A8oBphPOliM\"\u003eKaggle Grandmaster and startup\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU/RAM management is 1st priority\u003c/li\u003e\n\u003cli\u003eWork on a single model as long as you can (1 week for other models)\u003c/li\u003e\n\u003cli\u003eML is just function approximation of the real world\u003c/li\u003e\n\u003cli\u003eTree methods ARE sensitive to noisy/useless features (spend enough time in Feature selection...)\u003c/li\u003e\n\u003cli\u003eIn kaggle you do not always have CV LB correlation (you have absolute noise in the LB, or the dataset is very small)\u003c/li\u003e\n\u003cli\u003eEvery time the dataset is small - you can do 5 Fold CV, repeated with 20 different CV splits. Models bagged 20 times - 2000 models in total to train just 1 model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/394975\"\u003eWhen averaging models in forecasting tasks help\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"lb-cv-scores\"\u003eLB CV Scores\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lb-cv-scores\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eCompute correlation between LB and CV scores!\u003c/li\u003e\n\u003cli\u003eLocal CV setup to be as close as possible to the Kaggle API.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"model-augmentation\"\u003eModel augmentation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#model-augmentation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eBagging\n\u003cul\u003e\n\u003cli\u003esame model trained on different folds\u003c/li\u003e\n\u003cli\u003esame model trained on different seeds (if data is very noisy)\u003c/li\u003e\n\u003cli\u003esame model trained on different features\u003c/li\u003e\n\u003cli\u003esame model trained on different hyperparameters\u003c/li\u003e\n\u003cli\u003edifferent models take weighted average/mean (careful for MAE usually it hurts)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStacking\u003c/li\u003e\n\u003cli\u003eBoosting: train on the errors\u003c/li\u003e\n\u003cli\u003eUse output of one model to be features of another model\u003c/li\u003e\n\u003cli\u003eDetrending\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"powerful-features\"\u003ePowerful features\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#powerful-features\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eunbiased feature\n\u003ccode\u003efeatureX - groupby(FeatureY)[FeatureX].mean()\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eprevious FCT error feature (difference between FCT and TARGET), kind of boosting method\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erolling aggregates - mean, median, min, max, quantiles, std, kurtosis, skew\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eintercept and slope instead of mean aggregate, that is run linear regression on past samples within a window and put the intercept and slope as features\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erandom feature trick\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eneighbors_target_mean_500: The mean TARGET value of the 500 closest neighbors of each row\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"adversarial-validation\"\u003eAdversarial Validation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#adversarial-validation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eAdversarial Validation is a very clever and very simple way to let us know if our test data and our training data are similar; we combine our train and test data, labeling them with say a 0 for the training data and a 1 for the test data, mix them up, then see if we are able to correctly re-identify them using a binary classifier.\u003c/p\u003e\n\u003cp\u003eIf we cannot correctly classify them, i.e. we obtain an area under the receiver operating characteristic curve (ROC) of 0.5 then they are indistinguishable and we are good to go.\u003c/p\u003e\n\u003cp\u003eHowever, if we can classify them (ROC \u003e 0.5) then we have a problem, either with the whole dataset or more likely with some features in particular, which are probably from different distributions in the test and train datasets. If we have a problem, we can look at the feature that was most out of place. The problem may be that there were values that were only seen in, say, training data, but not in the test data. If the contribution to the ROC is very high from one feature, it may well be a good idea to remove that feature from the model.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation\"\u003ekaggle post\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"check-for-covariate-shift-in-train-and-test\"\u003eCheck for covariate shift in train and test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#check-for-covariate-shift-in-train-and-test\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eadversarial validation\u003c/li\u003e\n\u003cli\u003eKS test for the features to see if they are from the same distribution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"check-for-target-shift-in-train-and-test\"\u003eCheck for target shift in train and test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#check-for-target-shift-in-train-and-test\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eGood to understand th dynamics of the target variable. If the target variable is changing over time, then you need to be careful with the train/test split. You need to make sure the train and test are from the same time period.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKS Test\u003c/li\u003e\n\u003cli\u003eT-test for mean shift\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"hypothesis-testing--normality-tests\"\u003eHypothesis Testing / Normality tests\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#hypothesis-testing--normality-tests\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eI think KS and normality tests are very sensistive for large datasets. If you have 10k+ observations, then you would get a very low p-value for any test. So you need to be careful with the p-value.\u003c/p\u003e\n\u003cp\u003eAny small shifts would be detected and would reject the null hypothesis.\u003c/p\u003e\n\u003cp\u003eSo these tests are useful for small sample sizes. For large sample sizes you'd use the CLT and the t-test.\u003c/p\u003e\n\u003ch1 id=\"kolmogorov-smirnov-test\"\u003eKolmogorov-Smirnov test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#kolmogorov-smirnov-test\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eThe two sample Kolmogorov-Smirnov test is a  nonparametric test that compares the cumulative distributions of two data sets(1,2).\u003c/li\u003e\n\u003cli\u003eThe test is nonparametric. It does not assume that data are sampled from Gaussian distributions (or any other defined distributions).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"one-sample-kolmogorov-smirnov-test\"\u003eOne sample Kolmogorov-Smirnov test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#one-sample-kolmogorov-smirnov-test\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIt takes the difference between the empirical distribution of the sample and the theoretical distribution. Looks at the supremum which should converge to 0 almost surely.\u003c/p\u003e\n\u003ch2 id=\"two-sample-kolmogorov-smirnov-test\"\u003eTwo sample Kolmogorov-Smirnov test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#two-sample-kolmogorov-smirnov-test\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIt takes the difference between the empirical distributions of two samples. Looks at the supremum which should converge to 0 almost surely.\u003c/p\u003e\n\u003cp\u003eKolmogorov distribution shows the rate of this convergence.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn practice, the statistic requires a relatively large number of data points to properly reject the null hypothesis.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e1k + points should be ok with error less than 1% (see wiki page).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecaveat is that this test is very sensistive, if the median, varaince,shape changes it would have low p-value\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eInterpreting the P value\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe P value is the answer to this question:\u003c/p\u003e\n\u003cp\u003eIf the two samples were randomly sampled from identical populations, what is the probability that the two cumulative frequency distributions would be as far apart as observed? More precisely, what is the chance that the value of the Komogorov-Smirnov D statistic would be as large or larger than observed?\u003c/p\u003e\n\u003cp\u003eIf the P value is small, conclude that the two groups were sampled from populations with different distributions. The populations may differ in median, variability or the shape of the distribution. \u003c/p\u003e\n\u003ch2 id=\"t-test\"\u003eT-test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#t-test\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eparametric method, used when the samples satisfy the conditions of normality, equal variance and independence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis test assumes that the populations have identical variances by default.\u003c/p\u003e\n\u003ch2 id=\"shapirowilk-test\"\u003eShapiro–Wilk test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#shapirowilk-test\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eNormality test\u003c/p\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e scipy\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003estats \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e shapiro\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBetter than KS one sample test as there you need to know the parameters of the distribution i.e. mean and variance. Cannot standaradize the data, as you would be going into a circle. you can't standardize by using estimated parameters and test for standard normal; that's actually the same thing.\u003c/li\u003e\n\u003cli\u003egood with small sample sizes ~ 50 observations\u003c/li\u003e\n\u003cli\u003ehas more power compared to other normality tests, meaning it can detect deviations from normality more effectively.\u003c/li\u003e\n\u003cli\u003eNot Sensitive to Outliers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"likelihood-ratio-test\"\u003eLikelihood-ratio test\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#likelihood-ratio-test\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"change-point-detection-in-time-series\"\u003eChange point detection in Time Series\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#change-point-detection-in-time-series\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eChange point detection marks the locations where the underlying properties (statistical characteristics e.g. mean and variance) of the time series shift abruptly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etarget engineering and prediction switching\u003c/li\u003e\n\u003cli\u003etarget splitting, find its components\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"lgbm-xgbm--catboost\"\u003eLGBM, XGBM , CATBOOST\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lgbm-xgbm--catboost\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eimportant hyperparams are usually: learning_rate, num_iterations, colsample, sample, max_depth, n_leaves, early_stopping,  device type\u003c/li\u003e\n\u003cli\u003efeature importance: use num_splits to select features, used as a guide for useful and overfitting features\u003c/li\u003e\n\u003cli\u003eplot correlation between features\u003c/li\u003e\n\u003cli\u003eget_split_value_histogram for each feature (lgbm): do that for the most important features\u003c/li\u003e\n\u003cli\u003eget_leaf_output\u003c/li\u003e\n\u003cli\u003efeature binning of continuous features (max_bin)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"feature-selection\"\u003eFeature Selection\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#feature-selection\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eObviously you should only do this if it helps your CV.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAll tricks by Chris Deotte:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eforward feature selection (using single or groups of features)\u003c/li\u003e\n\u003cli\u003erecursive feature elimination (using single or groups of features)\u003c/li\u003e\n\u003cli\u003epermutation importance\u003c/li\u003e\n\u003cli\u003eadversarial validation\u003c/li\u003e\n\u003cli\u003ecorrelation analysis\u003c/li\u003e\n\u003cli\u003etime consistency\u003c/li\u003e\n\u003cli\u003etrain/test distribution analysis\u003c/li\u003e\n\u003cli\u003erandom feature trick\u003c/li\u003e\n\u003cli\u003eremove low variance features\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOne interesting trick called \"time consistency\" is to train a single model using a single feature (or small group of features) on the first month of train dataset and predict isFraud for the last month of train dataset. This evaluates whether a feature by itself is consistent over time. 95% were but we found 5% of columns hurt our models. They had training AUC around 0.60 and validation AUC 0.40. In other words some features found patterns in the present that did not exist in the future. Of course the possible of interactions complicates things but we double checked every test with other tests.\u003c/p\u003e\n\u003cp\u003eWhatever feature selection you do \u003cstrong\u003eyou should separete feature selection from model training.\u003c/strong\u003e This avoid data leakage. First do model selection and then model training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWrapper methods\u003c/strong\u003e: these are model dependent\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eforward selection\u003c/li\u003e\n\u003cli\u003ebackward selection\u003c/li\u003e\n\u003cli\u003erecursive feature elimination (greedy optimization)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFilter methods\u003c/strong\u003e: these are model independent, and involve preprocessing\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePearson correlation\u003c/li\u003e\n\u003cli\u003eLDA - linear discriminant analysis\u003c/li\u003e\n\u003cli\u003eANOVA\u003c/li\u003e\n\u003cli\u003eChi-Square\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese methods does not remove multicollinearity.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEmbedded methods\u003c/strong\u003e: regularization\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRidge, Lasso\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"recursive-feature-elimination\"\u003eRecursive Feature Elimination\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#recursive-feature-elimination\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eRank the features according to their importance\u003c/li\u003e\n\u003cli\u003eRemove one/or more features - the least important - and build a machine learning algorithm utilizing the remaining features.\u003c/li\u003e\n\u003cli\u003eCalculate a performance metric of your choice\u003c/li\u003e\n\u003cli\u003eIf the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\nCan run permutation importance on held out test set \u003ca href=\"https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\"\u003escikit-learn\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRepeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\u003c/p\u003e\n\u003ch2 id=\"mdi\"\u003eMDI\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#mdi\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Log Loss or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.\u003c/p\u003e\n\u003cp\u003eFurthermore, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.\u003c/p\u003e\n\u003cp\u003eThis problem stems from two limitations of impurity-based feature importances:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eimpurity-based importances are biased towards high cardinality features;\u003c/li\u003e\n\u003cli\u003eimpurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"permutation-importance\"\u003ePermutation importance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#permutation-importance\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e sklearn\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003einspection \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e permutation_importance\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePermutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is \u003cstrong\u003efor a particular model.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWe measure the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature. A feature is “important” if shuffling its values increases the model \u003cstrong\u003eerror\u003c/strong\u003e, because in this case the model relied on the feature for the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. \u003c/p\u003e\n\u003cp\u003eTo highlight which features contribute the most to the generalization power of the inspected model permutation importance should be computed on validation set.\u003c/p\u003e\n\u003cp\u003eSklearn permutation_importance takes as input a fitted model. Thus based on the selected features/weights during training it computes the importance of the features. These features importance show how good are these features for this particular model. \u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnother take on feature importance is: to do permutation importance before model training.\u003c/strong\u003e This should be done manually (I don't know about a library)\nAdvatange is that is more acurate, disadvantage is that you loose a lot of time in training.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNB\u003c/strong\u003e\nWhen two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.\u003c/p\u003e\n\u003cp\u003eOne way to handle this is to cluster features that are correlated and only keep one feature from each cluster.\u003c/p\u003e\n\u003ch2 id=\"model-class-reliance\"\u003eModel Class Reliance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#model-class-reliance\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1801.01489.pdf\"\u003ePaper\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"dynamic-label-smoothing\"\u003eDynamic label smoothing\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dynamic-label-smoothing\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"throw-complex-models-on-high-weight-data\"\u003eThrow complex models on high weight data\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#throw-complex-models-on-high-weight-data\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"2-submisssions--bet-both-on-heads-and-tails\"\u003e2 submisssions = bet both on heads and tails\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#2-submisssions--bet-both-on-heads-and-tails\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.kaggle.com/competitions/jane-street-market-prediction/discussion/224079\"\u003etricks on Jane Street comp\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"random-forecast\"\u003eRandom forecast\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#random-forecast\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eproduce ranomized forecasts multiple time. This is your baseline on what noise is in the competition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"correlation-screening\"\u003eCorrelation Screening\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#correlation-screening\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ecompute correlation of each feature with the target\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/dendron-wiki/notes/2b5bwf46z6v132wu7xghvrp\"\u003eMachine Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"ltvbyf1oqc8qx7khy58pby2"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>