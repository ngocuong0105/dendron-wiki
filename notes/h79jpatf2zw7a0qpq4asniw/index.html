<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Linear Regression</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Linear Regression"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/h79jpatf2zw7a0qpq4asniw/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/8/2025"/><meta property="article:modified_time" content="10/3/2025"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/h79jpatf2zw7a0qpq4asniw/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-28855ac016325484.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/hUMuUtBSCkU7YSeQn-rle/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="linear-regression">Linear Regression<a aria-hidden="true" class="anchor-heading icon-link" href="#linear-regression"></a></h1>
<p>Here I discuss how violation of linear regression assumptions affect model explainability and foresting performance.
It is a practical guide on the caveats of the Linear Model and how to deal with them - particularly useful in forecasting-focused settings like Kaggle. It is assumed the reader is familiar with the mathematics behind the model.</p>
<p>Linear model's assumptions are:</p>
<ul>
<li>The target variable is linear in the predictor variables. Violation is also known for mispecified model</li>
<li>The errors are normally distributed, independent, and homoscedastic (constant variance).</li>
<li>The errors are independent of the features (no endogeneity).</li>
<li>The features are not multi-collinear (not highly correlated).</li>
</ul>
<p>Example when each of these assumptions is violated:</p>
<ul>
<li>Non-linear relationship: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>n</mi><mi>o</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">y = 2x^2 + noise</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">se</span></span></span></span></span></li>
<li>Correlated errors: time series data with autocorrelated residuals</li>
<li>Heteroscedasticity: variance of errors increases with the value of x (leverage effect, further from mean pull the line more)</li>
<li>Endogeneity: omitted variable that affects that affects both x and y</li>
</ul>
<h1 id="multi-collinear-features">Multi-Collinear Features<a aria-hidden="true" class="anchor-heading icon-link" href="#multi-collinear-features"></a></h1>
<ul>
<li><strong>Multicolinearity affects model interpretability and not so much forecasting performance.</strong></li>
</ul>
<p>Experiment:</p>
<ul>
<li>Run LR <code>y ~ x1</code></li>
<li>Run LR <code>y ~ x1+x2</code>, where these two are highly correlated</li>
</ul>
<p>Results</p>
<pre><code>Uncorrelated features correlation:

Uncorrelated case model summary:
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.929
Model:                            OLS   Adj. R-squared:                  0.928
Method:                 Least Squares   F-statistic:                     1275.
Date:                Tue, 08 Jul 2025   Prob (F-statistic):           5.50e-58
Time:                        08:19:04   Log-Likelihood:                -78.314
No. Observations:                 100   AIC:                             160.6
Df Residuals:                      98   BIC:                             165.8
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.0443      0.054     93.689      0.000       4.937       5.151
X1             2.1139      0.059     35.712      0.000       1.996       2.231
==============================================================================
Omnibus:                        3.154   Durbin-Watson:                   2.216
Prob(Omnibus):                  0.207   Jarque-Bera (JB):                3.133
Skew:                           0.105   Prob(JB):                        0.209
Kurtosis:                       3.841   Cond. No.                         1.16
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

Highly correlated features correlation:
          X1        X2
X1  1.000000  0.999931
X2  0.999931  1.000000

Highly correlated case model summary:
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.939
Model:                            OLS   Adj. R-squared:                  0.938
Method:                 Least Squares   F-statistic:                     751.5
Date:                Tue, 08 Jul 2025   Prob (F-statistic):           9.10e-60
Time:                        08:19:04   Log-Likelihood:                -63.278
No. Observations:                 100   AIC:                             132.6
Df Residuals:                      97   BIC:                             140.4
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          4.9343      0.047    105.551      0.000       4.842       5.027
X1             6.8089      4.486      1.518      0.132      -2.095      15.713
X2            -4.7588      4.475     -1.064      0.290     -13.639       4.122
==============================================================================
Omnibus:                        0.401   Durbin-Watson:                   2.118
Prob(Omnibus):                  0.818   Jarque-Bera (JB):                0.103
Skew:                          -0.034   Prob(JB):                        0.950
Kurtosis:                       3.141   Cond. No.                         174.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<p><strong>Insights:</strong></p>
<ul>
<li>Forecasts of both model are good and in practice you don't need to drop the correlated features if you care about forecasts</li>
<li>Remember that the forecast <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span></span> is the orthogonal projection of the real value y on the subspace spanned by the design matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span>. If the design matrix has correlated features there are multiple ways to express the same <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span></span>. Hence different values for the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi></mrow><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9579em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span></span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> would give the same forecast.  </li>
<li>When there are correlated features you will see large variance in the beta estimates, also mathematically <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mover accent="true"><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi></mrow><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>σ</mi></mrow><annotation encoding="application/x-tex">var(\hat{beta})=(XX^{T})^{-1}\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2079em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span></span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span> will just be very large when the inside matrix is not positive definite.</li>
<li>large beta variance, means low t-statistic, which means high p-value</li>
<li>hence with correlated features the model is not interpretable but the sum is good forecasts.</li>
</ul>
<p><strong>The Geometry</strong></p>
<ul>
<li>When two variables are highly correlated, they both almost point in the same “direction” in the predictor space.</li>
<li>The model tries to allocate credit (and adjust slope) between them for explaining changes in y.</li>
<li>Many combinations of coefficients can fit the same plane equally well.
</li>
</ul>
<p><strong>Effect on Model Fit</strong> </p>
<ul>
<li>The overall plane/surface that regression fits (ŷ = intercept + b1<em>X1 + b2</em>X2) can still be the same, though b1 and b2 may be weird or unintuitive (like 378 and -377). Forecasts are still good.</li>
<li>The regression is “sure” about the sum effect, but not about the individual effects.
</li>
</ul>
<p><strong>Unstaple coefficients</strong></p>
<ul>
<li>Opposite signs and very large in absolute values to offset each other.</li>
<li>For forecasting, this generally does not hurt test-set predictive performance as long as your train/test data comes from the same distribution and the correlation patterns are stable. It can, however, make your model more sensitive to changes in the feature distribution ("unstable predictions" with changing data).</li>
</ul>
<h1 id="high-leverage-points">High Leverage Points<a aria-hidden="true" class="anchor-heading icon-link" href="#high-leverage-points"></a></h1>
<ul>
<li>identify outliers in the design matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span> (not in target variable <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span>)</li>
<li>Property of: Only the feature/design matrix X  </li>
<li>Definition: Measures how far an observation's x-values are from the mean of all x-values, i.e., how "unusual" its X-row is.</li>
<li>Mathematically: The diagonal elements <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> of the “hat” matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mi>X</mi><mo stretchy="false">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H=X(X^TX)^{-1}X^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>.</li>
<li>Note: Leverage is completely independent of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span>.
</li>
<li>outliers in the design matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span> can lead to the picture below:</li>
<li>if a row <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> deviates from the mean of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span> it will have large leverage and pull the regression line
<img src="/dendron-wiki/./assets/images/high_leverage_point.png" alt="high_leverage_point"></li>
</ul>
<p>NOTE:  In regression, leverage is a property of observations (rows, i.e. data points), not of features (columns, variables). </p>
<ul>
<li>You need to check rows that are with high leverage not features/columns!</li>
</ul>
<pre class="language-python"><code class="language-python"><span class="token comment"># Model Spec</span>
<span class="token comment"># Generate 8 normal data points</span>
X1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
X2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>X1 <span class="token operator">+</span> <span class="token number">3</span><span class="token operator">*</span>X2 <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>

<span class="token comment"># Add a high-leverage point (extreme X1)</span>
X1_leverage <span class="token operator">=</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>X1<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
X2_leverage <span class="token operator">=</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>X2<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
y_leverage <span class="token operator">=</span> np<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
</code></pre>
<p><strong>Results</strong></p>
<pre><code>Model with leveraged point
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.526
Model:                            OLS   Adj. R-squared:                  0.368
Method:                 Least Squares   F-statistic:                     3.328
Date:                Tue, 08 Jul 2025   Prob (F-statistic):              0.107
Time:                        09:31:52   Log-Likelihood:                -18.040
No. Observations:                   9   AIC:                             42.08
Df Residuals:                       6   BIC:                             42.67
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.0550      1.093      0.965      0.372      -1.620       3.730
X1             0.0434      0.282      0.154      0.883      -0.647       0.734
X2             3.9627      1.857      2.134      0.077      -0.580       8.505
==============================================================================
Omnibus:                        0.114   Durbin-Watson:                   1.807
Prob(Omnibus):                  0.944   Jarque-Bera (JB):                0.321
Skew:                           0.113   Prob(JB):                        0.852
Kurtosis:                       2.103   Cond. No.                         9.95
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Model without leveraged point
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.972
Model:                            OLS   Adj. R-squared:                  0.961
Method:                 Least Squares   F-statistic:                     88.04
Date:                Tue, 08 Jul 2025   Prob (F-statistic):           0.000127
Time:                        09:31:52   Log-Likelihood:                -5.0804
No. Observations:                   8   AIC:                             16.16
Df Residuals:                       5   BIC:                             16.40
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.2898      0.299      0.968      0.377      -0.480       1.059
X1             2.0443      0.233      8.772      0.000       1.445       2.643
X2             2.1176      0.528      4.008      0.010       0.759       3.476
==============================================================================
Omnibus:                        3.584   Durbin-Watson:                   2.783
Prob(Omnibus):                  0.167   Jarque-Bera (JB):                1.223
Skew:                          -0.958   Prob(JB):                        0.543
Kurtosis:                       3.002   Cond. No.                         4.46
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<h1 id="residuals-and-cooks-distance">Residuals and Cook's Distance<a aria-hidden="true" class="anchor-heading icon-link" href="#residuals-and-cooks-distance"></a></h1>
<ul>
<li>identify outliers in target variable <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span></li>
<li>Studentised residuals and cook's distance use leverage and residual error to find outliers in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span></li>
</ul>
<p><img src="/dendron-wiki/assets/images/y_outlier.png" alt="alt text"></p>
<h1 id="homo-or-hetero">Homo or Hetero<a aria-hidden="true" class="anchor-heading icon-link" href="#homo-or-hetero"></a></h1>
<ul>
<li>Homoscedacity is the assumption that residuals are with equal variance. </li>
</ul>
<details>
<summary> <b>CODE</b> </summary>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error

<span class="token keyword">def</span> <span class="token function">generate_data</span><span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">,</span> n1<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> n2<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    x1 <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> n1<span class="token punctuation">)</span>
    y1 <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x1 <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> n1<span class="token punctuation">)</span>
    x2 <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> n2<span class="token punctuation">)</span>
    y2 <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x2 <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">69</span><span class="token punctuation">,</span> n2<span class="token punctuation">)</span> <span class="token comment"># High variance, Different mean</span>
    <span class="token keyword">return</span> x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2

<span class="token keyword">def</span> <span class="token function">fit_single_model</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    y_pred <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> y_pred

<span class="token keyword">def</span> <span class="token function">fit_two_models</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model1 <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x1<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y1<span class="token punctuation">)</span>
    y1_pred <span class="token operator">=</span> model1<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x1<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    model2 <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x2<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y2<span class="token punctuation">)</span>
    y2_pred <span class="token operator">=</span> model2<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x2<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model1<span class="token punctuation">,</span> y1_pred<span class="token punctuation">,</span> model2<span class="token punctuation">,</span> y2_pred

<span class="token keyword">def</span> <span class="token function">plot_results</span><span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2<span class="token punctuation">,</span> X_all<span class="token punctuation">,</span> y_pred_all<span class="token punctuation">,</span> y1_pred<span class="token punctuation">,</span> y2_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Set 1 (Low variance)'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'blue'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> y2<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Set 2 (High variance)'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_all<span class="token punctuation">,</span> y_pred_all<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Fit on all points'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1_pred<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Fit on Set 1'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'blue'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> y2_pred<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Fit on Set 2'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Illustration of Heteroscedasticity'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">calculate_mse</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> mean_squared_error<span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Generate data</span>
    x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2 <span class="token operator">=</span> generate_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
    X_all <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    y_all <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>y1<span class="token punctuation">,</span> y2<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># Fit models</span>
    model_all<span class="token punctuation">,</span> y_pred_all <span class="token operator">=</span> fit_single_model<span class="token punctuation">(</span>np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_all<span class="token punctuation">)</span>
    model1<span class="token punctuation">,</span> y1_pred<span class="token punctuation">,</span> model2<span class="token punctuation">,</span> y2_pred <span class="token operator">=</span> fit_two_models<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2<span class="token punctuation">)</span>

    <span class="token comment"># Plot</span>
    plot_results<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> y1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> y2<span class="token punctuation">,</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_pred_all<span class="token punctuation">,</span> y1_pred<span class="token punctuation">,</span> y2_pred<span class="token punctuation">)</span>

    <span class="token comment"># MSE calculations</span>
    mse_all <span class="token operator">=</span> calculate_mse<span class="token punctuation">(</span>y_all<span class="token punctuation">,</span> y_pred_all<span class="token punctuation">)</span>
    y_sep_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>y1_pred<span class="token punctuation">,</span> y2_pred<span class="token punctuation">]</span><span class="token punctuation">)</span>
    mse_sep <span class="token operator">=</span> calculate_mse<span class="token punctuation">(</span>y_all<span class="token punctuation">,</span> y_sep_pred<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"MSE (fit to all data):"</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>mse_all<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"MSE (fit separately):"</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>mse_sep<span class="token punctuation">)</span><span class="token punctuation">)</span>


main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
</details>
<p>It reduces a bit the forecasting accuracy.</p>
<p><img src="/dendron-wiki/./assets/images/heteroscedacity.png" alt="alt text"></p>
<details>
<summary> <b>Summary of the results:</b> </summary>
<pre><code>=== Regression summary: All Data ===
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.059
Model:                            OLS   Adj. R-squared:                  0.050
Method:                 Least Squares   F-statistic:                     6.194
Date:                Thu, 10 Jul 2025   Prob (F-statistic):             0.0145
Time:                        08:37:53   Log-Likelihood:                -516.21
No. Observations:                 100   AIC:                             1036.
Df Residuals:                      98   BIC:                             1042.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          3.2053      8.499      0.377      0.707     -13.661      20.072
x1             1.8295      0.735      2.489      0.015       0.371       3.288
==============================================================================
Omnibus:                       21.818   Durbin-Watson:                   2.191
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               73.588
Skew:                          -0.605   Prob(JB):                     1.05e-16
Kurtosis:                       7.025   Cond. No.                         23.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

=== Regression summary: Set 1 (Low variance) ===
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.975
Model:                            OLS   Adj. R-squared:                  0.975
Method:                 Least Squares   F-statistic:                     1903.
Date:                Thu, 10 Jul 2025   Prob (F-statistic):           2.81e-40
Time:                        08:37:53   Log-Likelihood:                -66.142
No. Observations:                  50   AIC:                             136.3
Df Residuals:                      48   BIC:                             140.1
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.0644      0.258      4.120      0.000       0.545       1.584
x1             1.9420      0.045     43.622      0.000       1.853       2.032
==============================================================================
Omnibus:                        0.453   Durbin-Watson:                   1.942
Prob(Omnibus):                  0.798   Jarque-Bera (JB):                0.608
Skew:                           0.156   Prob(JB):                        0.738
Kurtosis:                       2.559   Cond. No.                         11.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

=== Regression summary: Set 2 (High variance) ===
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.000
Model:                            OLS   Adj. R-squared:                 -0.021
Method:                 Least Squares   F-statistic:                  0.001247
Date:                Thu, 10 Jul 2025   Prob (F-statistic):              0.972
Time:                        08:37:53   Log-Likelihood:                -275.16
No. Observations:                  50   AIC:                             554.3
Df Residuals:                      48   BIC:                             558.1
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         33.7690     44.502      0.759      0.452     -55.707     123.245
x1            -0.1028      2.911     -0.035      0.972      -5.956       5.751
==============================================================================
Omnibus:                        4.219   Durbin-Watson:                   2.211
Prob(Omnibus):                  0.121   Jarque-Bera (JB):                3.105
Skew:                          -0.535   Prob(JB):                        0.212
Kurtosis:                       3.587   Cond. No.                         79.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
</details>
<p>These results show how when you fit one for all it has low <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> mainly due to the large errors by high variance data. You will miss the signal in the low variance data (p-value for x1 is higher in the fit all model).</p>
<h1 id="loss-function-likelihood-pearson-correlation">Loss Function, Likelihood, Pearson Correlation<a aria-hidden="true" class="anchor-heading icon-link" href="#loss-function-likelihood-pearson-correlation"></a></h1>
<ul>
<li>Under normality and homoscedasticity assumptions, the OLS loss function is equivalent to the negative log-likelihood of the Gaussian distribution.</li>
<li>Also it is equivalent to the Pearson correlation coefficient between the predicted and actual values.</li>
</ul>
<h1 id="independent-errors-assumption">Independent Errors Assumption<a aria-hidden="true" class="anchor-heading icon-link" href="#independent-errors-assumption"></a></h1>
<ul>
<li>The assumption that the errors are independent of each other.</li>
<li>statsmodels has a fix for p-values and varaince estimae (still fits normal OLS) but gives different summary results. </li>
</ul>
<pre class="language-python"><code class="language-python">X_const <span class="token operator">=</span> sm<span class="token punctuation">.</span>add_constant<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
ols_model <span class="token operator">=</span> sm<span class="token punctuation">.</span>OLS<span class="token punctuation">(</span>y_ar1<span class="token punctuation">,</span> X_const<span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">)</span>
robust_model <span class="token operator">=</span> ols_model<span class="token punctuation">.</span>get_robustcov_results<span class="token punctuation">(</span>cov_type<span class="token operator">=</span><span class="token string">'HAC'</span><span class="token punctuation">,</span> maxlags<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre>
<h1 id="computation-and-optimization">Computation and Optimization<a aria-hidden="true" class="anchor-heading icon-link" href="#computation-and-optimization"></a></h1>
<p><strong>Solving <code>Ax = b</code></strong></p>
<h2 id="direct-inverse">Direct Inverse<a aria-hidden="true" class="anchor-heading icon-link" href="#direct-inverse"></a></h2>
<ul>
<li>
<p>Strict inverse using QR decomposition, Q is orthogonal matrix, R is upper triangular.
You can solve <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>x</mi><mo>=</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">Rx = Q^{T}b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">b</span></span></span></span></span> using back substitution. Recursively solving for each element of x starting from the last row up to the first row.</p>
</li>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo stretchy="false">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">X(X^{T}X)^{-1} y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></span> is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>p</mi><mn>3</mn></msup><mo stretchy="false">)</mo><mo>+</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>p</mi><mn>2</mn></msup><mo>∗</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(p^3) + O(p^{2}*n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span> run time (matrix inversion is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>p</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(p^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>)</p>
</li>
<li>
<p>Use Gram-Schmidt  to build QR decomposition </p>
</li>
<li>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>p</mi><mn>2</mn></msup><mo>∗</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(p^2*n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span> run time</p>
</li>
<li>
<p>Gram-Schmidt is doing regression on each feature one by one, orthogonally projecting the residuals onto the next feature.</p>
</li>
<li>
<p>Your book "Elements of Statistical Learning" has the exact algo.</p>
</li>
<li>
<p>In theory you can use Gaussian Elimination to calculate the inverse of A.
This is when you "attach" the identity matrix to A and do row operations to convert A to identity matrix, the identity matrix will become A inverse.
These operations are echelon form and are <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>p</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(p^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> run time.</p>
</li>
</ul>
<h2 id="pseudo-inverse">Pseudo Inverse<a aria-hidden="true" class="anchor-heading icon-link" href="#pseudo-inverse"></a></h2>
<ul>
<li>Produces more stable results and is strictly better than direct inverse in terms of forecasts</li>
<li>The big-O run time could be larger for well-defined invertible matrices, but in practice it is faster.</li>
<li>pseudoinverse instead of blowing up values close to 0 it suppresses them naturally giving stable results</li>
<li><strong>SVD*</strong> decomposition is used to compute the pseudoinverse</li>
<li><code>A = U @ SIGMA @ V^T -> A^+ = V @ SIGMA^+ @ U^T</code></li>
<li>U and V are orthonormal matrices</li>
</ul>
<h3 id="decomposition-methods">Decomposition Methods<a aria-hidden="true" class="anchor-heading icon-link" href="#decomposition-methods"></a></h3>
<ul>
<li>LU decomposition: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>L</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">A=LU</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span></span></span></span></span> where L is lower triangular and U is upper triangular. Used to solve Ax=b by solving Ly=b and then Ux=y (simpler than taking the inverse directly)</li>
<li>Cholesky decomposition (subcase of LU decomposition): <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A=LL^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span> for symmetric positive definite matrices.</li>
<li>QR decomposition: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">A=QR</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">QR</span></span></span></span></span> where Q is orthogonal and R is upper triangular. Used to solve Ax=b by solving Rx=Q^Tb.</li>
<li>SVD decomposition: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi mathvariant="normal">Σ</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A=U\Sigma V^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord">Σ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span> where U and V are orthogonal and Σ is diagonal. Used for pseudoinverse and dimensionality reduction.</li>
</ul>
<p><a class="color-tag" style="--tag-color: #fbdd7e;" href="/dendron-wiki/notes/q334c9dkrbs70eq5gujgcb3">#TODO</a> check how to implement Gram-Schmidt in numpy/scipy.</p>
<p>I tested in practice, the Gram-Schmidt was slower, though.</p>
<details>
<summary> <b>CODE</b> </summary>
<pre class="language-python"><code class="language-python">iimport numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time
<span class="token keyword">from</span> numpy<span class="token punctuation">.</span>linalg <span class="token keyword">import</span> inv<span class="token punctuation">,</span> solve
<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>linalg <span class="token keyword">import</span> qr

<span class="token comment"># Function for normal equation (direct inversion)</span>
<span class="token keyword">def</span> <span class="token function">normal_equation</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    XtX <span class="token operator">=</span> X<span class="token punctuation">.</span>T @ X
    Xty <span class="token operator">=</span> X<span class="token punctuation">.</span>T @ y
    beta <span class="token operator">=</span> inv<span class="token punctuation">(</span>XtX<span class="token punctuation">)</span> @ Xty
    <span class="token keyword">return</span> beta

<span class="token comment"># Function for QR decomposition</span>
<span class="token keyword">def</span> <span class="token function">qr_solution</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    Q<span class="token punctuation">,</span> R <span class="token operator">=</span> qr<span class="token punctuation">(</span>X<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'economic'</span><span class="token punctuation">)</span>
    beta <span class="token operator">=</span> solve<span class="token punctuation">(</span>R<span class="token punctuation">,</span> Q<span class="token punctuation">.</span>T @ y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> beta

<span class="token comment"># Simulation parameters</span>
n <span class="token operator">=</span> <span class="token number">50000</span>   <span class="token comment"># number of samples (can increase for stress testing)</span>
p <span class="token operator">=</span> <span class="token number">200</span>    <span class="token comment"># number of features</span>

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n<span class="token punctuation">,</span> p<span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n<span class="token punctuation">)</span>

<span class="token comment"># Run and time the normal equation</span>
start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
beta_normal <span class="token operator">=</span> normal_equation<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
time_normal <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Normal Equation Time: </span><span class="token interpolation"><span class="token punctuation">{</span>time_normal<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> seconds"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Run and time the QR decomposition</span>
start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
beta_qr <span class="token operator">=</span> qr_solution<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
time_qr <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"QR Decomposition Time: </span><span class="token interpolation"><span class="token punctuation">{</span>time_qr<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> seconds"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Check the difference in solution (should be very small!)</span>
diff <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>beta_normal <span class="token operator">-</span> beta_qr<span class="token punctuation">)</span> <span class="token comment"># sum(abs(diff))^(0.5)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"L2 norm of difference between solutions: </span><span class="token interpolation"><span class="token punctuation">{</span>diff<span class="token punctuation">:</span><span class="token format-spec">.2e</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Output</span>
<span class="token triple-quoted-string string">'''
Normal Equation Time: 0.0389 seconds
QR Decomposition Time: 0.5719 seconds
L2 norm of difference between solutions: 5.85e-16
'''</span>
</code></pre>
</details>
<h1 id="feature-selection">Feature Selection<a aria-hidden="true" class="anchor-heading icon-link" href="#feature-selection"></a></h1>
<ul>
<li>Forward or backward selection methods are O(n^2)</li>
<li>Can use regularization for feature selection</li>
<li>In practice if you have many features that are noisy or collinear, regularization will help with forecasting performance.</li>
<li>however if you choose the <strong>top correlated</strong> with the target features that could outperform fitting regularized LR on all features,
since you'd try to fit noise. Lasso might remove the noisy features but it still struggles.</li>
</ul>
<h1 id="regularization">Regularization<a aria-hidden="true" class="anchor-heading icon-link" href="#regularization"></a></h1>
<p>Regularization can be used in two ways: </p>
<ul>
<li>to reduce the variance of the model</li>
<li>to perform feature selection</li>
</ul>
<p>Feature selection and regularization can be connected</p>
<ul>
<li>They can be seen as L0 regularization (penalizing the number of non-zero coefficients)</li>
<li>L1 and L2 regularization are convex problems. The loss with regularization is a SUM OF CONVEX functions!</li>
<li>Minimizing the loss functions is equivalent to solving a
constrained optimization problem = minimizing OLS + L1/L2 constraint on the coefficients.</li>
<li>when you think of the constrained problems you can draw the OLS loss and the L1/L2 constraints and see where they intersect.</li>
<li>L2 constraint is a circle, L1 is a diamond.
The corners of the diamond are more likely to intersect with the OLS loss contours leading to sparse solutions.
In multiple dimensions the L1 constraint is a hypercube with many corners.</li>
</ul>
<p><strong>Solutions:</strong></p>
<p>If X is orthonormal, then the solutions to lasso and ridge are:</p>
<ul>
<li>lasso: sign(beta) * max(|beta|-lambda,0)  where beta is the solution to OLS</li>
<li>ridge: beta / (1+lambda)</li>
</ul>
<h2 id="ridge">Ridge<a aria-hidden="true" class="anchor-heading icon-link" href="#ridge"></a></h2>
<ul>
<li>Ridge (L2) regularization is a scaling regularization (divides the OLS solution by a factor (1+lambda))</li>
<li>Ridge regularization is PCA regression with soft-threshold. It shrinks the coefficients of the principal components.
It shrinks more the coefficients with low variance (less important components)</li>
<li>Ridge is equivalent to maximizing the posterior mean with prior beta ~ Normal(0,rho). then lambda = sigma/rho (sigma is the error variance)</li>
</ul>
<h2 id="lasso">Lasso<a aria-hidden="true" class="anchor-heading icon-link" href="#lasso"></a></h2>
<ul>
<li>Lasso (L1) regularization </li>
<li>sparse solutions (think of the constraint problem is a diamond)</li>
<li>convex problem - no closed form solution - use coordinate descent, stochastic descent or LARS</li>
<li>Lasso is equivalent to maximizing the posterior mode with prior beta ~ Laplace(0,b). then lambda = sigma/b (sigma is the error variance)</li>
</ul>
<p>Say you have MSE with constraint |beta| > 5, then this is NOT convex - you could be on the wrong side of the parabola.</p>
<h1 id="log-scale">Log Scale<a aria-hidden="true" class="anchor-heading icon-link" href="#log-scale"></a></h1>
<p>Make better visualizations when there is an outlier in the target variable.</p>
<p>Note how the y axis does not have equally distributed points. from 10^1 to 10^2 is the same length as from 10^2  to 10^3 </p>
<details>
<summary> <b>CODE</b> </summary>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns

<span class="token comment"># Sample data: 20 points lying roughly on a line, plus one big outlier</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token operator">=</span>x<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
y<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1000</span>  <span class="token comment"># outlier</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'x'</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">:</span> y<span class="token punctuation">}</span><span class="token punctuation">)</span>

fig<span class="token punctuation">,</span> axs <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Linear scale scatter plot</span>
sns<span class="token punctuation">.</span>scatterplot<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token string">'x'</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'y'</span><span class="token punctuation">,</span> data<span class="token operator">=</span>df<span class="token punctuation">,</span> ax<span class="token operator">=</span>axs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Linear scale"</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"x"</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"y"</span><span class="token punctuation">)</span>

<span class="token comment"># Log scale scatter plot</span>
sns<span class="token punctuation">.</span>scatterplot<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token string">'x'</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'y'</span><span class="token punctuation">,</span> data<span class="token operator">=</span>df<span class="token punctuation">,</span> ax<span class="token operator">=</span>axs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_yscale<span class="token punctuation">(</span><span class="token string">'log'</span><span class="token punctuation">)</span>
<span class="token comment"># equivalent to:</span>
<span class="token comment"># axs[1].scatter(df['x'], np.log(df['y']), color='r')</span>

axs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Log scale (y-axis)"</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"x"</span><span class="token punctuation">)</span>
axs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"y (log scale)"</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
</details>
<ul>
<li>When all points are close together (except for the outlier), the outlier dominates the linear y-range, which squeezes the other data against the axis.</li>
<li>With log scale, both the regular points and the outlier are shown in proportion, so you can see ALL points—even values that differ by orders of magnitude.</li>
</ul>
<p><img src="/dendron-wiki/./assets/images/log_scale_viz.png" alt="alt text"></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#multi-collinear-features" title="Multi-Collinear Features">Multi-Collinear Features</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#high-leverage-points" title="High Leverage Points">High Leverage Points</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#residuals-and-cooks-distance" title="Residuals and Cook&#x27;s Distance">Residuals and Cook&#x27;s Distance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#homo-or-hetero" title="Homo or Hetero">Homo or Hetero</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#loss-function-likelihood-pearson-correlation" title="Loss Function, Likelihood, Pearson Correlation">Loss Function, Likelihood, Pearson Correlation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#independent-errors-assumption" title="Independent Errors Assumption">Independent Errors Assumption</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computation-and-optimization" title="Computation and Optimization">Computation and Optimization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#direct-inverse" title="Direct Inverse">Direct Inverse</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pseudo-inverse" title="Pseudo Inverse">Pseudo Inverse</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#decomposition-methods" title="Decomposition Methods">Decomposition Methods</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#feature-selection" title="Feature Selection">Feature Selection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#regularization" title="Regularization">Regularization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ridge" title="Ridge">Ridge</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lasso" title="Lasso">Lasso</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#log-scale" title="Log Scale">Log Scale</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"h79jpatf2zw7a0qpq4asniw","title":"Linear Regression","desc":"","updated":1759517902964,"created":1751960242840,"custom":{},"fname":"machine learning.Linear Regression","type":"note","vault":{"fsPath":"vault"},"contentHash":"e591d22849952708411a7f3846d71826","links":[{"type":"wiki","from":{"fname":"machine learning.Linear Regression","id":"h79jpatf2zw7a0qpq4asniw","vaultName":"vault"},"value":"tags.TODO","alias":"#TODO","position":{"start":{"line":434,"column":1,"offset":23616},"end":{"line":434,"column":6,"offset":23621},"indent":[]},"xvault":false,"to":{"fname":"tags.TODO"}}],"anchors":{"multi-collinear-features":{"type":"header","text":"Multi-Collinear Features","value":"multi-collinear-features","line":27,"column":0,"depth":1},"high-leverage-points":{"type":"header","text":"High Leverage Points","value":"high-leverage-points","line":122,"column":0,"depth":1},"residuals-and-cooks-distance":{"type":"header","text":"Residuals and Cook's Distance","value":"residuals-and-cooks-distance","line":212,"column":0,"depth":1},"homo-or-hetero":{"type":"header","text":"Homo or Hetero","value":"homo-or-hetero","line":220,"column":0,"depth":1},"loss-function-likelihood-pearson-correlation":{"type":"header","text":"Loss Function, Likelihood, Pearson Correlation","value":"loss-function-likelihood-pearson-correlation","line":389,"column":0,"depth":1},"independent-errors-assumption":{"type":"header","text":"Independent Errors Assumption","value":"independent-errors-assumption","line":395,"column":0,"depth":1},"computation-and-optimization":{"type":"header","text":"Computation and Optimization","value":"computation-and-optimization","line":406,"column":0,"depth":1},"direct-inverse":{"type":"header","text":"Direct Inverse","value":"direct-inverse","line":410,"column":0,"depth":2},"pseudo-inverse":{"type":"header","text":"Pseudo Inverse","value":"pseudo-inverse","line":423,"column":0,"depth":2},"decomposition-methods":{"type":"header","text":"Decomposition Methods","value":"decomposition-methods","line":432,"column":0,"depth":3},"feature-selection":{"type":"header","text":"Feature Selection","value":"feature-selection","line":501,"column":0,"depth":1},"regularization":{"type":"header","text":"Regularization","value":"regularization","line":508,"column":0,"depth":1},"ridge":{"type":"header","text":"Ridge","value":"ridge","line":530,"column":0,"depth":2},"lasso":{"type":"header","text":"Lasso","value":"lasso","line":537,"column":0,"depth":2},"log-scale":{"type":"header","text":"Log Scale","value":"log-scale","line":548,"column":0,"depth":1}},"children":[],"parent":"2b5bwf46z6v132wu7xghvrp","data":{}},"body":"\u003ch1 id=\"linear-regression\"\u003eLinear Regression\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#linear-regression\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eHere I discuss how violation of linear regression assumptions affect model explainability and foresting performance.\nIt is a practical guide on the caveats of the Linear Model and how to deal with them - particularly useful in forecasting-focused settings like Kaggle. It is assumed the reader is familiar with the mathematics behind the model.\u003c/p\u003e\n\u003cp\u003eLinear model's assumptions are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe target variable is linear in the predictor variables. Violation is also known for mispecified model\u003c/li\u003e\n\u003cli\u003eThe errors are normally distributed, independent, and homoscedastic (constant variance).\u003c/li\u003e\n\u003cli\u003eThe errors are independent of the features (no endogeneity).\u003c/li\u003e\n\u003cli\u003eThe features are not multi-collinear (not highly correlated).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample when each of these assumptions is violated:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNon-linear relationship: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmsup\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey = 2x^2 + noise\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8974em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e2\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6595em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ei\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ese\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eCorrelated errors: time series data with autocorrelated residuals\u003c/li\u003e\n\u003cli\u003eHeteroscedasticity: variance of errors increases with the value of x (leverage effect, further from mean pull the line more)\u003c/li\u003e\n\u003cli\u003eEndogeneity: omitted variable that affects that affects both x and y\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"multi-collinear-features\"\u003eMulti-Collinear Features\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#multi-collinear-features\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMulticolinearity affects model interpretability and not so much forecasting performance.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExperiment:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRun LR \u003ccode\u003ey ~ x1\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRun LR \u003ccode\u003ey ~ x1+x2\u003c/code\u003e, where these two are highly correlated\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eResults\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUncorrelated features correlation:\n\nUncorrelated case model summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.929\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     1275.\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           5.50e-58\nTime:                        08:19:04   Log-Likelihood:                -78.314\nNo. Observations:                 100   AIC:                             160.6\nDf Residuals:                      98   BIC:                             165.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.0443      0.054     93.689      0.000       4.937       5.151\nX1             2.1139      0.059     35.712      0.000       1.996       2.231\n==============================================================================\nOmnibus:                        3.154   Durbin-Watson:                   2.216\nProb(Omnibus):                  0.207   Jarque-Bera (JB):                3.133\nSkew:                           0.105   Prob(JB):                        0.209\nKurtosis:                       3.841   Cond. No.                         1.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nHighly correlated features correlation:\n          X1        X2\nX1  1.000000  0.999931\nX2  0.999931  1.000000\n\nHighly correlated case model summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.939\nModel:                            OLS   Adj. R-squared:                  0.938\nMethod:                 Least Squares   F-statistic:                     751.5\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           9.10e-60\nTime:                        08:19:04   Log-Likelihood:                -63.278\nNo. Observations:                 100   AIC:                             132.6\nDf Residuals:                      97   BIC:                             140.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.9343      0.047    105.551      0.000       4.842       5.027\nX1             6.8089      4.486      1.518      0.132      -2.095      15.713\nX2            -4.7588      4.475     -1.064      0.290     -13.639       4.122\n==============================================================================\nOmnibus:                        0.401   Durbin-Watson:                   2.118\nProb(Omnibus):                  0.818   Jarque-Bera (JB):                0.103\nSkew:                          -0.034   Prob(JB):                        0.950\nKurtosis:                       3.141   Cond. No.                         174.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eInsights:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eForecasts of both model are good and in practice you don't need to drop the correlated features if you care about forecasts\u003c/li\u003e\n\u003cli\u003eRemember that the forecast \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6944em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.1944em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1944em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e is the orthogonal projection of the real value y on the subspace spanned by the design matrix \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. If the design matrix has correlated features there are multiple ways to express the same \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.6944em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.1944em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.1944em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e. Hence different values for the \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmrow\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003c/mrow\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{beta}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.9579em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9579em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2634em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.25em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e would give the same forecast.  \u003c/li\u003e\n\u003cli\u003eWhen there are correlated features you will see large variance in the beta estimates, also mathematically \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmover accent=\"true\"\u003e\u003cmrow\u003e\u003cmi\u003eb\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003c/mrow\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003evar(\\hat{beta})=(XX^{T})^{-1}\\sigma\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2079em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ev\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003er\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9579em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ee\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003et\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ea\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2634em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.25em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e will just be very large when the inside matrix is not positive definite.\u003c/li\u003e\n\u003cli\u003elarge beta variance, means low t-statistic, which means high p-value\u003c/li\u003e\n\u003cli\u003ehence with correlated features the model is not interpretable but the sum is good forecasts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eThe Geometry\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen two variables are highly correlated, they both almost point in the same “direction” in the predictor space.\u003c/li\u003e\n\u003cli\u003eThe model tries to allocate credit (and adjust slope) between them for explaining changes in y.\u003c/li\u003e\n\u003cli\u003eMany combinations of coefficients can fit the same plane equally well.\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eEffect on Model Fit\u003c/strong\u003e \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe overall plane/surface that regression fits (ŷ = intercept + b1\u003cem\u003eX1 + b2\u003c/em\u003eX2) can still be the same, though b1 and b2 may be weird or unintuitive (like 378 and -377). Forecasts are still good.\u003c/li\u003e\n\u003cli\u003eThe regression is “sure” about the sum effect, but not about the individual effects.\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eUnstaple coefficients\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpposite signs and very large in absolute values to offset each other.\u003c/li\u003e\n\u003cli\u003eFor forecasting, this generally does not hurt test-set predictive performance as long as your train/test data comes from the same distribution and the correlation patterns are stable. It can, however, make your model more sensitive to changes in the feature distribution (\"unstable predictions\" with changing data).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"high-leverage-points\"\u003eHigh Leverage Points\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#high-leverage-points\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eidentify outliers in the design matrix \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e (not in target variable \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/li\u003e\n\u003cli\u003eProperty of: Only the feature/design matrix X  \u003c/li\u003e\n\u003cli\u003eDefinition: Measures how far an observation's x-values are from the mean of all x-values, i.e., how \"unusual\" its X-row is.\u003c/li\u003e\n\u003cli\u003eMathematically: The diagonal elements \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eh_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eh\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e of the “hat” matrix \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eH\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmsup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eH=X(X^TX)^{-1}X^T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.08125em;\"\u003eH\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"mclose\"\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e.\u003c/li\u003e\n\u003cli\u003eNote: Leverage is completely independent of \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e.\n\u003c/li\u003e\n\u003cli\u003eoutliers in the design matrix \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e can lead to the picture below:\u003c/li\u003e\n\u003cli\u003eif a row \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex_i\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3117em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e deviates from the mean of \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e it will have large leverage and pull the regression line\n\u003cimg src=\"/dendron-wiki/./assets/images/high_leverage_point.png\" alt=\"high_leverage_point\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNOTE:  In regression, leverage is a property of observations (rows, i.e. data points), not of features (columns, variables). \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou need to check rows that are with high leverage not features/columns!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token comment\"\u003e# Model Spec\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e# Generate 8 normal data points\u003c/span\u003e\nX1 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nX2 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token operator\"\u003e*\u003c/span\u003eX1 \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token operator\"\u003e*\u003c/span\u003eX2 \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e0.5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e8\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Add a high-leverage point (extreme X1)\u003c/span\u003e\nX1_leverage \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eappend\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nX2_leverage \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eappend\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny_leverage \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eappend\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e4\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eModel with leveraged point\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.526\nModel:                            OLS   Adj. R-squared:                  0.368\nMethod:                 Least Squares   F-statistic:                     3.328\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):              0.107\nTime:                        09:31:52   Log-Likelihood:                -18.040\nNo. Observations:                   9   AIC:                             42.08\nDf Residuals:                       6   BIC:                             42.67\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0550      1.093      0.965      0.372      -1.620       3.730\nX1             0.0434      0.282      0.154      0.883      -0.647       0.734\nX2             3.9627      1.857      2.134      0.077      -0.580       8.505\n==============================================================================\nOmnibus:                        0.114   Durbin-Watson:                   1.807\nProb(Omnibus):                  0.944   Jarque-Bera (JB):                0.321\nSkew:                           0.113   Prob(JB):                        0.852\nKurtosis:                       2.103   Cond. No.                         9.95\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nModel without leveraged point\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.972\nModel:                            OLS   Adj. R-squared:                  0.961\nMethod:                 Least Squares   F-statistic:                     88.04\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           0.000127\nTime:                        09:31:52   Log-Likelihood:                -5.0804\nNo. Observations:                   8   AIC:                             16.16\nDf Residuals:                       5   BIC:                             16.40\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2898      0.299      0.968      0.377      -0.480       1.059\nX1             2.0443      0.233      8.772      0.000       1.445       2.643\nX2             2.1176      0.528      4.008      0.010       0.759       3.476\n==============================================================================\nOmnibus:                        3.584   Durbin-Watson:                   2.783\nProb(Omnibus):                  0.167   Jarque-Bera (JB):                1.223\nSkew:                          -0.958   Prob(JB):                        0.543\nKurtosis:                       3.002   Cond. No.                         4.46\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"residuals-and-cooks-distance\"\u003eResiduals and Cook's Distance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#residuals-and-cooks-distance\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eidentify outliers in target variable \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eStudentised residuals and cook's distance use leverage and residual error to find outliers in \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ey\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/assets/images/y_outlier.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch1 id=\"homo-or-hetero\"\u003eHomo or Hetero\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#homo-or-hetero\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eHomoscedacity is the assumption that residuals are with equal variance. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cdetails\u003e\n\u003csummary\u003e \u003cb\u003eCODE\u003c/b\u003e \u003c/summary\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e matplotlib\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epyplot \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e sklearn\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinear_model \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e LinearRegression\n\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e sklearn\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003emetrics \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e mean_squared_error\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003egenerate_data\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eseed\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e42\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n1\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e50\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n2\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e50\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    x1 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinspace\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n1\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y1 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e \u003cspan class=\"token operator\"\u003e*\u003c/span\u003e x1 \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n1\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    x2 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinspace\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e20\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n2\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y2 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e \u003cspan class=\"token operator\"\u003e*\u003c/span\u003e x2 \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e69\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e n2\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# High variance, Different mean\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e x1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003efit_single_model\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    model \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e LinearRegression\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efit\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y_pred \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e model\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epredict\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e model\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003efit_two_models\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    model1 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e LinearRegression\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efit\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y1_pred \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e model1\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epredict\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    model2 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e LinearRegression\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efit\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex2\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y2_pred \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e model2\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epredict\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex2\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e model1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e model2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003eplot_results\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e X_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efigure\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003efigsize\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e10\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e6\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003escatter\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Set 1 (Low variance)'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'blue'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003escatter\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Set 2 (High variance)'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'red'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Fit on all points'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'black'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e linewidth\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Fit on Set 1'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'blue'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e linestyle\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'--'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e label\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Fit on Set 2'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'red'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e linestyle\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'--'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elegend\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003exlabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'x'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eylabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'y'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etitle\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'Illustration of Heteroscedasticity'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eshow\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003ecalculate_mse\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey_true\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e mean_squared_error\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey_true\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003emain\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"token comment\"\u003e# Generate data\u003c/span\u003e\n    x1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2 \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e generate_data\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    X_all \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003econcatenate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003ereshape\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y_all \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003econcatenate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003ey1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"token comment\"\u003e# Fit models\u003c/span\u003e\n    model_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred_all \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e fit_single_model\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003enp\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003econcatenate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_all\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    model1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e model2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e fit_two_models\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"token comment\"\u003e# Plot\u003c/span\u003e\n    plot_results\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003econcatenate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003ex1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e x2\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"token comment\"\u003e# MSE calculations\u003c/span\u003e\n    mse_all \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e calculate_mse\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_pred_all\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    y_sep_pred \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003econcatenate\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003ey1_pred\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y2_pred\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    mse_sep \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e calculate_mse\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey_all\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y_sep_pred\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"MSE (fit to all data):\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token builtin\"\u003eround\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003emse_all\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"MSE (fit separately):\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token builtin\"\u003eround\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003emse_sep\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\nmain\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cp\u003eIt reduces a bit the forecasting accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/heteroscedacity.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003e \u003cb\u003eSummary of the results:\u003c/b\u003e \u003c/summary\u003e\n\u003cpre\u003e\u003ccode\u003e=== Regression summary: All Data ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.059\nModel:                            OLS   Adj. R-squared:                  0.050\nMethod:                 Least Squares   F-statistic:                     6.194\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):             0.0145\nTime:                        08:37:53   Log-Likelihood:                -516.21\nNo. Observations:                 100   AIC:                             1036.\nDf Residuals:                      98   BIC:                             1042.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.2053      8.499      0.377      0.707     -13.661      20.072\nx1             1.8295      0.735      2.489      0.015       0.371       3.288\n==============================================================================\nOmnibus:                       21.818   Durbin-Watson:                   2.191\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               73.588\nSkew:                          -0.605   Prob(JB):                     1.05e-16\nKurtosis:                       7.025   Cond. No.                         23.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Regression summary: Set 1 (Low variance) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.975\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     1903.\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):           2.81e-40\nTime:                        08:37:53   Log-Likelihood:                -66.142\nNo. Observations:                  50   AIC:                             136.3\nDf Residuals:                      48   BIC:                             140.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0644      0.258      4.120      0.000       0.545       1.584\nx1             1.9420      0.045     43.622      0.000       1.853       2.032\n==============================================================================\nOmnibus:                        0.453   Durbin-Watson:                   1.942\nProb(Omnibus):                  0.798   Jarque-Bera (JB):                0.608\nSkew:                           0.156   Prob(JB):                        0.738\nKurtosis:                       2.559   Cond. No.                         11.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Regression summary: Set 2 (High variance) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.021\nMethod:                 Least Squares   F-statistic:                  0.001247\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):              0.972\nTime:                        08:37:53   Log-Likelihood:                -275.16\nNo. Observations:                  50   AIC:                             554.3\nDf Residuals:                      48   BIC:                             558.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P\u003e|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         33.7690     44.502      0.759      0.452     -55.707     123.245\nx1            -0.1028      2.911     -0.035      0.972      -5.956       5.751\n==============================================================================\nOmnibus:                        4.219   Durbin-Watson:                   2.211\nProb(Omnibus):                  0.121   Jarque-Bera (JB):                3.105\nSkew:                          -0.535   Prob(JB):                        0.212\nKurtosis:                       3.587   Cond. No.                         79.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cp\u003eThese results show how when you fit one for all it has low \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eR^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e mainly due to the large errors by high variance data. You will miss the signal in the low variance data (p-value for x1 is higher in the fit all model).\u003c/p\u003e\n\u003ch1 id=\"loss-function-likelihood-pearson-correlation\"\u003eLoss Function, Likelihood, Pearson Correlation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#loss-function-likelihood-pearson-correlation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eUnder normality and homoscedasticity assumptions, the OLS loss function is equivalent to the negative log-likelihood of the Gaussian distribution.\u003c/li\u003e\n\u003cli\u003eAlso it is equivalent to the Pearson correlation coefficient between the predicted and actual values.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"independent-errors-assumption\"\u003eIndependent Errors Assumption\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#independent-errors-assumption\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eThe assumption that the errors are independent of each other.\u003c/li\u003e\n\u003cli\u003estatsmodels has a fix for p-values and varaince estimae (still fits normal OLS) but gives different summary results. \u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003eX_const \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e sm\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eadd_constant\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nols_model \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e sm\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eOLS\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ey_ar1\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e X_const\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003efit\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nrobust_model \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e ols_model\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eget_robustcov_results\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ecov_type\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'HAC'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e maxlags\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"computation-and-optimization\"\u003eComputation and Optimization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#computation-and-optimization\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eSolving \u003ccode\u003eAx = b\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"direct-inverse\"\u003eDirect Inverse\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#direct-inverse\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eStrict inverse using QR decomposition, Q is orthogonal matrix, R is upper triangular.\nYou can solve \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eR\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eRx = Q^{T}b\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eQ\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e using back substitution. Recursively solving for each element of x starting from the last row up to the first row.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003eX\u003c/mi\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eX(X^{T}X)^{-1} y\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\"\u003eX\u003c/span\u003e\u003cspan class=\"mclose\"\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e is \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(p^3) + O(p^{2}*n)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e∗\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e run time (matrix inversion is \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(p^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse Gram-Schmidt  to build QR decomposition \u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e∗\u003c/mo\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(p^2*n)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e∗\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e run time\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGram-Schmidt is doing regression on each feature one by one, orthogonally projecting the residuals onto the next feature.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYour book \"Elements of Statistical Learning\" has the exact algo.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn theory you can use Gaussian Elimination to calculate the inverse of A.\nThis is when you \"attach\" the identity matrix to A and do row operations to convert A to identity matrix, the identity matrix will become A inverse.\nThese operations are echelon form and are \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(p^3)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ep\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e run time.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"pseudo-inverse\"\u003ePseudo Inverse\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pseudo-inverse\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProduces more stable results and is strictly better than direct inverse in terms of forecasts\u003c/li\u003e\n\u003cli\u003eThe big-O run time could be larger for well-defined invertible matrices, but in practice it is faster.\u003c/li\u003e\n\u003cli\u003epseudoinverse instead of blowing up values close to 0 it suppresses them naturally giving stable results\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSVD*\u003c/strong\u003e decomposition is used to compute the pseudoinverse\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eA = U @ SIGMA @ V^T -\u003e A^+ = V @ SIGMA^+ @ U^T\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eU and V are orthonormal matrices\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"decomposition-methods\"\u003eDecomposition Methods\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#decomposition-methods\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLU decomposition: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmi\u003eU\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA=LU\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eLU\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e where L is lower triangular and U is upper triangular. Used to solve Ax=b by solving Ly=b and then Ux=y (simpler than taking the inverse directly)\u003c/li\u003e\n\u003cli\u003eCholesky decomposition (subcase of LU decomposition): \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA=LL^T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8413em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e for symmetric positive definite matrices.\u003c/li\u003e\n\u003cli\u003eQR decomposition: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmi\u003eR\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA=QR\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eQR\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e where Q is orthogonal and R is upper triangular. Used to solve Ax=b by solving Rx=Q^Tb.\u003c/li\u003e\n\u003cli\u003eSVD decomposition: \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eU\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eΣ\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eA=U\\Sigma V^T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eA\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8413em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\"\u003eU\u003c/span\u003e\u003cspan class=\"mord\"\u003eΣ\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\"\u003eV\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8413em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e where U and V are orthogonal and Σ is diagonal. Used for pseudoinverse and dimensionality reduction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca class=\"color-tag\" style=\"--tag-color: #fbdd7e;\" href=\"/dendron-wiki/notes/q334c9dkrbs70eq5gujgcb3\"\u003e#TODO\u003c/a\u003e check how to implement Gram-Schmidt in numpy/scipy.\u003c/p\u003e\n\u003cp\u003eI tested in practice, the Gram-Schmidt was slower, though.\u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003e \u003cb\u003eCODE\u003c/b\u003e \u003c/summary\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003eiimport numpy \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e time\n\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e numpy\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinalg \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e inv\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e solve\n\u003cspan class=\"token keyword\"\u003efrom\u003c/span\u003e scipy\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinalg \u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e qr\n\n\u003cspan class=\"token comment\"\u003e# Function for normal equation (direct inversion)\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003enormal_equation\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    XtX \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e X\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eT @ X\n    Xty \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e X\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eT @ y\n    beta \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e inv\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eXtX\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e @ Xty\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e beta\n\n\u003cspan class=\"token comment\"\u003e# Function for QR decomposition\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"token function\"\u003eqr_solution\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\n    Q\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e R \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e qr\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e mode\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'economic'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    beta \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e solve\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eR\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e Q\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eT @ y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"token keyword\"\u003ereturn\u003c/span\u003e beta\n\n\u003cspan class=\"token comment\"\u003e# Simulation parameters\u003c/span\u003e\nn \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e50000\u003c/span\u003e   \u003cspan class=\"token comment\"\u003e# number of samples (can increase for stress testing)\u003c/span\u003e\np \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e200\u003c/span\u003e    \u003cspan class=\"token comment\"\u003e# number of features\u003c/span\u003e\n\nnp\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nX \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandn\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003en\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e p\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandn\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003en\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Run and time the normal equation\u003c/span\u003e\nstart \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e time\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etime\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nbeta_normal \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e normal_equation\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ntime_normal \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e time\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etime\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token operator\"\u003e-\u003c/span\u003e start\n\u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string-interpolation\"\u003e\u003cspan class=\"token string\"\u003ef\"Normal Equation Time: \u003c/span\u003e\u003cspan class=\"token interpolation\"\u003e\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003etime_normal\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token format-spec\"\u003e.4f\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token string\"\u003e seconds\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Run and time the QR decomposition\u003c/span\u003e\nstart \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e time\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etime\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nbeta_qr \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e qr_solution\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003eX\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ntime_qr \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e time\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etime\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token operator\"\u003e-\u003c/span\u003e start\n\u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string-interpolation\"\u003e\u003cspan class=\"token string\"\u003ef\"QR Decomposition Time: \u003c/span\u003e\u003cspan class=\"token interpolation\"\u003e\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003etime_qr\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token format-spec\"\u003e.4f\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token string\"\u003e seconds\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Check the difference in solution (should be very small!)\u003c/span\u003e\ndiff \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003elinalg\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enorm\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ebeta_normal \u003cspan class=\"token operator\"\u003e-\u003c/span\u003e beta_qr\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e \u003cspan class=\"token comment\"\u003e# sum(abs(diff))^(0.5)\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003eprint\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string-interpolation\"\u003e\u003cspan class=\"token string\"\u003ef\"L2 norm of difference between solutions: \u003c/span\u003e\u003cspan class=\"token interpolation\"\u003e\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003ediff\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e\u003cspan class=\"token format-spec\"\u003e.2e\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Output\u003c/span\u003e\n\u003cspan class=\"token triple-quoted-string string\"\u003e'''\nNormal Equation Time: 0.0389 seconds\nQR Decomposition Time: 0.5719 seconds\nL2 norm of difference between solutions: 5.85e-16\n'''\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003ch1 id=\"feature-selection\"\u003eFeature Selection\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#feature-selection\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eForward or backward selection methods are O(n^2)\u003c/li\u003e\n\u003cli\u003eCan use regularization for feature selection\u003c/li\u003e\n\u003cli\u003eIn practice if you have many features that are noisy or collinear, regularization will help with forecasting performance.\u003c/li\u003e\n\u003cli\u003ehowever if you choose the \u003cstrong\u003etop correlated\u003c/strong\u003e with the target features that could outperform fitting regularized LR on all features,\nsince you'd try to fit noise. Lasso might remove the noisy features but it still struggles.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"regularization\"\u003eRegularization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regularization\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eRegularization can be used in two ways: \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eto reduce the variance of the model\u003c/li\u003e\n\u003cli\u003eto perform feature selection\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFeature selection and regularization can be connected\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThey can be seen as L0 regularization (penalizing the number of non-zero coefficients)\u003c/li\u003e\n\u003cli\u003eL1 and L2 regularization are convex problems. The loss with regularization is a SUM OF CONVEX functions!\u003c/li\u003e\n\u003cli\u003eMinimizing the loss functions is equivalent to solving a\nconstrained optimization problem = minimizing OLS + L1/L2 constraint on the coefficients.\u003c/li\u003e\n\u003cli\u003ewhen you think of the constrained problems you can draw the OLS loss and the L1/L2 constraints and see where they intersect.\u003c/li\u003e\n\u003cli\u003eL2 constraint is a circle, L1 is a diamond.\nThe corners of the diamond are more likely to intersect with the OLS loss contours leading to sparse solutions.\nIn multiple dimensions the L1 constraint is a hypercube with many corners.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSolutions:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIf X is orthonormal, then the solutions to lasso and ridge are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elasso: sign(beta) * max(|beta|-lambda,0)  where beta is the solution to OLS\u003c/li\u003e\n\u003cli\u003eridge: beta / (1+lambda)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"ridge\"\u003eRidge\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ridge\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRidge (L2) regularization is a scaling regularization (divides the OLS solution by a factor (1+lambda))\u003c/li\u003e\n\u003cli\u003eRidge regularization is PCA regression with soft-threshold. It shrinks the coefficients of the principal components.\nIt shrinks more the coefficients with low variance (less important components)\u003c/li\u003e\n\u003cli\u003eRidge is equivalent to maximizing the posterior mean with prior beta ~ Normal(0,rho). then lambda = sigma/rho (sigma is the error variance)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lasso\"\u003eLasso\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lasso\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLasso (L1) regularization \u003c/li\u003e\n\u003cli\u003esparse solutions (think of the constraint problem is a diamond)\u003c/li\u003e\n\u003cli\u003econvex problem - no closed form solution - use coordinate descent, stochastic descent or LARS\u003c/li\u003e\n\u003cli\u003eLasso is equivalent to maximizing the posterior mode with prior beta ~ Laplace(0,b). then lambda = sigma/b (sigma is the error variance)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSay you have MSE with constraint |beta| \u003e 5, then this is NOT convex - you could be on the wrong side of the parabola.\u003c/p\u003e\n\u003ch1 id=\"log-scale\"\u003eLog Scale\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#log-scale\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eMake better visualizations when there is an outlier in the target variable.\u003c/p\u003e\n\u003cp\u003eNote how the y axis does not have equally distributed points. from 10^1 to 10^2 is the same length as from 10^2  to 10^3 \u003c/p\u003e\n\u003cdetails\u003e\n\u003csummary\u003e \u003cb\u003eCODE\u003c/b\u003e \u003c/summary\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e matplotlib\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003epyplot \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"token keyword\"\u003eimport\u003c/span\u003e seaborn \u003cspan class=\"token keyword\"\u003eas\u003c/span\u003e sns\n\n\u003cspan class=\"token comment\"\u003e# Sample data: 20 points lying roughly on a line, plus one big outlier\u003c/span\u003e\nnp\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eseed\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nx \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003earange\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e21\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e \u003cspan class=\"token operator\"\u003e*\u003c/span\u003e x \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e \u003cspan class=\"token number\"\u003e1\u003c/span\u003e \u003cspan class=\"token operator\"\u003e+\u003c/span\u003e np\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003erandom\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003enormal\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003escale\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e size\u003cspan class=\"token operator\"\u003e=\u003c/span\u003ex\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003esize\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\ny\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token operator\"\u003e-\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e \u003cspan class=\"token number\"\u003e1000\u003c/span\u003e  \u003cspan class=\"token comment\"\u003e# outlier\u003c/span\u003e\ndf \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e pd\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eDataFrame\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e{\u003c/span\u003e\u003cspan class=\"token string\"\u003e'x'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e x\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token string\"\u003e'y'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e:\u003c/span\u003e y\u003cspan class=\"token punctuation\"\u003e}\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\nfig\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e axs \u003cspan class=\"token operator\"\u003e=\u003c/span\u003e plt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003esubplots\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e figsize\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e12\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e \u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Linear scale scatter plot\u003c/span\u003e\nsns\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003escatterplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'x'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'y'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e data\u003cspan class=\"token operator\"\u003e=\u003c/span\u003edf\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e ax\u003cspan class=\"token operator\"\u003e=\u003c/span\u003eaxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'b'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e marker\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'o'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_title\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"Linear scale\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_xlabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"x\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e0\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_ylabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"y\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"token comment\"\u003e# Log scale scatter plot\u003c/span\u003e\nsns\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003escatterplot\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003ex\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'x'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e y\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'y'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e data\u003cspan class=\"token operator\"\u003e=\u003c/span\u003edf\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e ax\u003cspan class=\"token operator\"\u003e=\u003c/span\u003eaxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e color\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'r'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e marker\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e'o'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_yscale\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e'log'\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e# equivalent to:\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e# axs[1].scatter(df['x'], np.log(df['y']), color='r')\u003c/span\u003e\n\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_title\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"Log scale (y-axis)\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_xlabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"x\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\naxs\u003cspan class=\"token punctuation\"\u003e[\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e]\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eset_ylabel\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"y (log scale)\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003etight_layout\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\nplt\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003eshow\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cul\u003e\n\u003cli\u003eWhen all points are close together (except for the outlier), the outlier dominates the linear y-range, which squeezes the other data against the axis.\u003c/li\u003e\n\u003cli\u003eWith log scale, both the regular points and the outlier are shown in proportion, so you can see ALL points—even values that differ by orders of magnitude.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/log_scale_viz.png\" alt=\"alt text\"\u003e\u003c/p\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1744237493819,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"ea936af1aea818f3052610daac103a63","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","v77wdzobzcackzimz6a7crv"],"parent":null,"data":{},"body":"\nWelcome to my Knowledge Base! Here I write about my perception of life, document exciting things I've learned, debate (with myself) on controversial topics. If you know me you will not be surprised to find out that I write mostly about engineering and maths. Other topics I'm interested in are economics, politics, business, chess and poker."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"h79jpatf2zw7a0qpq4asniw"},"buildId":"hUMuUtBSCkU7YSeQn-rle","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>