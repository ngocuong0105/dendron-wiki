<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/dendron-wiki/favicon.ico"/><title>Distributed Computing</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Distributed Computing"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://ngocuong0105.github.io/dendron-wiki/notes/t0xvuftusxbozb7la786f9d/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/16/2025"/><meta property="article:modified_time" content="10/6/2025"/><link rel="canonical" href="https://ngocuong0105.github.io/dendron-wiki/notes/t0xvuftusxbozb7la786f9d/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" as="style"/><link rel="stylesheet" href="/dendron-wiki/_next/static/css/73bff7fc08ed1d26.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/dendron-wiki/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/dendron-wiki/_next/static/chunks/webpack-5a49f804ab2869ab.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/main-2a75a40d33729b12.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/_app-47f94d7e2ed9c5e2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/dendron-wiki/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_buildManifest.js" defer=""></script><script src="/dendron-wiki/_next/static/AZXonBxzXDJ7mmxS9Br0O/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="distributed-computing">Distributed Computing<a aria-hidden="true" class="anchor-heading icon-link" href="#distributed-computing"></a></h1>
<h1 id="spark">Spark<a aria-hidden="true" class="anchor-heading icon-link" href="#spark"></a></h1>
<p><strong>Terminology</strong></p>
<ul>
<li>node = computer (VM, physical machine)</li>
<li>cluster = group of nodes</li>
<li>executor = process that runs on a node</li>
<li>driver = main process that coordinates the execution of tasks across the cluster</li>
</ul>
<p><strong>Abstractions</strong></p>
<ul>
<li>every spark application consists of a <strong>driver</strong> program that runs the user's main function and runs a set of <strong>executor</strong> processes in <em>parallel</em> on a cluster</li>
<li>RDD = resilient distributed dataset. This is the main <em>abstraction</em> that Spark provides. It is a collection of objects that can be ran in parrallel across a cluster</li>
<li>Users may ask Sprk to <strong>persis</strong> and RDD in memory</li>
<li>RDDs automatically recover from node failures</li>
<li>2nd <em>abstraction</em> that Spark provides is <strong>shared variables</strong> that can be used in parallel operations. There are 2 types of shared variables: <strong>broadcast variables</strong> (shared among all nodes) and <strong>accumulators</strong> (sums,counts)</li>
</ul>
<p><strong>RDD Operations</strong></p>
<ul>
<li>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. </li>
<li>All transformations in Spark are <strong>lazy</strong></li>
<li>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the <strong>persist (or cache)</strong> method</li>
<li><strong>Shuffle Write</strong> is the amount of data executors had to write to other executors so the other executor could read.</li>
<li><strong>Shuffle Read</strong> is the amount of data executors read that was provided by another executor</li>
</ul>
<p><strong>Shuffling</strong></p>
<ul>
<li>Shuffling is the process where data is redistributed across different executors (nodes/machines) in the Spark cluster — usually because it needs to group data differently than it was originally stored.</li>
<li>groupBy, distinct, reduceByKey, sortBy reaarange the data so lal records with the same key (join key) end up on the same partition/executor.</li>
<li>In ideal cases (no task failures, no data loss, perfect partitioning), <strong>Shuffle Write ≈ Shuffle Read.</strong></li>
</ul>
<h1 id="pyspark">PySpark<a aria-hidden="true" class="anchor-heading icon-link" href="#pyspark"></a></h1>
<h2 id="properties">Properties<a aria-hidden="true" class="anchor-heading icon-link" href="#properties"></a></h2>
<ul>
<li>To start a spark session you need to pass a SparkConf file in the SparkContext</li>
</ul>
<p><strong>Driver</strong></p>
<ul>
<li>spark.driver.memory = 16g Memory for the driver process (the "main program")</li>
<li>spark.driver.cores = 4 Number of CPU cores for the driver process</li>
</ul>
<p><strong>Executor</strong></p>
<ul>
<li>spark.executor.memory = 28g. Amount of memory for each executor process.</li>
<li>spark.executor.cores = 4 Number of CPU cores for each executor. Each core can handle up to 7g data in memory</li>
<li>spark.cores.max = 36. total number of cores in the cluster. So you can have 9 executors with 4 cores each</li>
<li>spark.executor.instances 0. Static executors to launch. 0 means not set—dynamic allocation will control.</li>
</ul>
<p><strong>Executor Dynamic Allocation</strong></p>
<ul>
<li>spark.dynamicAllocation.enabled = true. Let spark dynamically allocate executors based on workload</li>
<li>spark.dynamicAllocation.maxExecutors = 9 Max number of executors Spark will request dynamically.
spark.dynamicAllocation.executorAllocationRatio</li>
<li>spark.dynamicAllocation.executorAllocationRatio = 0.8. Allocate fraction of the estimated required executors. Controls aggressiveness to avoid overloading the cluster.
Say you have 100 tasks and an executer on averages executes 4 tasks. So we will need 25 tasks. If we set ratio 0.8, spark will allocate 0.8*25=20 executors</li>
</ul>
<p><strong>SQL</strong></p>
<ul>
<li>spark.sql.autoBroadcastJoinThreshold = 134217728 (128MB). If a table is smaller than this, Spark will broadcast it to avoid shuffles in joins.</li>
<li>spark.sql.shuffle.partitions = 200. After a shuffle operation (e.g join, groupby) Spark will output the data in chunks (number of output partitions)</li>
<li>Too low number, some executors will be overloaded with large partitions, leadint to slow or skewed jobs or even OOM errors</li>
<li>Too high number, will lead to scheduling overhead and many small tasks, increasing job time</li>
<li>A good value is usually 2–4 × total executor cores in your cluster, but it depends on <strong>data size and job characteristics.</strong></li>
</ul>
<p><a href="https://drive.google.com/file/d/1Dz5x9OPOYFs0nczzfeR7QBNY_tbB11v8/view?usp=drive_link">Notebook</a></p>
<p>PySpark is a Python API that allows you to use the power of Apache Spark - fast &#x26; scalable big data processing system.</p>
<ul>
<li>you write Python code</li>
<li>PySpark lets tat code run on many computers at once (a "cluster")</li>
<li>analyze, process and transform huge datasets that would not fit on a single computer's memory</li>
<li>distributed processing - distribute the data in multiple computers and splits up the work</li>
<li>supports Pandas like code + SQL queries</li>
</ul>
<h1 id="optimizations">Optimizations<a aria-hidden="true" class="anchor-heading icon-link" href="#optimizations"></a></h1>
<ul>
<li>cache initialized data. Whenever you reuse a dataframe in a for loop </li>
</ul>
<pre class="language-python"><code class="language-python"> 
</code></pre>
<h1 id="pyspark-1">PySpark<a aria-hidden="true" class="anchor-heading icon-link" href="#pyspark-1"></a></h1>
<p>Example spark configs:</p>
<pre><code>spark.executor.memory: 28g
spark.executor.memoryOverhead: 8g
spark.executor.cores: 4
spark.executor.instances: 256 # total number of executors, not needed if using dynamic allocation
spark.dynamicAllocation.initialExecutors: 64
spark.dynamicAllocation.minExecutors: 64
spark.dynamicAllocation.maxExecutors: 256
spark.driver.memory: 28g # DRIVER!
</code></pre>
<ul>
<li>spark is a distributed computing framework that allows you to work with large datasets across a cluster of machines/computers</li>
<li><strong>worker node</strong> is typically one physical or virtual computer in the cluster</li>
<li><strong>driver node</strong> is the main process that coordinates the execution of tasks across the cluster. It is responsible for creating the SparkContext, which is the entry point to using Spark.</li>
<li>spark executor is a <strong>process</strong> that runs on each worker node in the cluster and is responsible for executing tasks and managing resources. Like a separate Python interpreter that runs on the worker node.</li>
<li>node can have multiple executors running on it.</li>
<li>each executor has its own memory and CPU resources allocated to it, which are used to execute tasks in <strong>parallel</strong>.</li>
<li><strong>executor.cores</strong>: each executor can have multiple cores (i.e threads) that can execute tasks <strong>concurrently</strong>.</li>
<li><strong>executor instances (spark.executor.instances)</strong>: total number of executors (processes) launched on all worker nodes in the clust</li>
<li>If using dynamic allocation, leave spark.executor.instances unset or set min/max via:</li>
</ul>
<pre><code>spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors
spark.dynamicAllocation.maxExecutors
</code></pre>
<p><strong>Why to choose dynamic allocation?</strong></p>
<ul>
<li>Variable Input Size</li>
<li>Fluctuating Resource Needs - joins, groupby-s, aggregating and exploding data</li>
<li>shared, multi-user cluster : release executors when idle so others can use those resources</li>
<li>cost optimization, not paying for idle resources</li>
<li>long-running applications, your job scales up and down depending on activity</li>
</ul>
<p><strong>When NOT to Use Dynamic Allocation</strong></p>
<ul>
<li>Your resource needs are steady and predictable (and you’re on a dedicated cluster).</li>
<li>Very short jobs or jobs with very short "bursts" of high demand (executor startup delays can hurt performance).</li>
<li>You depend on RDD caching across all executors (since dynamic allocation can kill executors, losing cached data).</li>
</ul>
<p>Driver and Workers:</p>
<p><img src="/dendron-wiki/./assets/images/spark_driver_worker.png" alt="alt text"></p>
<h1 id="example-optimization">Example optimization<a aria-hidden="true" class="anchor-heading icon-link" href="#example-optimization"></a></h1>
<p>Hey! After some experiment runs I've updated and chose these spark parameters</p>
<pre><code>    spark.executor.memory: 28g
    spark.executor.memoryOverhead: 3g
    spark.executor.cores: 4
    spark.dynamicAllocation.initialExecutors: 8
    spark.dynamicAllocation.minExecutors: 8
    spark.dynamicAllocation.maxExecutors: 32
    spark.driver.memory: 28g
</code></pre>
<p>Run log</p>
<p>Run time 360 seconds, cost 1.41$ (previously it was 7$)</p>
<p>CPU average usage by the whole Pod is~30-40%. I won't decrease more the amount of executors since 8 to 32 dynamically allocated is already low compared to other jobs in this service.</p>
<p>Memory average usage by the whole Pod is 50-65% - this is ok since we want some room left</p>
<p>I use dynamic number of executors instead of fixed since CAPI has variable size input (as we onboard/churn clients num conversions can change a lot, matches data also depends a lot on the quality of the conversion client send to us which can vary over time). Also there are a few joins/groupbys in the DQS job that require variate amount of resources</p>
<p>Spark recommends memoryOverhead of executor to be about 6-10% of the container size = spark.executor.memory + spark.executor.memoryOverhead. I chose ~10% to be on the safe size</p>
<h1 id="spark-monitoring">Spark Monitoring<a aria-hidden="true" class="anchor-heading icon-link" href="#spark-monitoring"></a></h1></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark" title="Spark">Spark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pyspark" title="PySpark">PySpark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#properties" title="Properties">Properties</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#optimizations" title="Optimizations">Optimizations</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pyspark-1" title="PySpark">PySpark</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#example-optimization" title="Example optimization">Example optimization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#spark-monitoring" title="Spark Monitoring">Spark Monitoring</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"t0xvuftusxbozb7la786f9d","title":"Distributed Computing","desc":"","updated":1759744985124,"created":1752649602765,"custom":{},"fname":"engineering.Concepts.Distributed Computing","type":"note","vault":{"fsPath":"vault"},"contentHash":"9d34f4286087f55f5d5f009fbf968da2","links":[],"anchors":{"spark":{"type":"header","text":"Spark","value":"spark","line":8,"column":0,"depth":1},"pyspark":{"type":"header","text":"PySpark","value":"pyspark","line":40,"column":0,"depth":1},"properties":{"type":"header","text":"Properties","value":"properties","line":42,"column":0,"depth":2},"optimizations":{"type":"header","text":"Optimizations","value":"optimizations","line":80,"column":0,"depth":1},"pyspark-1":{"type":"header","text":"PySpark","value":"pyspark-1","line":90,"column":0,"depth":1},"example-optimization":{"type":"header","text":"Example optimization","value":"example-optimization","line":136,"column":0,"depth":1},"spark-monitoring":{"type":"header","text":"Spark Monitoring","value":"spark-monitoring","line":160,"column":0,"depth":1}},"children":[],"parent":"7khpigcji0ufpuptuqozcft","data":{}},"body":"\u003ch1 id=\"distributed-computing\"\u003eDistributed Computing\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#distributed-computing\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"spark\"\u003eSpark\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#spark\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eTerminology\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enode = computer (VM, physical machine)\u003c/li\u003e\n\u003cli\u003ecluster = group of nodes\u003c/li\u003e\n\u003cli\u003eexecutor = process that runs on a node\u003c/li\u003e\n\u003cli\u003edriver = main process that coordinates the execution of tasks across the cluster\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAbstractions\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eevery spark application consists of a \u003cstrong\u003edriver\u003c/strong\u003e program that runs the user's main function and runs a set of \u003cstrong\u003eexecutor\u003c/strong\u003e processes in \u003cem\u003eparallel\u003c/em\u003e on a cluster\u003c/li\u003e\n\u003cli\u003eRDD = resilient distributed dataset. This is the main \u003cem\u003eabstraction\u003c/em\u003e that Spark provides. It is a collection of objects that can be ran in parrallel across a cluster\u003c/li\u003e\n\u003cli\u003eUsers may ask Sprk to \u003cstrong\u003epersis\u003c/strong\u003e and RDD in memory\u003c/li\u003e\n\u003cli\u003eRDDs automatically recover from node failures\u003c/li\u003e\n\u003cli\u003e2nd \u003cem\u003eabstraction\u003c/em\u003e that Spark provides is \u003cstrong\u003eshared variables\u003c/strong\u003e that can be used in parallel operations. There are 2 types of shared variables: \u003cstrong\u003ebroadcast variables\u003c/strong\u003e (shared among all nodes) and \u003cstrong\u003eaccumulators\u003c/strong\u003e (sums,counts)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eRDD Operations\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. \u003c/li\u003e\n\u003cli\u003eAll transformations in Spark are \u003cstrong\u003elazy\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the \u003cstrong\u003epersist (or cache)\u003c/strong\u003e method\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShuffle Write\u003c/strong\u003e is the amount of data executors had to write to other executors so the other executor could read.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShuffle Read\u003c/strong\u003e is the amount of data executors read that was provided by another executor\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eShuffling\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eShuffling is the process where data is redistributed across different executors (nodes/machines) in the Spark cluster — usually because it needs to group data differently than it was originally stored.\u003c/li\u003e\n\u003cli\u003egroupBy, distinct, reduceByKey, sortBy reaarange the data so lal records with the same key (join key) end up on the same partition/executor.\u003c/li\u003e\n\u003cli\u003eIn ideal cases (no task failures, no data loss, perfect partitioning), \u003cstrong\u003eShuffle Write ≈ Shuffle Read.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"pyspark\"\u003ePySpark\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pyspark\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"properties\"\u003eProperties\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#properties\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTo start a spark session you need to pass a SparkConf file in the SparkContext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDriver\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003espark.driver.memory = 16g Memory for the driver process (the \"main program\")\u003c/li\u003e\n\u003cli\u003espark.driver.cores = 4 Number of CPU cores for the driver process\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExecutor\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003espark.executor.memory = 28g. Amount of memory for each executor process.\u003c/li\u003e\n\u003cli\u003espark.executor.cores = 4 Number of CPU cores for each executor. Each core can handle up to 7g data in memory\u003c/li\u003e\n\u003cli\u003espark.cores.max = 36. total number of cores in the cluster. So you can have 9 executors with 4 cores each\u003c/li\u003e\n\u003cli\u003espark.executor.instances 0. Static executors to launch. 0 means not set—dynamic allocation will control.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExecutor Dynamic Allocation\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003espark.dynamicAllocation.enabled = true. Let spark dynamically allocate executors based on workload\u003c/li\u003e\n\u003cli\u003espark.dynamicAllocation.maxExecutors = 9 Max number of executors Spark will request dynamically.\nspark.dynamicAllocation.executorAllocationRatio\u003c/li\u003e\n\u003cli\u003espark.dynamicAllocation.executorAllocationRatio = 0.8. Allocate fraction of the estimated required executors. Controls aggressiveness to avoid overloading the cluster.\nSay you have 100 tasks and an executer on averages executes 4 tasks. So we will need 25 tasks. If we set ratio 0.8, spark will allocate 0.8*25=20 executors\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003espark.sql.autoBroadcastJoinThreshold = 134217728 (128MB). If a table is smaller than this, Spark will broadcast it to avoid shuffles in joins.\u003c/li\u003e\n\u003cli\u003espark.sql.shuffle.partitions = 200. After a shuffle operation (e.g join, groupby) Spark will output the data in chunks (number of output partitions)\u003c/li\u003e\n\u003cli\u003eToo low number, some executors will be overloaded with large partitions, leadint to slow or skewed jobs or even OOM errors\u003c/li\u003e\n\u003cli\u003eToo high number, will lead to scheduling overhead and many small tasks, increasing job time\u003c/li\u003e\n\u003cli\u003eA good value is usually 2–4 × total executor cores in your cluster, but it depends on \u003cstrong\u003edata size and job characteristics.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://drive.google.com/file/d/1Dz5x9OPOYFs0nczzfeR7QBNY_tbB11v8/view?usp=drive_link\"\u003eNotebook\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePySpark is a Python API that allows you to use the power of Apache Spark - fast \u0026#x26; scalable big data processing system.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eyou write Python code\u003c/li\u003e\n\u003cli\u003ePySpark lets tat code run on many computers at once (a \"cluster\")\u003c/li\u003e\n\u003cli\u003eanalyze, process and transform huge datasets that would not fit on a single computer's memory\u003c/li\u003e\n\u003cli\u003edistributed processing - distribute the data in multiple computers and splits up the work\u003c/li\u003e\n\u003cli\u003esupports Pandas like code + SQL queries\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"optimizations\"\u003eOptimizations\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#optimizations\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ecache initialized data. Whenever you reuse a dataframe in a for loop \u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-python\"\u003e\u003ccode class=\"language-python\"\u003e \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"pyspark-1\"\u003ePySpark\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pyspark-1\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eExample spark configs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003espark.executor.memory: 28g\nspark.executor.memoryOverhead: 8g\nspark.executor.cores: 4\nspark.executor.instances: 256 # total number of executors, not needed if using dynamic allocation\nspark.dynamicAllocation.initialExecutors: 64\nspark.dynamicAllocation.minExecutors: 64\nspark.dynamicAllocation.maxExecutors: 256\nspark.driver.memory: 28g # DRIVER!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003espark is a distributed computing framework that allows you to work with large datasets across a cluster of machines/computers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eworker node\u003c/strong\u003e is typically one physical or virtual computer in the cluster\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edriver node\u003c/strong\u003e is the main process that coordinates the execution of tasks across the cluster. It is responsible for creating the SparkContext, which is the entry point to using Spark.\u003c/li\u003e\n\u003cli\u003espark executor is a \u003cstrong\u003eprocess\u003c/strong\u003e that runs on each worker node in the cluster and is responsible for executing tasks and managing resources. Like a separate Python interpreter that runs on the worker node.\u003c/li\u003e\n\u003cli\u003enode can have multiple executors running on it.\u003c/li\u003e\n\u003cli\u003eeach executor has its own memory and CPU resources allocated to it, which are used to execute tasks in \u003cstrong\u003eparallel\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eexecutor.cores\u003c/strong\u003e: each executor can have multiple cores (i.e threads) that can execute tasks \u003cstrong\u003econcurrently\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eexecutor instances (spark.executor.instances)\u003c/strong\u003e: total number of executors (processes) launched on all worker nodes in the clust\u003c/li\u003e\n\u003cli\u003eIf using dynamic allocation, leave spark.executor.instances unset or set min/max via:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003espark.dynamicAllocation.enabled=true\nspark.dynamicAllocation.minExecutors\nspark.dynamicAllocation.maxExecutors\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eWhy to choose dynamic allocation?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVariable Input Size\u003c/li\u003e\n\u003cli\u003eFluctuating Resource Needs - joins, groupby-s, aggregating and exploding data\u003c/li\u003e\n\u003cli\u003eshared, multi-user cluster : release executors when idle so others can use those resources\u003c/li\u003e\n\u003cli\u003ecost optimization, not paying for idle resources\u003c/li\u003e\n\u003cli\u003elong-running applications, your job scales up and down depending on activity\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eWhen NOT to Use Dynamic Allocation\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYour resource needs are steady and predictable (and you’re on a dedicated cluster).\u003c/li\u003e\n\u003cli\u003eVery short jobs or jobs with very short \"bursts\" of high demand (executor startup delays can hurt performance).\u003c/li\u003e\n\u003cli\u003eYou depend on RDD caching across all executors (since dynamic allocation can kill executors, losing cached data).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDriver and Workers:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/dendron-wiki/./assets/images/spark_driver_worker.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003ch1 id=\"example-optimization\"\u003eExample optimization\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-optimization\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eHey! After some experiment runs I've updated and chose these spark parameters\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    spark.executor.memory: 28g\n    spark.executor.memoryOverhead: 3g\n    spark.executor.cores: 4\n    spark.dynamicAllocation.initialExecutors: 8\n    spark.dynamicAllocation.minExecutors: 8\n    spark.dynamicAllocation.maxExecutors: 32\n    spark.driver.memory: 28g\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun log\u003c/p\u003e\n\u003cp\u003eRun time 360 seconds, cost 1.41$ (previously it was 7$)\u003c/p\u003e\n\u003cp\u003eCPU average usage by the whole Pod is~30-40%. I won't decrease more the amount of executors since 8 to 32 dynamically allocated is already low compared to other jobs in this service.\u003c/p\u003e\n\u003cp\u003eMemory average usage by the whole Pod is 50-65% - this is ok since we want some room left\u003c/p\u003e\n\u003cp\u003eI use dynamic number of executors instead of fixed since CAPI has variable size input (as we onboard/churn clients num conversions can change a lot, matches data also depends a lot on the quality of the conversion client send to us which can vary over time). Also there are a few joins/groupbys in the DQS job that require variate amount of resources\u003c/p\u003e\n\u003cp\u003eSpark recommends memoryOverhead of executor to be about 6-10% of the container size = spark.executor.memory + spark.executor.memoryOverhead. I chose ~10% to be on the safe size\u003c/p\u003e\n\u003ch1 id=\"spark-monitoring\"\u003eSpark Monitoring\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#spark-monitoring\"\u003e\u003c/a\u003e\u003c/h1\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"t0xvuftusxbozb7la786f9d"},"buildId":"AZXonBxzXDJ7mmxS9Br0O","assetPrefix":"/dendron-wiki","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>