{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr).","n":0.277}}},{"i":2,"$":{"0":{"v":"Daily","n":1}}},{"i":3,"$":{"0":{"v":"Journal","n":1}}},{"i":4,"$":{"0":{"v":"2025","n":1}}},{"i":5,"$":{"0":{"v":"06","n":1}}},{"i":6,"$":{"0":{"v":"2025-06-09","n":1},"1":{"v":"\n\nMeta layoff-s bottom five percent in performance.\n\nHires new people\n\nBest to join tech company is when the stock is down\n\nNow it is an employers market. Meta can afford to remove the bottom percent and hire new people (cheaper).\n\nIn Quant Hedge Funds it doesn't matter the company you work for, matters the team you work in and the pod of your team. The Portfolio Manager of the team and so on. Citadel has many pods within and all teams within compete with each other.","n":0.11}}},{"i":7,"$":{"0":{"v":"2025-06-06","n":1},"1":{"v":"# PySpark\n\n- Check point\n- Cache\n- persist to disk\n\nHigh quality applications better than spam applications.","n":0.267}}},{"i":8,"$":{"0":{"v":"2025-06-05","n":1},"1":{"v":"\n# Presentation Skills\n\n7 habits that quietly kill your influence: \n\n- Starting with the data\n- Using passive language\n- Presenting 5 insights at once\n- Explaining everything you did\n- Assuming they remember context\n- Avoiding giving a recommendation\n- Using visuals that require decoding\n\nInstead:\n\n- Start with what’s at stake / the opportunity\n- Use direct and active language (own it!)\n- Present 1 idea (+ supporting evidence)\n- Explain your point, not your process\n- Recap context before you dig in\n- Recommend a few next steps\n- Use visuals that tell a story\n","n":0.11}}},{"i":9,"$":{"0":{"v":"2025-06-03","n":1},"1":{"v":"\n# Negotiation\n\nHonest with current salary, but get competing offers. Apply at multiple places. Always tell the recruiter that you have more places where you apply.","n":0.2}}},{"i":10,"$":{"0":{"v":"05","n":1}}},{"i":11,"$":{"0":{"v":"2025-05-29","n":1},"1":{"v":"# CAP Theorem\n\n- Consistency\n- Availability\n- Partition Tolerance\n\n2 Nodes + Database then you have the CAP Theorem\n\nConsistency: \n","n":0.243}}},{"i":12,"$":{"0":{"v":"2025-05-15","n":1}}},{"i":13,"$":{"0":{"v":"2025-05-14","n":1},"1":{"v":"# Development Seminar during Hackathon\n\nOwn your development, manager used as guidance only. \n\n70/20/10 Rule\n- 70% of learning things is during work, hard challenges, doing stuff\n- 20% of learning is through Networking, Social interactions\n- Formal Training / Courses 10%\n\n\n- Challenge the Status Quo - think about something tangible\n\nDistribution of the pie, of your development:\n- Performance 10%\n- Image (gow others perceive you and how you perceive yourself) - your brand! 30%\n   - authenticity\n   - credibility\n   - inclusivity\n\n- Exposure 60% - tune your own horn","n":0.11}}},{"i":14,"$":{"0":{"v":"2025-05-13","n":1},"1":{"v":"\n# Pomodoros\n\n1. Hackathon","n":0.577}}},{"i":15,"$":{"0":{"v":"2025-05-12","n":1},"1":{"v":"\n# Random facts\n- Male duck is called drake\n- New pope Leo 16th\n- Pope Francis died day after meeting J.D. Vance. \n- Queen Elizabeth died two days after meeting Liz Truss\n- Orzo is Italian pasta that looks like rice\n- RFK Jr, nephew of JFK - secretary of health, anti-vaccine, conspiracy theorist, former drug addict\n","n":0.137}}},{"i":16,"$":{"0":{"v":"2025-05-06","n":1},"1":{"v":"\n# 28th Birthday\n\nHad a great party on Saturday. We did a themed \"communist\" party. Communist cocktails menu, a quiz to distinguish the fake comrades from the real ones.\n\n\nDear fellow comrades. Welcome to the communist party! Today is a very special day, however, a trusted source from KGB has told me that there might be a capitalist imposter among us. In order to identify them and to distinguish the fake comrades from the real ones we will be conducting a quiz. This quiz will test your knowledge and your loyalty to the communist party. \n\n\nGreat party ideas\n- fake dollars\n- cocktails bar\n- themed\n\n# Lucky Cat \n\nWent with Maya to Lucky Cat to celebrate birthday. ","n":0.094}}},{"i":17,"$":{"0":{"v":"2025-05-02","n":1}}},{"i":18,"$":{"0":{"v":"2025-05-01","n":1},"1":{"v":"\n# 28th B-Day\n\nThis year I will celebrate at home - Granger House. [finish investing, no career grinding]\n\n27th birthday I celebrated in a bar in Stratford. [chill + start investing]\n\n26th I celebrated in Whitechapel! [grind year and success]\n\n25th I celebrated at home in Bethnal Green. [grind year]\n\n24th I celebrated in bar Caldo. [grind year]\n\n23rd Oxford\n\nI wish last two years I have made more progress career-wise. Or at least were able to learn more things. I am currently in the trap of being alright. \n\n\n# Statically vs Dynamically typed languages\n\nPython is dynamically typed but supports static typing using tools like **mypy**\n\nstatically typed languages:\n\n- Java, C++\n- require defining explicit definition of the type of the variables\n- variables cannot change their type\n- type checks are done at compile time (before runtime)\n\ndynamically typed languages:\n\n- Python\n- no need to tell type of variables *x=5* , then *x= 'string'*\n- one variable can take different types\n- type checks done at runtime (a bit slower)\n- **type hinting** or **type annotation** *x: int = 5* is just used fo documentation purposes\n- you can use static type checker tool like *mypy*\n\n\n# DataFramely\n\nData Validation for DataFrames using schema-based approach. Allows you to do type, range, nullability checks at run-time. If they do not pass the program return a SchemaError.\n\n[QuantCo Polars DataFrame Validation Library](https://tech.quantco.com/blog/dataframely)\n\nPython does not support real static type checks. Tools like mypy introduces static type checks but are not able to check the contents of tables/dataframes. \n\nPandera is a tool that allows you to:\n- define a schema\n- do checks on dataframes at runtime\n- improve readability on dataframes objects and easier debugging\n\n\n\n","n":0.062}}},{"i":19,"$":{"0":{"v":"04","n":1}}},{"i":20,"$":{"0":{"v":"2025-04-30","n":1},"1":{"v":"\n# QRT interview\n\nBe more proactive during interviews.\n\n# Dinner at VietRest\n\nwow","n":0.316}}},{"i":21,"$":{"0":{"v":"2025-04-27","n":1},"1":{"v":"\n# Dendron Daily\n\n1. Daily journal\n2. Leetcode competition\n3. Finances\n4. Daily Exposure documentation + setup co-pilot + forma (1 pomo)\n5. Read & Journal Open AI blog + Read QuantCo dataframely article **1 pomo**\n6. MIT Lecture **3 pomos**","n":0.169}}},{"i":22,"$":{"0":{"v":"2025-04-26","n":1},"1":{"v":"\n# Sinners\n\nYesterday, we decided to be productive members of society and went to the movies to watch The Sinners. Because nothing says \"good life choices\" like watching a movie about questionable ones\n\nWe then marched over to YiQi, a Pan-Asian restaurant where — get this — every dish came with a little label telling you which country it was from. Because obviously, I need to know the exact coordinates of my ribs before eating them. F*ck those ribs, I loved them. The Pad Thai was good too \"tasty but slightly swimming in oil.\"\n\nTo \"walk it off,\" we wandered from Heytea through Piccadilly Circus, Trafalgar Square, and along the river near the London Eye. We sat in a random fancy park, took way too many flower photos, and pretended we were influencers. \n\nThe highlight? Admiring gum art on Millennium Bridge. Yes, actual chewing gum turned into \"art.\"","n":0.083}}},{"i":23,"$":{"0":{"v":"2025-04-25","n":1},"1":{"v":"\n# Dendron Daily\n\n1. Daily Leetcode + some prep (1 pomo)\n2. Leetcode Prep GSA (1 pomo)\n3. GSA Glassdoor + Engineering Interview Criteria (1 pomo)\n4. Interview Behavior Prep (1pomo)\n5. Onpoint (3 pomos)\n6. Daily Exposure documentation + setup co-pilot + forma (1 pomo)\n7. Read & Journal Open AI blog + Read QuantCo dataframely article **1 pomo**\n8. MIT Lecture **3 pomos**\n\n\n\n# GSA\n\nBe more proactive during interviews. Taylor your xperience to the role.","n":0.121}}},{"i":24,"$":{"0":{"v":"2025-04-24","n":1},"1":{"v":"# Daily pomos\n\n1. On point: **3 pomos**\n2. Daily Exposure documentation + setup co-pilot: **1 pomo**\n3. Leetcode daily + GSA prep **2 pomos**\n4. GSA Leetcoding **2 pomo**\n5. Jira planning + journal previous missed days: **1 pomo**\n6. Read & Journal Open AI blog + Read QuantCo dataframely article **1 pomo**\n7. MIT Lecture **3 pomos**\n\n## T-strings in python\n\nTemplate strings in python `t\"Insert {user_input}\"` would be a generalization of `f-strings`. Often people would use\nf-strings in SQL syntax and HTML strings when they should not! These are susceptible to SQL-ingestion attacks. t-strings\nwill be introduced in python late 2025 and will allow people to access the string components **before** combining\nthe string and the user input. This will allow developers to do safety checks against malicious user input.","n":0.091}}},{"i":25,"$":{"0":{"v":"2025-04-23","n":1},"1":{"v":"\n# Pomodoro method\n\nPomodoro means tomatoes in Italian\n\n1. Make a todo-list of tasks you need to finish\n2. Start pomodoro 25 minutes with 5 minutes break\n\nStops you from worrying about the endless tasks you need to finish, and makes you focus on the things you can get done now.\n\n\nTake 15 minutes at the beginning of your workday (or at the end if you're planning for the next day) to plan out your pomodoros. Take your to-do list for the day and note how many pomodoros each task will take.\n\nRemember, tasks that will take more than 5 pomodoros should be broken down into smaller, more manageable tasks. Smaller tasks, like responding to emails, can be batched together in a single Pomodoro.\n\nGet away from screen during a break. Coding tasks usually require more time to get into the zone, so you might combine pomodoros.\n\n# Daily pomodoro planning\n\n1. Leetcode Daily, Weekly problem + Pandas: **1 pomo**\n2. Leetcode Competition: **1h30min ~ 4 pomos**\n3. Recap Competition: **1 pomo**\n\n","n":0.079}}},{"i":26,"$":{"0":{"v":"2025-04-22","n":1},"1":{"v":"# Gym\n\nStrong urge to talk with em.","n":0.378}}},{"i":27,"$":{"0":{"v":"2025-04-21","n":1},"1":{"v":"# Chao em yeu\n\npretty emotional morning. Probably for the best. Yêu em – nhưng giờ chỉ còn lại là nỗi đau.","n":0.218}}},{"i":28,"$":{"0":{"v":"2025-04-20","n":1},"1":{"v":"# One day in Soho...\n\nChotto Matte, woow food was amazing. \n\nSoho shopping hehe. Great night at home.","n":0.243}}},{"i":29,"$":{"0":{"v":"2025-04-19","n":1},"1":{"v":"# Lucky Cat\n\n60th floor. Two proposals. Window seats. \n\nGot up early, had good morning kiss hehe and was quite fun overall. I felt our time passed very quickly.\n\nWatched \"How to loose a guy in 10 days\"","n":0.167}}},{"i":30,"$":{"0":{"v":"2025-04-18","n":1},"1":{"v":"\n#  Chain of Thought\n\n[o1 release](https://openai.com/index/learning-to-reason-with-llms/)\n\n\n- Essentially the model has been trained on how to think and it goes through an explicit reasoning process before giving its final answer.\n- The tradeoff being made is:\n- Vastly improved problem solving capabilities for queries where there are verifiably correct answers.\n- 100x or more slower than non-reasoning models since the LLM is producing all those internal reasoning tokens.  A “fast” query with a reasoning model is 10s before you get any output.  A slow query is minutes.\n- 5x more expensive than non-reasoning models.\n- This tradeoff is tunable.  The longer you let the model reason, the more capable it is.\n- **Important** The reasoning models do NOT improve the capabilities for subjective tasks.  OpenAI even observed that people tend to prefer the text of gpt-4o vs o1.  The improved capabilities are just for when there are verifiably correct answers.\n\n# Hot Pot and Poke\n\nNewfounland viewing 536 flats, 97% occupancy, hey tea and zara.\n\nHot Pot was disappointing. Poke was ok-ish - at least the salmon was good, chicken not great.\n\nDrank and had fun. Luckily not too drank. ","n":0.075}}},{"i":31,"$":{"0":{"v":"2025-04-17","n":1},"1":{"v":"# Dreams and the future\n\nPeople often overestimate what they can do in 12 months and underestimate what they can achieve in 5 to 10 years.\n\nLooking ahead for 1 year is just operational - annual planning.\n\nLooking ahead 5 years is setting long-term goal.\n\nLooking ahead 10 years is visionary.","n":0.146}}},{"i":32,"$":{"0":{"v":"2025-04-16","n":1},"1":{"v":"# o3 and o4-mini release\n\n#AI\n\n[youtube](https://www.youtube.com/watch?v=sq8GBPUb3rk&t=823s)\n\nToday OpenAI released its latest reasoning models. LOL - these model reach performance equivalent to the top 200 programmers in Codeforces.. this is super crazy. The presenters say that the core idea behind the technology is next token prediction.. And this works like magic - very impressive to be honest.\n\nFor the first time the reasoning models can use tools: code in Python, search the web, the terminal, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. So it is **multimodal**.\n\nThis reasoning models can browse your codebase and fix your code. It shows it logic, explanation and verifies the solution.\n\no3 is cheaper than o1, both training and inference. \n\nThey released codeX CLI tool that has this extra reasoning\n\n","n":0.088}}},{"i":33,"$":{"0":{"v":"2025-04-15","n":1},"1":{"v":"# AI Industry Developments\n\n#AI\n\n- Most LLMs are really just a fancy autocomplete. Given a set of words (tokens) \nthey are able to predict what the next word most likely is. They use billion of parameters to do this.\n- In 2022 LLM practitioners, realized that if they ask the LLM to show its thought process, and use it as prompts, LLMs\ncan do much better in reasoning tasks **chain of thought prompting**.\n- Open AI o1 wrapped this idea into a model and this model showed much better reasoning capabilities. **chain of thoughts (CoTs)**.\nThis new “reasoning model” goes through extensive internal reasoning in order to decompose problems, brainstorm ideas, check its work, iterate, etc.  Think of it like an internal monologue.\n- [**IMPORTANT**] Reasoning models are best when we have verifiable tasks. That is why they are good for coding since we can verify if a code is correct by writing tests.\n- One day LLMs reasoning models will be good at solving math problems too. Solutions are currently verifiable only by human, but once we are able to automate this process then models trained with chain of thoughts idea + self-verification will be able to solve hard math problems.\n\n## Reasoning Models\n\nAs of today best reasoning models are o3 (OpenAI), Sonnet 3.7 (Anthropic backed by Amazon)\n\nDeep Research \n\nOperators, Computer-Using Agent. This is fucking awesome. It opens the browser and starts looking at the screen, moves the mouse and clicks...\n\nAI Agent, Assistant, Bot\n\n- Agents perform complex operations, can reason on their own, and proactively make decisions. It has a \"self\" and is able to \"think\". Examples: Operators and Deep Research Products\n- Assistants assist human when working on tasks. Best example is the vanilla chatGPT, VsCode Co-pilot\n- Bot perform simple repetitive tasks, web scrappers, autocomplete, dial-call operators, classic call center support automation\n\n\n[o1 release, CoT](https://openai.com/index/learning-to-reason-with-llms/)\n\n\n","n":0.058}}},{"i":34,"$":{"0":{"v":"2025-04-14","n":1},"1":{"v":"\n# Shared repository model vs Fork & Pull model\n\nShared repo model is for smaller private teams where everyone works on the same repository. In fork and pull model developers create their own copy of the repo and work on it.\n\n\n# Scaling an app - Infra Engineer's POV\n\nMoving from a single monolith repo to services-oriented-architecture.Building a distributed computer system.\n\n# AI\n\nWow… I just watched a lecture from MIT’s Intro to Deep Learning course, and I can’t believe how much it has evolved over the past five years. It’s honestly mind-blowing. I felt this rush of excitement the entire time — like something clicked. I think this is it. This is what I want to do. ML, DL, AI… this whole world feels like home to me.\n\nIt’s crazy to think about how far we’ve come. Back in university, all those concepts felt so abstract — just math and theory. But now? I can literally talk to AI agents. I can generate images with a few lines of code. My computer can speak, understand, and even create. It’s like living in a sci-fi movie, except it’s real.\n\nI feel so inspired. The future feels wide open, and I want to be part of building it. There’s something magical about this field — the pace, the creativity, the impact. I’m dreaming big tonight. I want to learn more, build things, and maybe even teach one day. Who knows?\n\nLLMs does not have the classical DL problem of overfitting. They have other problems..","n":0.064}}},{"i":35,"$":{"0":{"v":"2025-04-13","n":1},"1":{"v":"# Ping from the past\n\nHit me like a wave. I've concealed her in my memory quietly but once I got that text, it hit me.\nHit me high and low. As she resurfaced, it brought me memories from the past. She sent me just a \nmessage - simple on the surface but stirred a storm inside me. I didn't expect it but deep inside me \nI know I have been waiting for it. The weirdest thing is, despite everything, I still want to do \nanything for her.\n\nHow do I feel? Mix of excitement, longing and confusion. Part of me lights up as I talked with her, \nbut another part of me is cautious as it remembers the pain from the betrayal. I can feel the love\nand excitement from the fact that we will see each other next week. But underneath that pull there is\nsomething heavier: the memory of being betrayed. It echoes the past clearly as a voice that I'd never\nbe able to forget - reminding me the sadness, the angriness, the emotional toll and the reasons \nwe broke up.\n\nWhy do I feel this way? Because betrayal does not erase love - it complicates it. Part of me wants\nto make the pain worth it. To turn the sadness and pain into opportunity. To rewrite the ending and \nturn the betrayal into redemption. \n\n\n# Keeping the past in the past and setting boundaries\n\nI've been thinking a lot since you reached out. Hearing from you brought up a lot of emotions. \nThere's still a part of me that cares, but there's also a part that remembers the pain, the betrayal,\nand the trust that was broken. \n\nI'm not angry. I'm not trying to rehash the past. But I've worked hard to heal, and \nI need to protect that progress. If you're reaching out because you genuinely want to talk — \nwith honesty and clarity — I'm open to that.\n\nBut if this is just a moment of nostalgia or uncertainty, I need to step back. I can't reopen wounds \nthat I've fought to close. I wish you well, truly. But I also wish peace for myself. ","n":0.053}}},{"i":36,"$":{"0":{"v":"2025-04-12","n":1},"1":{"v":"\n# The prohibition week\n\nOr also know as \"сухата седмица\"... Keep pushing it. It will be better. It is better day by day and you can feel it.\n\n# Digit DP\nChatGPT taught me a new DP technique for problems about counting integers with some special properties. AI is amazing...\n\n# Long time no catchup with...\n\nWith my little sister. Maya is amazing.. she is my little copy and my little upgrade. She is truly my soulmate and I adore her. Hunger games, birthday plans, studying plans, dystopia (the movie genre where we talk about the dystopian future i.e. the ugly future), the **new** seven wonders of the world, the great chinese wall, dollar/gbp conversion rates.. I love it when I can talk with Maya about everything. Didn't fucking know that the great chinese wall is 21k kms, aka half the equator!!\n\n\n","n":0.085}}},{"i":37,"$":{"0":{"v":"2025-04-11","n":1},"1":{"v":"# Horizontal vs Vertical Scaling\n\nMore machines vs better machines..\n\n# Latency vs Throughput\n\nLatency is measured in milliseconds, throughput is measured in requests per second, or queries per second. \n\nLow latency is like someone is being able to respond quickly to questions. High Throughput means is like someone who is great at multitasking.\n\nLow latency and acceptable throughput is needed for real-time applications like gaming. \n\nHigh throughput and acceptable latency is needed for batch processing. No need for immediate response but you want to get a lot of data from source to destination.\n\n\n# Gym\n\nHit it harder! This is what I was thinking during today's gym session. Five out of five, all exercises done. Listening to music during the session makes it feel longer. Without the headphones it is less distracting and all the focus is in hitting the gym. But.. there is always a but... My body becomes so energized that it does not sleep well after the gym session. I might have overtrained, usually 5/5 creates my sleeping problems. On Sunday I'll try to go to the gym early during the day, to see if it's better.","n":0.074}}},{"i":38,"$":{"0":{"v":"2025-04-10","n":1},"1":{"v":"\n#  The Dire Wolf Is Back - And It's Not Just a Game of Thrones Thing\n\nDe-extinction! Species that has been extinct for more than 10000 years are brought back to life. These cute little puppies (not puppets) are not only in GoT but in real life, were born somewhere in a lab in the USA. Colossal Biosciences claim to have deciphered the dire wolf genome. They rewrote the genetic code of a common gray wolf to match the dire wolf genome. After careful genetic engineering, Romulus, Remus and baby Khaleesi were born. \n\nWhat are dire wolfs? They are bigger, stronger and have larger jaws. Close to monsters. And you don't want to be close to them..\n\n# KISS\n\nKeep It Simple Stupid! Follow this principle in coding, in presenting, in marketing, in sales and in life... In order to convey your message effectively you need to make it simple and understandable!\n\n\n# Consciousness\n\nThe ability to be present, to really understand your surroundings and to be emotionally involved in what is happening. Consciousness is to be considerate, to be kind, to be emotionally intelligent. Be aware and responsive of the external environment around you.\n\nDon't just pretend to listen to other people. Really listen to them, understand what they are saying and debate!\n\n# Leetcoding\n\nDon't fall in the fix test case per test case loop. Sometimes you need to start over... I solved a hard problem that is to find the number of integer less than MAX, such that each digit is less than max_digit. Combinatorial problem where you need to be very precise...","n":0.062}}},{"i":39,"$":{"0":{"v":"2025-04-09","n":1},"1":{"v":"# Day One: My Journaling Journey Begins\n\nA fresh start - I will journal every day. This time for **REAL**! Every single day I will write my thoughts, express my feelings and document my progress. \nNo excuses! And my journal is not going to be the boring one where people just share what they did during the day, list their wins and losses, complain, \nrant or use it as a therapist. My journal will of course have all this but it will be with a style! Walltopia's marketing style! It will be a unique journal \nwhere I write my thoughts, express my feelings, celebrate my wins, acknowledge my losses and much more. This journal will improve my communication skills. \nI will be more precise and will explain my thoughts concisely. I will explain complex technical ideas in simple terms that is understandable to different audiences. \nI will have less misunderstandings with people around me. For this reason I will **journal**, **read out** loud whenever I read books and engage in more conversations. \nOccasionally, I will use chatGPT to help me improve my writings but instead of copy pasting information, I will read, understand it and write myself. \nThis will be true for all the things I read.\n\nExercises and practices to improve my communication skills:  \n- journal every day and read out loud what I've written, then record it\n- write summary of ideas that I've learned or read about\n- reading books out loud\n- record myself explaining ideas\n\n## Focus: My Greatest Enemy in The Past Year\n\nI start to work or study and then boom! I scroll through instagram, check my phone notifications, open a new tab... All the progress gone, I'm OUT of the zone. And time flies so quickly when I'm OUT of the zone. When I work or study it is much slower - looking at the Pomodoro clock it is as if it's frozen. \n\nI've realized that the trick is not some magical productivity hack or tool (though Pomodoro certainly helps and I will use it). The real trick? Catching myself during the act! Acknowledge that I'm being distracted and do not act on this distraction.\n\nBecause here is the thing - once I'm in the zone, I'm unstoppable! Keep pushing it. Once I get it rolling, it rolls by itself. It's a simple flow once I've started it. Hardest part is to start - so don't loose to meaningless distractions.\n\nUse Pomodoro to set and measure goals. Use focus times for focusing!\n\nThat's the game!  \nFocus, flow, reward...  \nRepeat!\n\n\n## The Long Overdue Plan\n\nNo more procrastination! No more delays! Time to execute. Jira tasks are created, planned and estimated. Sprint planning is on and I'll track my progress. Timelines are set. To the moon and beyond!\n\n\n## AI: Engineer's Threat or Hidden Opportunity..\n\nLayoffs, layoffs, layoffs. No need for junior software engineers. AI can do everything a junior can do. Efficiency, cost-cutting, recession. These are all the buzz words I see on LinkedIn, hear from colleagues and what state-of-the art AI owners talk about. Will AI take our jobs?  \n\nThat I don't know. One thing is sure though - AI will **change** our jobs. Software engineers, applied scientists, quant researches none of those jobs will be the same. AI brings one of the biggest automation in day to day responsibilities for tech practitioners. Whether it will be able to replace or not, I think it is not with high probability. But this is just my opinion, other people have different take.  \n\nWhat I know is that these are **VERY EXCITING TIMES TO LIVE IN**. It is the biggest opportunity for developers to increase their productivity by a factor of at least 10. Companies will hire people who are able to use AI and be hyper productive. More people will built more products more efficiently. We live in times where things will move even faster! The task of being able to tell the computer what you really want is even more important. AI can output only what you require it to output!\n\n","n":0.039}}},{"i":40,"$":{"0":{"v":"Work","n":1}}},{"i":41,"$":{"0":{"v":"Yelp","n":1},"1":{"v":"\n- argue from basic principles\n- whatever problem they make you solve argue from the basics\n- When you solve problems try to generalize\n\nE.g problem with matching you don't need to consider specific PII info like email, phone, IP,UA, MAIDID. Think about strong and weak indices. Use more general terms.\n\nProblem framing is all about Generalizable Ideas, Generalizable Problem Framing and so on.\n\nData Quality Score learnings\n- argue from basic principles\n- functional thinking, map raw data to quality\n- probability thinking, \n- add normalizing function to have dqs between 0 and 1, 1/x easiet pu looses interpretability and can lead to very small numbers\n- think about expectations, divergence from expected match rate, lol\n- expected match rate modeling\n\n\n- bias vs variance\n\nF1 Score\n- precision vs recall\n\n\n# A/B Testing\n\n","n":0.091}}},{"i":42,"$":{"0":{"v":"How was Yelp built?","n":0.5},"1":{"v":"\n# From monolith code base to service-oriented-architecture\n\nYelp has 2 main bodies of code - **services** and **yelp-main**. Yelp started with a monorepo yelp-main - a single application like a swiss knife that is doing everything, handling every user and every request. This is very common for tech companies. As their user base grow, their problems grow proportionally. The codebase becomes too complicated, concerns are not cleanly separated, many changes from many engineers go out together and performance of the whole system is measured by the performance of the worst part of the system. It is not scalable. The solution to this problem is services: individual, smaller code bases, running on separate machines with separate deployment cycles, that more cleanly address more targeted problem domains. This is called a Service-Oriented-Architecture: SOA.\n\nAs you build out services in your architecture, you will notice that instead of maintaining a single pool of general-purpose application servers, you are now maintaining many smaller pools. This leads to a number of problems. How do you direct traffic to each machine in the pool? How do you add new machines and remove broken or retired ones? What is the impact of a single broken machine on the rest of the application?\n\nDealing with these questions, across a collection of several services, can quickly grow to be a full-time job for several engineers, engaging in an arduous, manual, error-prone process fraught with peril and the potential for downtime.\n\n## The two Yelp repos\n\nYelp-main is the monolithic code base that represents the bulk of the existing (now legacy) Yelp app. It has existed since Yelp's inception and we are trying our best to separate functionality into services. This repo is not owned by any team. Teams own different parts of the repo. And we use **fork and pull model** [[engineering.git]]\n\nServices at Yelp are applications that provide a set of functionality via a well defined  interface. All code development happens in the same repository. Developers share development work via git branches. This is described as the **shared repository model**. The master branch contains the latest stable changes.\n\n**Move to Micro Services architecture**\n\nYelp has moved to Microservices architecture. Initially Yelp started with a monolith web application called “yelp-main”.\n\nSo when you visit yelp and start searching for any specific business or topic, you would hit their auto completion service as you type the search query. Then on searching, it would hit their backend service that would try to understand the query semantics and finds the suitable results matching the query. After the search results are displayed it is possible to select a specific business and order anything. This would result in hitting their payment service and also other services required for communicating to the partners.\n\nAs they broke the monolith “yelp-main”, the “yelp-main” become more of a frontend application, responsible for aggregating and rendering data from their growing number of backend services.\n\n\n\n# PaaSTA was born\n\nPaaSTA is Yelp’s defacto platform for running services and containerized batches. It is open-sourced [repo](https://github.com/Yelp/paasta) though it is probably close to impossible to set it up on your own. PaaSTA is Yelp's internal Distributed System Infrastructure to run all services.\n\n## Principles\n#TODO https://paasta.readthedocs.io/en/latest/about/paasta_principles.html\n\n\n## Tech blog\n#TODO\nhttps://engineeringblog.yelp.com/2015/11/introducing-paasta-an-open-platform-as-a-service.html\n\n\n\n## SPARK\nhttps://engineeringblog.yelp.com/2020/03/spark-on-paasta.html\n\n## Tron\nhttps://engineeringblog.yelp.com/2010/09/tron.html\n\nYelp internal scheduling \n[open-sourced repo](https://github.com/Yelp/Tron)\n\n\n# Scale from 0 to 139 million users\n\n[VP Engineering article](https://engineeringblog.yelp.com/2014/10/scaling-traffic-from-0-to-139-million-unique-visitors.html)\n\n\nYelp was born pre-AWS, so we had to operate our own data centers.","n":0.043}}},{"i":43,"$":{"0":{"v":"Transmetrics","n":1},"1":{"v":"\nOne of my project [presentations](https://docs.google.com/presentation/d/1FfyZGXqXL6Vi7mIQHXBMduG6y1PQjlqN-teB9RiO_OM/edit#slide=id.g13bbfb81574_0_0) with a client.\n\nNotes on [forecasting](https://drive.google.com/file/d/1Vlmfu4JjuJOTZguUFlzbW26uJiBmGkLg/view?usp=sharing)","n":0.316}}},{"i":44,"$":{"0":{"v":"SIG","n":1},"1":{"v":"\nI had to determine whether there is an opportunity for a profitable trading strategy for the HSI futures during the closing auction in HKSE. This project was divided in two main parts. First one collecting and clean data form Bloomberg Terminal – stock indication data from the closing auction. Organise it using VBA macros and then transform to pandas ­dataframes. The second stage of the project was the more interesting one which was coming with a trading ideas and backtesting models for trading during the closing auction.\n\nThe closing auction is the last event of every trading day and allows market participants to execute at the closing price. During the closing auction we cannot trade stock but we could trade the futures. The goal of the project was to use the information of stock indications during the auction to trade the futures. First I had determine the the fair value for the futures during the closing auction. This was done by calculating the cost of carry for the basket of stocks in the index, also known as the EFP or basis. The second step was to decide what actions to take based on this value and decide when to enter and when to exit trades on the futures market. People trade features for several reasons. They might want to hedge the underlying, in case they want to lock the underlying price from now and do not be susceptivble for its future movements. Another reason is to market make and provide liquidity. Although SIG is a market maker, our reason for trading the futures in this particular project was to speculate and find if there is opportunity for profitable trading strategy.\n\nThe approach I took when working on this project was to start with somethng very simple, think carefully about the assumptions I made and test them. In the end I tried 3 different trading strategies, each building on findings from the previous one.\nSo after computing the fair value of the futures, I had to decide what actions I should take. I could not trade at fair value because I would be just taking risk and market exposure for nothing. That is why I decided to test how many bips of edge I needed to enter a trade. I tested 3 different levels – 10bps, 20bps, 30bps.\n\nThe simplest stategy with which I started was just to enter trade in the beginning of the auction if I saw enough edge. If the price of the future was above the fair value with 10,20,30 bps I would sell it. If the price of the future was below fair with 10,20,30 bps I would buy it. As for closing the position I would always close position in the end of the auction as I did not want to be exposed during market non-working hours. This strategy was not very good and retained 1 bps out of the 10 bps that it required to enter a trade. However, while trying this strategy I noticed that it almost always entered trades early in the auction just after the reference period between the 1st and 6th minute.  The reason for that is because during this period there is higher volatility on the stock indications and the bps I required to enter a trade were tight. I realised that in the beginning of the auction there were some market participants who just wanted to trade at the closing price of the stock but not necessarily had any opinion on the futures. For example some big asset managers just wanted to participate in the auction to get the closing price. In order to take this into account I decided that my strategy should have a warm-up period of 10 seconds after the reference period, during which it cannot enter a trade. This way I wanted to avoid the big fluctuations observed in the beginning of the auction.\n\nSeeing that the first strategy which solely used the fair value of the futures did not work I started doubting my confidence in this fair value. That is why my second strategy was the volume strategy. I computed the historical volumes of stock indications during past auctions, and decided that I would trade the futures only once I see enough edge and the volumes were bigger than a certain threshold which was computing based on the historical volumes. This strategy did not improve the expected profits and still retained 1 bps out of 10 requested to enter a trade. However, the Sharpe ratio was improved slightly, meaning the volume strategy had lower variance while making the same amount of money.\nFinnally, as it seemed that I could not be confident in the fair value of the futures that I computed from the stock indications, I decided to change slightly the stock indications. Since there were some asset managers and ETF providers which just entered the auction to match the closing price, I decided to clip the data. Stock indications which were in the top 5% were removed because I thought they such stock indications were coming from people who did not have opinion on the futures market and I should not consider them as an input for finding signal. On historical values clipped data strategy retained 5 bps out of 10 bps. So it seemed that there could be some edge there to be made. \nAnother observations I found was that trading later in the auction would be with less risky as the coputed EFP would converge to the fair value of the futures, but in order to enter trades I had to lower the amount of bps we required to enter a trade. Essentially saying that the later you enter a trade the safer it is, but the smaller expected profit you can make.\n\n# How does the closing auction work?\n\nClosing auction lasts between 8 and 10 minutes and has 4 periods. The first one is the reference period which takes place during the first minute of the auction. It determines the reference price, (+-5%) around which auction orders can be fulfilled.   This period is used to avoid people trying to manipulate the auction.\nThe second period is order input period during which all market participants submit their orders.\nThe third period is No-cancellation period between the 6th and 8th minute during which all submited orders cannot be amended or cancelled.\nFinally the last two minutes are for the random closing period when the auction finishes.\nHSI ~ 25000, spreads 1 or 2. \nBuy futures = aggree to buy the underlying from now at a prespecified price. Loose on dividends, as do not own the stock. Win on interest as no need to pay now.  \nmacro analysis: find who are the big players in the auctions: like asset managers, big banks etc. I realised that these big blayer usually submit their orders as early as possible in the auction just to match the closing printing price\nOther players on the market the market makers , they tend to submit later in the auction to get more information .\nOur goal was to use the infromation during the auction to trade the futures because we could not trade stock during the auction but could trade the futures. \ncomputed daily efp- basis  diff between futures and index.\nforward looking bias\nwho are the other players? Do they have opinion on what you trade?","n":0.029}}},{"i":45,"$":{"0":{"v":"CapitalOne","n":1},"1":{"v":"\n# Merging Feature\n1.1 Summary points on merging feature:\n- Given cashflows for diifferent products - aggregate them to get a summary report\n    e.g NPV -> weighted mean on accounts\n    e.g Discount rate -> harmonic mean\n    - These reports are used in the decision making to decide which programs are\n        worth  pursuing and which not \n    -  Monitoring the risk the bank is taking when giving credits\n- It follows ETL process: extract data from parquet files; transform data by\ncombination functions\n- Load data in OneFlow workflow to produce automatic excel report\n\n1.2  We heavylift the database (parquet files), the dispatching manually in python. Not like Django Flask frameworks where everything is checked this is getting the correct request, is it from the write url etc. \n- Created additive dispatching for computing 200+ fields each using different type of  combining function( yaml file combination  functions mapping field -> function) (name mapping algo tailor to Principles of Computer System Design Book) From book notes: Frequent name-mapping algorithms:\n- Table lookup\n- Recursive lookup\n- Multiple lookup\n\n1.3 Main benefit is that they are additive no need for refactoring endless if else statements. When add in another Field or Dunction\nRead more about that in SICP Chapter 2 Python version page 52, Type dispatching. In your case the dispatching is the merging function for the different CFS and CRM fieldds\n1.4 Challenges: How to handle so many fields=business metrics -> scaled using static \ndispatch mapping and message passing.\n- Unit tests did not prove enough covarege for the combination functions\n- Integration tests with realistic data showed a lot of mismatches and errors in our combining functions. Though unit tests tried to capture edge cases etc, realistic data turned out to be much messier than we expected.\n- Thus for T-C feature we will create the feature and do integration tests in parallel.\n\n# T-C feature\n2.1 Given  test and control data sets return reports of the differences between the two\n- Used to test hypothesis , e.g. control data are logtime users test are new clients, see how \nthey behave\n\n2.2 Created  Directory for Dispatiching (imrovement on the previous feature) yaml to paralize   \n\nwhen new dispatches and messegaes were created across different people\nThis direcotry of dispatching grouped similar fields = summary statistics/business metrics!\n2.3 Introduced test driven developement, before relied only on unit tests, now create integration tests alongside the feature. Reasoning for using it is because OneFlow is a data tool and heavilyt relies on the data. If it is messy, tests fail more easily. Alot of edge cases to mke sure OneFlow does the right computation\n\nBenefits: \nreduced debugging effort – when test failures are detected, having smaller units aids in tracking down errors.\n\n2.4 applied Incremental developement:\n- Unit testing : when you test a module in isolation, you can be confident that any bug you find is in that unit – or\nmaybe in the test cases themselves.\n-Regression testing : when you’re adding a new feature to a big system, run the regression test suite as often as\npossible. If a test fails, the bug is probably in the code you just changed.\n\n# Performance improvements\n\n- joblib parralel for program level runs\n- numba improvements to targeted calculations (math decorator)\n\n\n# Convocation\n\nOrganise Data all hands event for data uk. Fun fair theme with stalls. Increase team effectiveness. Outcome teams. Program increments.\n\n\n# Other\n\n- Data expo [presentation](https://drive.google.com/drive/folders/1VT4c2V9zxzlYeKot3dV6I4j-HhLwcE7f) of OneFlow\n- PUG talk, python users group ML [presention](https://drive.google.com/drive/folders/1VT4c2V9zxzlYeKot3dV6I4j-HhLwcE7f)\n\n Approaches when developing a feature:\n1. develop feature + unit tests\n    then create intergation test\n2. develop feature +unit test + create integration test ( Test-driven development \n    parallelize inegration test while developing a feature\n    Not realying only on unit tests only\n3. Github flow vs GitFlow we are moving from one to the other\n4. Docusourus tool for static web pages\n5. .env file lets you customize the individual working environment variables.\n    Using environment variables  common practice during Development not a healthy \n    practice to use with Production.\n    - in .enf file: add secrets, passwords, FEATURE FLAGS\n    E.g if sharing codebase with the users we can have our own local .env where the     \n    feature flag is turned on. Hoever in production(the users) do not have this feature\n    turned on.\n- PM stuff:\nFollow Agile principles by using tools such as Jira and Confluence Enforcing the scrum rituals (sprint planning/retrospective, daily huddles, and customer demos). \n\n- Building system: infrastructure, storage, network protocols\n\n---\n\n**Behave** is a python library that enforces behaviour driven developement BDD which is an extension of TDD.\n\nAs we all know in TDD we write tests first and then   develop  individual functions in the code\n\nBDD is the same but it acts on the level of features rather than functions.\nSo in BDD users and developers writetest cases in English so  that non-programmers can understand.\nAnd only then developers start writing code\n\nInvolves users/BA-s and non-technical people can participate in the implementation process\n\nWrites tests using normal language - allows users to write tests\n\nFEATURES files are written by users\n\nAdvantages\nAllow users to be in the developement process + instead of writing feature spec and requirements, they write test directly. \n\nDisadvantage is that it might take more BA-s time and given that tehy are already quite busy it might not be optimal\n\nIf we use behave it will probably worth it only for some integration tests\nand do not use it for all unit tests which\n\n---\n\nRecoveries liquidation array = money we recover when our clients go in default\n\nI compute RLA for each account using only matrix operations avoiding loops.  After making 100 runs each with 3k account the speed up is 46\\%. $O(N^3)$ problem I did using one loop and for each iteration of the loop I use matrix operations.\n\n```python\nliquid_array_per_id =\n            np.outer(co_pop1, padded_pop1_non_debt_sale)\n            + np.outer(co_pop2, padded_pop2_non_debt_sale)\n            + np.outer(co_pop3, padded_pop3_non_debt_sale)\n            + window_multiply(debt_sale_prop * np.array(co_pop1),sale_price)\n```\n#### Time complexity\n\nAssume we have `N` accounts, `S` statements and `CO` statements since charge off. The output contains for each account (`N` in total) and each statement (`S` in total) a liquidation array of size (`CO`). Hence the output is with dimension `N x S x CO` and just writing the output would require `O(N x S x CO)` runtime (i.e. that is the minimum runtime required).\n\nOur RLA computation as we saw above calls two main functions which have non-constant run time - `np.outer(co_pop1, padded_pop1_non_debt_sale)` and `window_multiply(debt_sale_prop * np.array(co_pop1), sale_price)`\n\n`co_pop1` is with dimension `S` and `padded_pop1_non_debt_sale` is with dimension `CO`,  hence  `np.outer(co_pop1, padded_pop1_non_debt_sale)` costs `O(S x CO)`.\n\n`debt_sale_prop` is with dimension `CO` and hence `window_multiply(debt_sale_prop * np.array(co_pop1), sale_price)` costs `O(S x CO)`\n\nSince we call these two functions **for each account**,  the total runtime is `O(N x S x CO)` which is the minimum achievable.\nThus, in terms of orders of growth, this is the fastest possible computation. Note the old RLA version was also `O(N x S x CO)`  but had a very large constant factor because of using multiple nested for loops.\n\n\n# Key words\n\nnumba, jit decorator, dask, docosaurous, Agile, behave driven developement, cash flows, valuations, ","n":0.03}}},{"i":46,"$":{"0":{"v":"Tutorial","n":1},"1":{"v":"\nWelcome to Dendron! Dendron is a developer-focused knowledge base that helps you manage information using **flexible hierarchies**!\n\nYou are currently in the tutorial vault (a vault is the folder where your notes are stored). Feel free to edit this note and create new files as you go through the quickstart!\n\n## Create a Note\n\n1. Use `Ctrl+L` / `Cmd+L` to bring up the lookup prompt\n1. Type `dendron` and select `Create New`\n\n- > NOTE: After you press enter, Dendron will create and open the `dendron` note. Use `<CTRL>-<TAB>` to come back to this note\n\nYou just created your first note!\n\n- > NOTE: Notes in Dendron are just plain text markdown with some [frontmatter](https://wiki.dendron.so/notes/ffec2853-c0e0-4165-a368-339db12c8e4b) on the top. You can edit them in Dendron or using ~~vim~~ your favourite text editor.\n\n## Find a Note\n\n1. Use `Ctrl+L` / `Cmd+L` to bring up the lookup prompt again\n1. Type `dendron` and press `<ENTER>`\n\n- > TIP: you don't have to type out the entire query, press `<TAB>` to autocomplete\n\nYou just `looked up` a note!\n\n- > NOTE: in Dendron, you can find or create notes using the lookup prompt\n\n## Organize your Notes\n\n1. Bring up the lookup prompt again\n1. Type `tutorial.one`\n\nYou just created your first hierarchy!\n\n- > NOTE: hierarchies in Dendron are just `.` delimited files. This makes each note both a file and a folder and makes it easy to keep your notes organized\n\n- > TIP: You can use the [Dendron Tree View](https://wiki.dendron.so/notes/hur7r6gr3kqa56s2vme986j) to view your hierarchy. If it's not currently in focus, you can use `CTRL+SHIFT+P`/`CMD+SHIFT+P` to open the command prompt and type in `Dendron: focus on tree view` to make it appear\n\n## Create a link\n\n1. In the current note, type `[[` - this should trigger the autocomplete. You can type `one` to narrow it down to the note you just created and hit enter\n<!-- Enter '[[' below-->\n\n<!-- End space-->\n\nYou just created your first link!\n\n- > NOTE: the links with the `[[` are called wikilinks (because they were first popularized by Wikipedia)\n- > TIP: If you hover your mouse over the link, you can get a preview of the contents inside the note!\n\n## Navigate a link\n\n1. Move your text cursor over the link you just created. Hold down `<CTRL>+<ENTER>`/`<CMD>+<ENTER>`\n\n- > TIP: You can also use `CTRL+CLICK` or `CMD+CLICK` to navigate links via mouse\n\nYou just navigated the link!\n\n## Refactor a Note\n\n1. Open [[tutorial.one]], bring up the command prompt (`CTRL+SHIFT+P`/`CMD+SHIFT+P`) and type `Dendron: Rename Note`\n1. Replace `tutorial` with `my-note` and then press `<ENTER>`\n1. You just refactored the note!\n\n- > NOTE: when you rename a note, Dendron updates all links and references of the original note being renamed. Try switching back to [[tutorial]] to see the updated link!\n- > TIP: in addition to renaming one note at a time, dendron has [an entire collection](https://wiki.dendron.so/notes/srajljj10V2dl19nCSFiC) of refactoring commands that let you change headers, move around sections, and refactor entire hierarchies!\n\n## Conclusion\n\nCongrats, you finished the Dendron tutorial!\n\nWas there anything **unclear or buggy** about this tutorial? Please [**report it**](https://github.com/dendronhq/dendron/discussions/3266) so we can **make it better**!\n\n## Next Steps\n\nDepending on your needs, here are some common next steps:\n\n- I want to **start writing**: [Create a daily journal note](command:dendron.createDailyJournalNote) ([docs](https://wiki.dendron.so/notes/ogIUqY5VDCJP28G3cAJhd))\n\n- I want to **use templates**: Use the [Appy Template](https://wiki.dendron.so/notes/ftohqknticu6bw4cfmzskq6) command to apply [templates](https://wiki.dendron.so/notes/861cbdf8-102e-4633-9933-1f3d74df53d2) to existing notes\n\n- I want to do a **longer tutorial**: Check out our [5min tutorial to explore more of Dendron's functionality](https://wiki.dendron.so/notes/678c77d9-ef2c-4537-97b5-64556d6337f1/)\n\n- I want to **implement a particular workflow** (bullet journal, zettelkasten, etc): Check out community [workflow guides](https://wiki.dendron.so/notes/9313b845-d9bf-42c9-aad1-0da34794ce26)\n\n- I want to use Dendron for **tasks and todos**: See the [Getting Things Done (GTD), Bullet Journaling, and Other Task Management Workflows](https://wiki.dendron.so/notes/ordz7r99w1v099v14hrwgnp) for how the founder of Dendron uses it to manage his work.\n\n- I want to explore **advanced features**: See [next steps](https://wiki.dendron.so/notes/TflY5kn29HOLpp1pWT9tP) for longer walkthroughs and advanced functionality!\n\n- I want to start clean with a **new vault at a custom location**: Run [Dendron: Initialize Workspace](command:dendron.initWS) from the command prompt (or click this link) to start from a clean slate\n\n- I want to use Dendron as a **knowledge base for my team**: Read the [Dendron team setup](https://wiki.dendron.so/notes/98f6d928-3f61-49fb-9c9e-70c27d25f838) to get started\n\n> Coming from Obsidian? Click [here](command:dendron.importObsidianPod) to import your Obsidian notes (or any markdown notes) into Dendron to see how they look.\n\n## Community\n\nDendron is more that just a tool - we are also a community of individuals that are passionate about knowledge management. If you need help or want to connect with the community, join us in the [Discords](https://link.dendron.so/discord).\n\nYou can also:\n\n- Star us on [GitHub](https://github.com/dendronhq/dendron)\n- Follow us on [Twitter](https://twitter.com/dendronhq)\n- Subscribe to the [Dendron Newsletter](https://link.dendron.so/newsletter)\n","n":0.037}}},{"i":47,"$":{"0":{"v":"Tags","n":1}}},{"i":48,"$":{"0":{"v":"Zadachi","n":1}}},{"i":49,"$":{"0":{"v":"TODO","n":1}}},{"i":50,"$":{"0":{"v":"QED","n":1}}},{"i":51,"$":{"0":{"v":"AI","n":1}}},{"i":52,"$":{"0":{"v":"Random","n":1}}},{"i":53,"$":{"0":{"v":"Second Price Auction","n":0.577},"1":{"v":"\nDid you know that Yelp (and most other tech companies) use second price auction for the ad bidding system? I certainly didn't :sweat_smile:. In this framework the winning advertiser pays the amount of the second-highest bid rather than their own bid. This incentivizes advertisers to bid their true value without fear of overpaying. The point is to charge the advertiser only as much as he would need to pay to win the auction - maximizing her return on investment. It is also mathematically proven that this strategy is 'optimal' reaching Nash Equilibrium.\nIn the standard first price auction where highest bidder pays the price that was submitted, bidders are incentivized to bid less than their true valuation of the ad slot - only then their payoff is positive if they win the slot or 0 if not. More interestingly that for the auctioneer (i.e Yelp) the expected revenue is the same when running either type of auctions\n\n- https://en.wikipedia.org/wiki/Vickrey_auction#Revenue_equivalence_of_the_Vickrey_auction_and_sealed_first_price_auction\n- [Generalized Second Price Bid-Sealed](https://en.wikipedia.org/wiki/Generalized_second-price_auction)","n":0.079}}},{"i":54,"$":{"0":{"v":"Latency vs Throughput","n":0.577},"1":{"v":"\n# Latency vs Throughput\n\nThroughput is measured in transactions per second (TPS) or requests per second RPS . It is the amount of information processed, transmitted or stored in a given amount of time.\n\nLatency is measured in milliseconds . It is time it takes a single piece of data to travel from source to destination.\n\nHigh throughput means the system can handle multiple data at the same time. Low latency means that system can handle single data quickly.\n\n\n**High Throughput, Acceptable Latency**: Batch processing systems (e.g., data ingestion systems) where large volumes of data are processed together, and immediate response is not critical.  \n\n**Low Latency, Acceptable Throughput**: Real-time applications (e.g., video conferencing) where rapid responses are more important than processing high volumes of data consistently.","n":0.091}}},{"i":55,"$":{"0":{"v":"Horizontal vs Vertical Scaling","n":0.5},"1":{"v":"Horizontal scaling is when you add more machines and distribute the work across them - parallelization\n\nVertical scaling is when you improve performance of the current machines - better CPUs, GPUs with more RAM, lower latency so on.\n\nPractically if a system is horizontally scalable it is better as it is easier to do - you just buy new machines. Improving current ones requires research/optimization and so on.\n\n# Horizontal scaling\n- more complex management\n- no required downtime, no need to stop current machines when you add more\n- higher fault tolerance\n\n# Vertical scaling\n- easier management\n- required downtime\n- single point of failure\n- limited by hardware capacity\n","n":0.1}}},{"i":56,"$":{"0":{"v":"First-Price Sealed-Bid Auction","n":0.577},"1":{"v":"First-price bid-sealed auction (FPBSA) is the standard auction that most people are familiar with. Every participant submits a bid that others are not aware of. All bids are revealed and highest bidder wins and pays the amount they've submitted.\n\nIn FPBSA, each bidder is characterized by their monetary valuation of the intem for sale.\n\nThe optimal strategy for all bidders is to bid slightly less then their valuation. This type of auction we describe as **non-truthfull** as no bidder reveals their true valuation.\n\nSay Alice valuation for the item on the auction is $a$\n- If she bids more than a, then her payoff is either 0 if she does not win the auction, or negative if she wins\n- If she bids exactly a, then her payoff is always 0\n- If she bids $x < a$, then her payoff is either 0 or $a-x$\n\n$x$ depends on other peoples bids. It should be $\\eps$ (say the minimum incremental value in the auction) more than the maximum of all opponents bids and less than $a$.","n":0.077}}},{"i":57,"$":{"0":{"v":"Dutch Auction","n":0.707},"1":{"v":"Seller start with high asking price and lowers until any buyer agrees. Strategically similar to first-price bid-sealed auction.","n":0.236}}},{"i":58,"$":{"0":{"v":"Bayesian game","n":0.707},"1":{"v":"In game theory, Bayesian game is a strategic multiplayer game in which players have incomplete information.\n\nAuctions with hidden bids are examples of this. In them agents do not know the bid (respectively payoffs) of other agents.","n":0.167}}},{"i":59,"$":{"0":{"v":"Politics and Economics","n":0.577},"1":{"v":"\n\n\n**Всяко преразпределение на богатство от страна на държава е начин да се вземат пари от работещи хора и да се дадат на неработещи/по-малко работещи хора.\n**\n# Inflation\nWhen your money loose its value. When money printing exceeds the speed of actual production.\n\nSmall inflation is healthy. Low inflation (deflations) would make people keep their cash. If you could by something\ncheaper in the future then you would wait. Too high inflation leads to products becoming too expensive for people.\n\n\nCountries always want small inflation in their currency. Inflation -> more export than import = good. Export = stuff you sell,\nImport = stuff you buy. You want more sell than buy. If you currency is very expensive with no inflation, you will only import.\n\nInflation is a way government to take money from its citizens without them realizing it. A way to put money in the people who hold assets. Inflation comes from printing money and whoever gets this money becomes richer (usually government people).\n\nIn inflation everything becomes more expensive. Often people do not save cash, but rather buy mortgage. Money looses its value.\n\nIn period of deflation, or when money has lots of value, then go and invest and buy stuff.\n\n# Recession\nEconomic downturn. When businesses produce less, people buy less and money is staying in one place.\n\n- recession = drop in economical activity and results in reduced GDP.\n- inflation = increase in price of products, money looses their power. Measured in CPI.\n\nOften low or too high inflation leads to recession.\n\nThe year is 2022. 2 years after COVID. We enter recession??? Weird how during covid we thought it was a crisis and we\nwere in recession. However the true recession comes now, with super high inflation (15% in UK/EU), and it comes after all\ngovernments printed a bunch of money to \"help\" people. Macro economics is super weird and I am not sure how to build\nintuition for that.\n\n# Events\n\n- Jan 2022, Meta stock price crashes 50%.\n- US printed 2 trillion + dollars the most of all governments and the USD became more expensive. It became on par with the\neuro\n- When covid started stock market crashed but there was no big recession, compared to the one now in 2022.\n- now in 2022 stock market crashed even worse and there is big recession coming + inflation\n- in 2016/2017/2018 housing market was way cheaper than now and I was afraid to get into it (with parents money). Looking\nbackwards that was a mistake. I don't know how but during this whole period of recession/pandemic/inflation housing market\nis still increasing\n- 24 Feb 2022 Russia started War in Ukraine. Russia attacks in Europe, europe is weak, europe depends a lot on gas and petrol\non Russia. Maybe this was indication you should buy dollars... 30% profit.\n- July 2022 Boris Johnson resigned. Liz Truss vs Sonak (communist dont know how got in Tori - said put max prices in\nsupermarkets, ask the top 10% companies to increase their corporate taxes and things that is fair.)\n\n\n# Events in Bulgarian politics\n- 2nd Aug new slujebno pravitelstvo given by president Radev - a bit pro russia and wants to get deal with Gazprom\n- 7tth Aug DB wants to do pre election coalition with PP. PP said they will look in statistics and if they get more seats\nthey will do the coalition\n-18 Aug PP and DB would not go to elections together\n- 18 Aug Slavi Trifonov did not declare his net worth in KPKOPNI\n- 2nd Oct election time\n# Printing money\nIn general, when governments print large amount of money it would be bad for you. This is a smooth way to steal from you.\nIndeed, the act of printing money benefits only those who would get large portions of the money. You are unlikely to be part\nof this portion, like most people. Therefore, printing money just makes the money you have less valuable and you should\nlook for other places to store value.\n\n\n# Socialism vs Capitalism\nMarket demand should drive what type of market is better. Healthcare should be social, government should participate in that\nmarket because demand for health is distributed uniformly. Probability of rich and poor people get cancer is relatively\nthe same. For sports cars market should be capitalism. P(rich person buys lambo) >> P(poor person buys lambo).\n\n\nMarkets where there should be government intervention:\n- healthcare\n- education?\n- war weapons\n\n# What should governments do?\n\n- protect borders\n- enforce laws equally\n- create laws\n- make business environment better\n- take care of poorest people? Maybe with small amount of taxes.\n- justice system\n- infrastructure? Not sure about this one. Building towns and roads can be done by private sector I think.\n\nShould the government play any social part in people's life? The more responsibilities a government has, the less freedom\nindividual people have. Therefore I think government should not do social politics at all. Social politics is a fancy term\nfor taking money from the ones who have money to the ones who do not have money. That is the core of social politics. It\ntakes money from those who work and gives it to those who don't.\n\n# When a company should go public?\n\n# Euro funds and government subsidiaries\n\nGovernments should not support businesses. During crisis, during economic upturn, whatever. Businesses should exist and make\nmoney only because their clients say so and buy their products  Euro fund, 'obshtestveni prouchki' is just a way for\nthe government to give money to their friends and family.\n\n\n# Investing\nIn your end lifetime, when you compute total networth accumulated through all years, it is likely most of the money you make\nwould be from investments rather than from your work. Always have this in mind.\n\nLook for opportunities when people are afraid to invest, do not invest when everybody is doing that. Now the stock markets\nhave crashed by quite a bit (July 2022), I think I should put some money on this.\n\nHousing market is crazy, peak after peak. It feels it slows down a bit and we might reach a downturn. For my personal\nfinances I would have enough money to get in the market after 2-3 years. Let's see if there is a downturn...\n\n\n\n# European green deal\n\nMore bureucracy?\n\n\n# Thoughts on diversity and inclusion\nPeople from the PRIDE are super innocent. I support most of their positions. Rights to get married and in general they just\nwant to be accepted by the community. If they get these thing our society would only be net happier. This does not make other\npeople unhappy. Adoption is another topic though. The life of the adopted kids could get worse. Straight couples should have\npriority when adopting in societies where gay people are still not accepted.\n\n\n\n# Should we support in-country made stuff?\n\n\n\n# Reform the justice system thoughts\n\n- from state to private prosecutors will lead to less corrupted prosecutors\n- judges have to be chosen by the state? The state enforces law after all. How to fix corrupted judges? Transparency. Live stream all judge decisions.\n- we need to give incentives for judges (chosen by the state) to give fair verdicts if high level politicians are being executed\n\n# Reform the pension scheme\n\n- receiving a pension should be **volunatary** act rather than mandatory one\n- if you want pension you pay national insurance taxes, if you don't you don't pay these\n- the pension system is broken because it has unlimited supply of income coming from regular insurance taxes. It does not need to be optimized and managed well because it cannot go bankrupt. The fear of failing and going bankrupt is what drives businesses and organizations to move forward. Death is inevitable in every meaningful life. We need to let the broken pension system to either die and build a new one or let it recover. But we need to put in on the free market with voluntary taxes. Then it will receive money only if it can assure the society that it will pay good pensions/provide good service for these taxes.\n\n\n# BBR (bg bank za razvitie)\nShould that be privatized?\n\n\n# PP vs DB\nPP are a bit left populists (e.g 'Use right instruments' for left ideas). However, they have strong leadership. DB match\nmy political views the best among all parties but have weak leadership. Hristo Ivanov is a good politician but his best role\nin a party is to be an advisor rather than leader. He is a bit 'filosof'. I am happy he recently proposed wide coalition\nbetween them and PP and all the right centrist 'city' parties/community.\n\nPP have shown several times that are good negotiators and able to group different peoples in one place. Is that true though?\nIn the coalition of 4 parties BSP, PP, ITN, DB, BSP has been able to push for many of their leftist ideas. PP put a cap\nprice on electricity in Jan 2022. THat's super weird move and I doubt they teach this stuff in Harvard Business School.\n\nDB has been quite consistent with their promises to the people (except being in coalition with BSP). They have pushed for\njustice reform, e-government, low taxes and fighting against corruption. I am pleased with what they've done. I hope\nPP would agree to have pre-election coalition with them.\n\n# Are BSP less corrupted as of 2022?\n\nIt is very controversial to see PP, ITN and DB to agree to be in coalition with BSP. All these 3 parties have talked against\nBSP to some extend or another. Does that mean BSP are less corrupted now or political parties just need their votes and\nbecome corrupted as well? I think it is pretty hard to determine if a party is corrupted anyways. PP, ITN and DB do not\nmost probably do not know if BSP is corrupted. They are for sure much less corrupted than 15 years ago when they were in\npower. What, I admire about BSP is that they have very consistent political views and have clearly identified them in the\npolitical and economical spectrum. I do not agree with their views but respect that they stay consistent. I am definitely not\nhappy that they have seats in the government but what can we do?\n\n# Government 2nd Aug 2022.\n\nNew government by president Radev (3rd in the last year and a half). Tis one is a bit communist, pro-Russia and do not\nhave all stars like Kiril and Asen. Seems not competent too. Radev made some really good moves with bringing Asen and Kiril\nand riding the protests wave in summer 2021. Now he seems to have deviated from this direction and is more pro-Russia.\nLike ITN it looks like everybody is fighting for themselves.\n\n\n# Rank Choice Voting System\n\nThis voting system is used when we need a clear winner (president or mayor elections).\nFirst Past the Post is not a a fair system. Even with balotage system, the 2nd round is not fair (although probably better than FPTP), e.g. elections for mayor in Sofia with Fandukova, Manolova, Boris Bonev and Da Bulgaria. The balotage had Fandukova vs Manolova, but with ranked choice we could have Bonev(or Da Bulgaria) vs Fandukova.\n\nEach person votes by ranking the parties 1,2,3 etc.\n\nIf there is a mojaorty winner they win. If not the lowest candidate is removed and the votes of the people who voted for him are distributed to the other candidates (whoever was their second choice gets all the votes). This is repeated until there is a majority winner.\n\n\n# Questionier before voting\n\nEvery time you vote you need to fill in the answers of 3 questions that parties have answered. This is a way so that people know what they have voted for.\n\n\n# Industrial Policy\n\nIdea: \"Attempts to create competitive advantage through government direction and support\"\n\nExample of success: \"Intentional industrial policies likely played a critical role in creating the modern South Korean economy.\" In the 20 years after 1973 South Korean real gdp per head rose by 349%.\n\n“It is possible” that for the initial stages of industrialisation “government efforts had a significant role”\n\nAnother example:  Under Xi Jinping and his “Made in China” project, the Chinese state has played an even more activist role in directing economic activity. The country now has a global monopoly in many industries. Subsidies have allowed firms to slash prices, putting foreign competitors out of business.\n\n\n**Problems:**\n\n- subsidies allow the survival of inefficient firms, making the industries they are in less competitive. Firms may focus on securing subsidies, which removes resources from productive use.\n- Total factor productivity (the amount of output per unit of labour and capital) across China has probably fallen under Mr Xi.\n- helping one firm with subsidies tends to harm others. \n- misdirected transfer of public resources to large domestic and foreign firms\n\nExample problem:\n- India’s “production-linked incentives” (pli) scheme, which pays manufacturers a sum for every unit produced (say mobile phones).  Mobile exports soared. However imports soared too. Mayme producers were simply re-exporting phones via India to get another subsidy.\n\n\n\n# Homeland economics\n\nHomeland economics is a response to four big shocks:\n- the 2008 financial crisis (finance)\n- Covid (pandemic, finance)\n- US China trade war (geopolitical)\n- Russia's invasion of Ukraine (geopolitical, energy)\n\n**leads to high subsidieries**\n\n\"Industrial policy and protectionism could endanger trade, without making Western economies safer\"\n\n# Federal republics\n\n- more equally distributed power and resources\n- more stable across the country\n- cities are more equally developed\n- Germany Switzerland, USA, Canada, Australia\n\n\n# Airlines Hub\nHub and spoke model is a system of routing airline flights through a central hub airport. This allows airlines to consolidate their operations and offer more connections to passengers, while also reducing costs and increasing efficiency. The hub airport serves as a central point for flights to and from various destinations, allowing airlines to maximize their route networks and provide better service to customers.\n\nbig airlines fly only to their hubs","n":0.021}}},{"i":60,"$":{"0":{"v":"Economics","n":1}}},{"i":61,"$":{"0":{"v":"Random Concepts","n":0.707},"1":{"v":"Collection of economic principles, concepts and useful decision-making tools for everyday life.\n\n# Opportunity cost\n\nThat is the cost of choosing one opportunity over another when you have to choose exactly one. Opporunity cost = Net return option A - Net return option B.\n\nComputing opportunity cost is about balancing between **scarcity and choice**. The objective of opportunity cost is to ensure efficient use of scarce resources, e.g. time.\n\n# Sunk cost\n\nWiki: Sunk cost (also known as **retrospective cost**) is a cost that has already been incurred and **cannot be recovered**.\n\nThe opposite of **prospective costs**, which are future costs that may be avoided if action is taken.\n\n**In other words, a sunk cost is a sum paid in the past that is no longer relevant to decisions about the future.**\n\nIdioms:\n- water under the bridge\n- crying over spilt milk\n\nOnly **current alternatives** matter. Example is in poker when you compute pot-odds. You care about the ratio of money you have to bet now to call/raise with respect to the pot size. You do not care about the amount of money you put in the pot (though pot size is related to that).\n\n\nClosely related to **DESIRE NOT TO APPEAR WASTEFUL**.\n\n## Marginal principle (bygones principle)\nIf a new factory was originally projected to yield $100 million in value, and after $30 million is spent on it the value projection falls to $65 million, the company should abandon the project rather than spending an additional $70 million to complete it. Conversely, as a rational actor, if the value projection falls to $75 million the company should continue the project.\n\nObviously, sunk costs are bad and should be minimized.\n\n## Identifying sunk cost\n\nAsk yourself: \"Can you recover partially or the full amount of the cost you paid for?\"\n\n- Stock price drop is not a sunk cost (unless you close position)\n- Poker call pot is a sunk cost\n- Buying a car is not a sunk cost (not fully, can resell)\n- Repairing a cor is a sunk cost (cannot unrepair and get money back)\n\n\n# Endownment effect\n\n\n# Survivorship bias\n\nCross the road like us, we are all fine...\n\n\n# Milton Friedman\n\nИмa чeтиpи нaчинa, пo ĸoитo мoжeтe дa xapчитe пapитe cи. Moжeтe дa xapчитe coбcтвeнитe cи пapи зa ceбe cи. Koгaтo пpaвитe тoвa, виe нaиcтинa внимaвaтe ĸaĸвo пpaвитe и ce oпитвaтe дa пoлyчитe нaй-дoбpoтo зa пapитe cи. Cлeд тoвa мoжeтe дa пoxapчитe coбcтвeнитe cи пapи зa няĸoй дpyг. Haпpимep, ĸyпyвaм пoдapъĸ зa няĸoгo. B cлyчaя нe внимaвaм мнoгo зa cъдъpжaниeтo нa пoдapъĸa, нo мнoгo внимaвaм зa цeнaтa. Cлeд тoвa мoгa дa xapчa пapитe нa няĸoй дpyг зa ceбe cи. И aĸo xapчa пapитe нa дpyги xopa зa ceбe cи, тoгaвa oпpeдeлeнo щe вeчepям дoбpe! И нaĸpaя, мoгa дa xapчa пapитe нa няĸoй дpyг зa няĸoй дpyг. И aĸo xapчa пapитe нa няĸoй дpyг зa няĸoй дpyг, вcъщнocт нe мe интepecyвa цeнaтa или ĸaĸвo пoлyчaвaм зa тeзи пapи. Eтo ĸaĸ пpaвитeлcтвoтo xapчи пapи. A тoвa e oĸoлo 40% oт нaциoнaлния ни дoxoд.\n\n# Others\n\nБизнеса е обикновено някакво гонене на мечта. Нужно е общо подклаждане на тази мечта.\n\nСоциалната държава просперира, защото хората са по-привлечени от безгрижието, отколкото свободата - свободата идва с отговорност.\n\n\n\n","n":0.044}}},{"i":62,"$":{"0":{"v":"Japan lost decade","n":0.577}}},{"i":63,"$":{"0":{"v":"Japan's lost decade","n":0.577},"1":{"v":"\n\n# The Economist report in 2022\nhttps://www.economist.com/special-report/2002/09/28/japans-lost-decade\nLook at all chapters","n":0.333}}},{"i":64,"$":{"0":{"v":"WW2","n":1}}},{"i":65,"$":{"0":{"v":"WW1","n":1}}},{"i":66,"$":{"0":{"v":"US politics","n":0.707},"1":{"v":"\n# Taco\n- Trump always chickens out\n- 2025 Jul 12 Trump extends tariffs negotiates with a month as well\n- 3 months ago he would impose tariffs on all countries with trade deficit\n- he first announced with a big table all the tariffs, markets went down S&P down 10-15% (almost went 2 years back) - should have bought the dip\n- then he said he would negotiate with China and EU, UK and all, and the tariffs would be imposed in 3 months\n- now he treated again to impose tariffs but markets are not reacting","n":0.104}}},{"i":67,"$":{"0":{"v":"UK Political System","n":0.577},"1":{"v":"\n\n# The two-House system\n\n## The House of Commons \n\nThe Commons is publicly elected. The party with the largest number of members in the Commons forms the government.\n\nMPs (members of parliament) are elected in general elections and by-elections.\nThe UK elects 650 MPs.\n\n\nThe Chancellor of the Exchequer is the government's chief financial minister and as such is responsible for raising revenue through taxation or borrowing and for controlling public spending.\n\n\n## The House of Lords. \n\n\n## Elections\n\n**General elections**\nA general election is an opportunity for people in every part of the UK to choose their MP - the person who will represent their local area (constituency) in the House of Commons for up to five years.\n\nUK elections: First-past-the post voting system on local level.\n\nUK has 650 constinuencies, each represented by one MP. London has 73 constituencies.\n\n\n**First-past-the-post voting system**: The candidate with the most votes in a constituency wins and becomes the MP for that seat. Usually, the winning candidate would have less than half of the votes (problem of this voting system).\n\nSmaller parties don't stand a chance. Often you can get 15% of the total vote but get only 1 MP.\n\nBG elections: Proportional representation on national level.\n\n## Main parties\n\n- Conservative Party (Tories)\n- Labour Party \n\n","n":0.071}}},{"i":68,"$":{"0":{"v":"Israel Palestine conflict","n":0.577},"1":{"v":"\n\n# State of Palestine\nOfficially governed by the Palestine Liberation Organization (PLO), it claims the West Bank (including East Jerusalem) and the Gaza Strip as its territory, though the entirety of that territory has been under Israeli occupation since the 1967 Six-Day War.\n\nThe Gaza Strip has been ruled by the militant Islamic group Hamas and has been subject to a long-term blockade by Egypt and Israel since 2007.\n\n# Hamas\n\n# Arab\n\n# Geography\n\nPalestine:\n- Gaza\n- West Bank (east of Juresilm)\n\nLebanon (Ливан) is on the north.\n\nSyria is on the east.\n\nEgypt is on the south.\n\nJordan is on the east\n\n# WW1: The question of Palestine\n\nFrom 16th century until the end of WW1, this part of the Middle East was controlled by the Ottoman Empire.\n\nIt was granted to the British after WW1.\n\nBoth Israelis and Palestinians were struggling for self-determination and sovereignty over the territory, developing respective movements for their causes.\n\nLarge-scale Jewish immigration followed in succeeding decades, including during Nazi persecution and Holocaust. Both sides continued to assert their right to establish a state.\n\n\n# 1948: Israel declares independence\n\nUK, US, WW2 winners (Антантата) decide to divide the territory between Israelis and Palestinians.\n\nIsrael declares independence in May 1948. The next day, a coalition of Arab states, allied with Palestinian factions, attack Israeli forces in what becomes the first of several Arab-Israeli wars. In the end, Israel gains control of an even larger portion of territory — not including the areas of the West Bank and Gaza Strip. An estimated 700,000 Palestinians flee or are driven from their land in what Palestinians refer to as the “Nakba,” or “catastrophe” in Arabic.\n\n# July 1956: The Suez Crisis\n\nSuez canal WAS on the border between Israel and Egypt. It is the connection between the Mediterranean Sea and the Red Sea. It is the shortest sea link between Europe and Asia.\n\nEgypt nationalized the Suez Canal and Israel, France and the UK attacked Egypt. A peace deal, backed by the United States and Soviet Union, ends the fighting.\n\n\n# 1967: The six day war \n\nIn June of 1967, a war known as the “Six-Day War” or the 1967 Arab-Israeli War breaks out amid lingering conflicts, including Egypt’s continued blockade of shipping into the Gulf of Aqaba. Israel takes control of the Gaza Strip, Sinai, the West Bank, the Golan Heights and predominantly Palestinian East Jerusalem. The Arab armies suffer massive losses.\n\n# October 1973: The Yom Kippur War\n\n\nA coalition of Arab nations, led by Egypt and Syria attacks Israel on Yom Kippur, a Jewish holy day when nobody works.\n\n\n# September 1978: Egypt Israel deal\n\nIt lays the foundation to a peace deal between the two countries the next year, including Israel’s eventual withdrawal from the Sinai Peninsula. It also sets out a framework for a process of Palestinian self-government in the West Bank and Gaza. \n\nPalestine Liberation Organization (PLO) is recognized by Israel and the United States as a negotiating partner.\n\n# 1993: Oslo accords.\n\nPLO and Israel chart the expansion of a limited Palestinian self-rule in the West Bank and Gaza. Left unresolved, however, are key issues such as Israeli settlements in the West Bank and the status of Jerusalem, which is viewed by the Palestinians as the capital of any future state.\n\n\n\n# 2006: Hamas elected in Gaza\n\nIsrael withdraws its troops from Gaza in 2005. The Palestinian militant group Hamas wins legislative elections the next year, leading to political strains with the more moderate Fatah party controlling the West Bank.\n\n\n# December 2008: Israel attacks Gaza\n\nAfter rockets barrages into Israel by Palestinian militants.\n\n\n\n# December 2017: U.S. recognizes Jerusalem as capital\n\nThe Trump administration recognizes Jerusalem as the capital of Israel and announces that it plans to shift the U.S. Embassy from Tel Aviv, stirring outrage from Palestinians.\n\n\n# December 2022: Netanyahu sworn in for sixth term\n\nBenjamin Netanyahu is sworn in again as Israeli prime minister, after winning an election that gives him his sixth term and elevates a once-fringe bloc of far-right politicians into powerful seats. He cobbles together the most far-right government in Israeli history, which critics say has begun to crush any prospect of a two-state solution.\n\n# October 2023: Israel says it’s ‘at war’ after Hamas attack\n\nNetanyahu formally declares war on Hamas on Oct. 8 following a surprise assault by Hamas militants that came a day after the 50th anniversary of the start of the 1973 Yom Kippur War.\n\nThe U.S., E.U. and Britain condemned the attacks while pledging support to Israel. The leaders went on to say that they recognize the “legitimate aspirations” of Palestinians but that Hamas “does not represent those aspirations.”\n\nUAE have condemned the attacks. \n\nBahrain joined the UAE in calling Hamas’s attacks an “escalation” and denounced the “reported kidnappings.”\n\nSaudi Arabia, Egypt and Jordan, Israel’s neighbors, urged restraint.\n\nIran congratulated Hamas, while Qatar said Israel was to blame for the violence.\n\nTurkey and China, called for both sides to end violence.","n":0.036}}},{"i":69,"$":{"0":{"v":"Holocaust","n":1}}},{"i":70,"$":{"0":{"v":"Green deal","n":0.707},"1":{"v":"\n# Green deal\n\nCarbon border adjustment mechanism CBAM:\n- environmental tax on imports and exports\n- carbon neutral by 2050 for the EU block\n- European Parliament voted for a plan to raise the cost for firms to produce carbon\n- approved in 2020\n- finances green projects\n\n## Brussels effect\nRelies on the domino effect to make the rest of the world follow suit. Climate policy is the first big area where the “Brussels effect” will be properly tested. On issues such as online privacy, where the EU’s norms have become global ones, countries and companies fall in line because it is not especially expensive to follow suit. Likewise, timber producers in Indonesia have to abide by EU rules otherwise they will not be able to sell into its market. In short, the Brussels effect works when following EU rules is an easy or obvious compromise to make. Expecting other countries to mimic an expensive carbon-trading system is a much bigger ask—particularly when all it guarantees is access to a market that will only shrink as a proportion of the global economy.\n\n## Get the green right\n\nNational taxes are more likely to lead to leakage, where polluting activity shifts across borders.\n\n# Carbon prices\n\nShould we put a price on carbon?\n\nAs of Oct 1, 2023 a quarter of global emissions are now covered, and the share is rising fast.\n\n\nNow the price in EU is 80 EUR per tonne of CO2, and it's rising fast. It was 30 EUR in 2017.\n\n1 tonne of CO2 is equivalent to:\n- 2,500 miles = 4k driven by an average gasoline-powered passenger vehicle \n- 120,000 smartphones charges\n- 500 litres of Diesel consumed\n\n\nA global carbon price would produce far greater economic benefits than border taxes, but would require closer international co-operation.\n\nHow woul you tax Imports?? It is hard to calc carbon emissions.\n\n\n# Effects of carbon prices\n- push industrial jobs overseas, beyond the reach of emissions-trading schemes\n- lower economic activity and raise consumer prices\n- it does reduce emissions (A/B test with synthetic control on UK shoed 20-26% decrease)\n-  encourage green innovation","n":0.055}}},{"i":71,"$":{"0":{"v":"Globalisation","n":1},"1":{"v":"# Globalisation\n\nAfter the cold war, the world saw the power of markets and globalisation took off in the 1990s.\n","n":0.229}}},{"i":72,"$":{"0":{"v":"Global warming","n":0.707},"1":{"v":"\n# Greenhouse effect\n\nThe greenhouse effect is a natural process that warms the Earth's surface. It occurs when the sun's energy reaches the Earth, some of it is reflected back to space, and the rest is absorbed and re-radiated by greenhouse gases in the atmosphere. These gases trap heat and prevent it from escaping back into space, similar to how a greenhouse traps heat, hence the name.\n\nGreenhouse gases, such as carbon dioxide (CO2), methane (CH4), and water vapor (H2O), naturally exist in the Earth's atmosphere. They allow sunlight to pass through but trap heat, creating a warming effect. This process is essential for life on Earth because it keeps the planet's surface warm enough to support life.\n\nHowever, human activities, especially the burning of fossil fuels and deforestation, have significantly increased the concentration of greenhouse gases in the atmosphere. This enhanced greenhouse effect intensifies the warming of the Earth's surface, leading to global warming and climate change.s","n":0.08}}},{"i":73,"$":{"0":{"v":"Fossil fuel","n":0.707},"1":{"v":"# Fossil fuel = изкопаеми горива\n\nMillions to hundreds of millions of years ago, the remains of plants and animals (such as diatoms) built up in thick layers on the earth’s surface and ocean floors, sometimes mixed with sand, silt, and calcium carbonate. Over time, these layers were buried under sand, silt, and rock. Pressure and heat changed some of this carbon and hydrogen-rich material into coal, some into oil (petroleum), and some into natural gas.\n\n# Coal\nCoal is called a fossil fuel because it was made from plants that were once alive! Since coal comes from plants, and plants get their energy from the sun, the\nenergy in coal also came from the sun. The coal we use today took millions of years to form. We can’t make more in a short time. That is why coal is called nonrenewable.\n\n\nBurning coal was easier because coal burned longer than wood and, therefore, did not have to be collected as often. People began using coal in the 1800s to\nheat their homes. Trains and ships used coal for fuel. Factories used coal to make iron and steel. Today, we burn coal mainly to make electricity.\n\n\nCarbon dioxide (CO2) is a gas we all produce when we breathe. However, burning fossil fuels like coal, oil, and natural gas for energy makes a lot of extra CO2. Too much CO2 in the air can trap heat, causing global warming.\n\nScientists are working on ways to capture CO2 from power plants before it goes into the air. They want to store it deep underground or in the ocean to prevent it from adding to climate change. This process is called carbon capture and storage, or carbon sequestration.\n\nEven though we still use a lot of coal and other fossil fuels for energy, capturing CO2 from power plants could help fight climate change. Scientists are researching these methods to make sure they are safe for the environment, so we can continue using energy without harming our planet.\n\n## Natural Gas\nNatural gas forms from ancient buried plants and animals deep underground. It's trapped in rocks and extracted through drilling, providing us with a clean-burning energy source for heating, cooking, and electricity.\n\nWhat is Natural Gas:\nNatural gas is a type of fuel that comes from deep underground. It's made up of gases like methane and is a clean-burning energy source.\n\nWhere is it Used:\nNatural gas is used in homes for cooking, heating water, and warming houses. It's also used in power plants to generate electricity. Additionally, it's used in some vehicles as a fuel instead of gasoline.\n\nHow it's Used:\nIn homes, natural gas flows through pipelines and is used in stoves and water heaters. Power plants burn it to produce electricity, which powers our homes and businesses. In vehicles, natural gas is stored in special tanks and used as fuel for engines.\n","n":0.046}}},{"i":74,"$":{"0":{"v":"Economic cycles","n":0.707},"1":{"v":"\n# How The Economic Machine Works\n\nSuper useful [video](https://www.youtube.com/watch?v=PHe0bXAIuk0&t=606s) with basic concepts on how the economy works.\n\n## What is Economy?\n\nThe Economy is the set of all transactions in the market. A transactions is the exchange of money with goods or services. **Each transactions is one person's spending and another person's income.** Hence total spend in one country is the total income in that country.\n\n\n## What is Credit?\n\nCredit is just an IOU. It is a promise that one person makes to another. **Credit is a liability to the one taking it and an asset to the one giving it.**\nRather than being a modern innovation, credit is a very primitive and fundamental economic building block. The concept of credit has been created before \nthe concept of money. Types of credit:\n\n- car loan (liability)\n- mortgage (liability), but you can get asset if you give a rent contract\n- shares (asset)\n- savings account (asset)\n- work contract: you work now, the company promises to pay you in the end of the month (asset if you are employee)\n- rent contract: you pay now, the landlord promises you to provide shelter for the month (liability if you are renting)\n- student loan (liability)\n\n## Why there are economic cycles?\n\nThe reason economic cycles exist is because there is credit. In an economy with no credit there will not be any cycles. Credit allows\npeople to spend money now and pay back in the future. In economy with no credit thre will be no cycles. Think of your mum's and dad's lives. They have lived their whole live with no credit and their lives has been relatively linear.\n\n\nEconomic cycles exists because there is credit!!! Credit intrinsically has a cycle nature - spend now, pay in the future. Money on the other hand is not the same as credit. Spend now and pay now. You earn then you spend and so on. Money has a linear nature.\n\n- In economy with no credit, increase in spend can be done only with increase with money/income increase with productivity\n- In economy with credit, increase in spend can be done using credit (created out from thin air)\n\n**Definition**. The economy is the space of all transactions in the market. A transaction is an exchange of money with product/service and can be funded with\nmoney either from income or from credit.\n\nMarket is the place where people buy and sell products/services.\n\n**Lemma**. Each person's spending is another's person income. Therefore higher spending leads to higher income.\n\n**Theorem**. Price = Total spend/Total products\n\n**MOST OF THE SPEND IS FROM CREDIT and not money! Most transactions are funded with credit**. As credit has cyclyc nature, the market will have cycles.\n\n**Lemma. When credit is easily available there is an economic expansion, when it is not easily available there is a recession**\n- When credit is easily available economy is expanding. When credit is not available the economy is contracting\n\n**Lemma**. Credit is a liability to a borrower and an asset to a lender.\n  \n### The short term debt cycle\n\nThe short term debt cycle (5-8 years) is controlled mainly by the interest rates. When credit is cheap, total spend increases, prices increases, leading to INFLATION. Central bank does not like inflation. To tackle it they increase interest rates. Credit becomes expensive, people borrow less, pay more for interest and this leads to decrasing total spend, price decrease = DEFLATION. Since one person spend is anothers person income, income decreases, economic activity decreases = RECESSION.\n\nAfter this increase of interest rates, the bank waits until inflation decreases and recession is not a big problem. Then central bank decreases interest rate and this marks the End of the short term debt cycle. **Next short term debt cycle would have larger spend and larger credit. People like to push it.**\n\nHaving many growing short term debt cycles is not sustainable and cannot continue forever...\n\n### Long term debt cycle\n\nWhen debt repayments become higher than income for long period of time, total spend reduces as people spend more to cover their debt.  Then spend decreases, and since one person's debt is another person's income, income decreases. The Central Bank decreases interest rates and when it hits 0 and people still have large debt this is when the DEPRESSION begins. There is no iterest on the debpt but people still cannot repay their debt. Identify DEPRESSION by looking at the DEBT Burden. What comes next is the **economy deleveraging**, same as depression. Less spending - Less income - Less borrowing - Less credit vicious cycle.\n\nEconomic depression is the state of the market whereas economic deleveraging is the process that is happening during this market state. Market depression is very similar to recession.\nHowever, the difference is that in recession the government can decrease interest rates to stimulate more borrowing, more credit more spending. In depression interest rates are already very low (Japan's lost decade). Japan's interest rate was negative for 8 years. From 2000 to 2020 Japans debt rose from 100% to 200% relative to GDP.\n\nDuring deleveraging these 4 things happen, all with the goal to reduce the debt burden:\n- people, businesses and even governments cut spending to pay down their debt\n- debt restructuring and defaults(pain)\n- redistribute wealth from those that have to those that have not\n- bank prints money\n\n- 1 could not work when there is large decrease in spend. Since one person's spend is another person's income, total income decreases and could decrease faster than debt.\n- 2 is usually not enough measure\n- 3 income decreases means taxes decrease. Government need to raise more money to cover costs and help the poor\n- 4 bank prints money when interest rates are already close to 0 and there is crisis. It uses to buy financial assets and government bonds. Happened in 1930 and in 2008. The central bank prints money, and loans using bonds to the central government which then redistributes it to people in need.\n\n**Proposition**. When the bank prints money, this helps only those who have financial assets!!! And those who use stimulus programs.\n\nLong term debt cycle:\n- Leveraging 50 years\n- Deleveraging/depression 2-3 years\n- Reflation 7 years\n\nHence the term lost decade. After the reflation things are back to the same level as after 50 years of leveraging. Deleveraging is the process of selling assets to reduce debt. Paying off your debt.\n\n### Takeaways\n- Identify where you are in the short term cycle by looking at the interest rates.\n- Identify the long term cycle by looking at the debt burden. In 2008 USA had the highes Houshold Debt to GDP ratio: https://tradingeconomics.com/united-states/households-debt-to-gdp\n\n**You need to navigate in short term cycles and defend yourselve from the big drop in the long term cycle.**\n\nLet's recap the short term cycle phases:\n- grow, decrease, grow again,  decrease again\n- low interest rates, high interest rates, low again, high again\n- growth -> inflation -> deflation -> new cycle begins with growth that is higher than the previous growth\n\n(?) The best time to buy when interest rates are low again -> this is when the new cycle will start You need to save cash for this time. \nBest time to sell (usually after 7-8 years) is when interest rates are high, as this will mark the start of the deflation (end of the cycle).\n\n**Lemma**. During low interest rates you need assets, during high interest rates you need cash.\n\n**Lemma**. Note the timeline for the short term debt cycle can be very different for different markets (per country, per city, per neighborhood, per asset)\n\nActions during long term cycle. In your lifetime you'd expect to have 1 or max 2 depressions. There is only one/two VERY important action you need to take:\n- if you can identify the peak or near the peak of the end of the long term debt cycle (track debt burden). SELL ASSETS\n- at the end of the depression BUY ASSETS \n\nWorst case scenarios if you mess up:\n- If you buy at the peak of short term cycle, you'd be screwed for 4-5 years, after which you'd be at 0. Lost 4-5 years and then in 4-5 years you are at profit.\n- If you buy at the peak of the long term cycle you lose 10 years and would be in profit in 15-16 years. (example. 2008 till  2023)\n\n\n1. Don't confuse recession with depression! Lots of people do mot act because of fear of depression.\n2. Don't have debt rate larger than income. Debt burden will crush you.\n3. Dont have income rise faster than productivity. You won't be competitive.\n4. In the long term all that matters is productivity.\n\n---------\n\nCurrency devaluation in 1970 Nixon says \"In order to defend the dollar we would exchange dollar for gold only when it is in the best interest for America\" The truth was that Ameraca would have defaulted as they did not have enough gold. This allowed to the US spend more than they earned as they could print more.  In 1930 Roosevelt put embargo on gold too. In both occasions this caused the stock prices to rise. As the bank could print 🤑🤑🤑 **WHENEVER THE BANK PRINTS MONEY THE VALUE OF STOCKS, COMMODITIES ASSETS RISE AND THE VALUE OF PAPER MONEY WILL FALL.**\n\n- The 1920 economic boom led to the 1930 depression\n- The 2007 housing bubble led to the 2008 housing crash\n","n":0.026}}},{"i":75,"$":{"0":{"v":"Papers","n":1},"1":{"v":"\n- HEART, Google UI metrics\n\n- RecSys, Matrix factorization\n\n- Look in algorithms repo Readings and you have lots of papers there\n\n\n[Papers With Code](https://paperswithcode.com/)","n":0.213}}},{"i":76,"$":{"0":{"v":"Squaring the plane","n":0.577},"1":{"v":"[Squaring the plane](https://drive.google.com/file/d/1BX0tL1ksgh926C6rEN7QWUS8io4auXJg/view?usp=sharing) paper. Famous problem of creating a square of unique squares, also known as tiling. This paper solves the question of whether or not the infinite plane could be tiled using one copy of each square of integer side length: one\ncopy of the squares $(1×1), (2×2), (3×3)$, and so on.\n\nImperfect squares problem is if there could be repeated squares. You can use Fibonacci trick to get this easily.\n\nThe pape gives an algo which they prove that would fill in the infinite plane with squares.\n\n\n1. Start with any perfect ell and square it up.\n2. Create a new ell by attaching to the rectangle the smallest square not yet used.\n3. Square (*rectanlge*) this ell up, making sure that new squares are added in all four directions.\n4. Repeat steps 2 and 3 ad infinitum.\n\n\n**Theorem.** It is possible to tile the plane with nonoverlapping squares using exactly one square of each integral side. Note the plane is a rectangle.\n\nRelated problem by [Numberphile](https://www.youtube.com/watch?v=NoRjwZomUK0)\n","n":0.079}}},{"i":77,"$":{"0":{"v":"Intention-to-treat concept","n":0.707},"1":{"v":"\n[Paper](https://drive.google.com/file/d/14i8Mjy3OMFIWsef-xzwZ-n5cIEen2-eR/view?usp=share_link)\n\nOne practical problem that investigators usually come across in RCT (randomized controlled trials) is that subjects do not always follow instructions. (medical drug experiments). Hence, RCT often suffers from two major complications, i.e., noncompliance and missing outcomes.\n\nITT analysis includes every subject who is randomized according to randomized treatment assignment, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol.\n\nITT analysis avoids overoptimistic estimates of the efficacy of an intervention.\n\nIIT analysis reduces type I error.\n\nMany arguments against ITT analysis appear valid. To begin with, if a subject who actually did not receive any treatment is included as a subject who received treatment, then it indicates very little about the efficacy of the treatment. In ITT analysis, estimate of treatment effect is generally conservative because of dilution due to noncompliance.\n\nITT analysis has been criticized for being too cautious and thus being more susceptible to type II error.\n\n\nUse IIT as complementary method.\n\n\"When the ITT and per-protocol (PP) analyses come to essentially the same conclusions, confidence in the study results is increased.\"\n\nPP analysis is defined as a subset of the ITT population who completed the study without any major protocol violations.","n":0.071}}},{"i":78,"$":{"0":{"v":"In-Shuffle algorithm","n":0.707},"1":{"v":"$O(n)$ time, $O(1)$ space in-shuffle algoritm\n\nhttps://arxiv.org/pdf/0805.1598.pdf\n\n[leetcode](https://leetcode.com/problems/shuffle-the-array/)","n":0.408}}},{"i":79,"$":{"0":{"v":"Greedy Function Approximation GBM","n":0.5},"1":{"v":"#TODO [Paper](https://jerryfriedman.su.domains/ftp/trebst.pdf)","n":0.707}}},{"i":80,"$":{"0":{"v":"Mental Health","n":0.707},"1":{"v":"I found these good notes somewhere long time ago but don't remember whose are those.\n\n# Fundamental model\n\nOne way of looking at emotions is as a gradient for wants. All people have a set of root (terminal) desires and values (these might differ from person to person), which they prioritize in different ways. A person’s brain takes these abstract, vague root desires / values and breaks them down into proxy desires and goals. The brain creates emotions as a way to signal whether these proxy goals are being achieved or not. Positive emotions are generated when the brain perceives that the proxy goals are being achieved; negative emotions are generated when the brain perceives that the proxy goals are not being achieved.\n\n**People have root wants / desires.**\n\nThese are usually impossible, or at least extremely difficult and slow to change (one way of intentionally changing them is by repeated targeted application of the depersonalization trick which is described in a future section). The brain breaks these down into proxy wants / desires and goals. The exact way this happens depends on your worldview / belief system.\nEmotions are generated by the brain to signal whether the brain thinks that the proxy goals are being achieved or not.\n\n# Discovering what you want\n\nDirect inquiry:\n\nA simple tool for discovering your root wants and structure of the proxy goals that your brain is generating is repeatedly asking the question why. The algorithm works like this:\n1. You experience a negative emotion E.\n2. You ask yourself “Why am I experiencing E?” Your brain produces a reason X_1\n3. You ask yourself “Why would X_1 make me experience E?” Your brain produces a reason X_2\n4. You ask yourself “Why would X_2 make me experience E?” …\n5. You continue this cycle until you hit a root desire, or until you are genuinely unsure as to why X_n makes you experience E.\n\nWhen using this process, you want to be careful and to make sure the reasons you’re coming up with are the real reasons why you are experiencing the emotion E. This is not always easy to do.   \n\n# Focusing [2]\n\nFocusing is a mental tool that helps you investigate and examine what your root desires are, and what are the proxy goals that your brain has constructed. \n\nStep 1: Discover. Figure out what your emotional experience is at this moment. Pay attention to how your body is feeling\n- What is my experience like right now?\n- Does everything in my life feel fine right now?\n- What’s between me and feeling fine?\n\nStep 2: Select a single emotion / sensation from everything that arose in the previous step.\n\nStep 3: Understand the felt sense\n- How does this emotion feel physically in your body?\n\nAsk questions:\n- What’s the worst of this feeling?\n- What needs to happen for this to feel fine?\n\nMake hypotheses and guesses as to what the core of the felt sense is, and ask your subconscious mind if they are correct. Run thought experiments to investigate how you would feel in different hypothetical situations. Often, you'll hit on some words or phrases that \"click\", but aren't a complete answer. Build on them and explore the most promising areas.\nAt some point, you'll finally hit on a description that \"clicks\" really well, and feel a release of tension. This is the core of the felt sense.\n\n# Exploration\nExplore different things (e.g.  places / people / social groups / activities / books / art / etc) and observe the thoughts and emotional responses that they generate in you. Try to draw inferences about your proxy goals and core values from these thoughts & emotions.\n\nThis kind of exploration can be more or less directed, depending on what your specific needs are.\n\n\n# Handling strong emotions and urges\n\n## Exposure therapy\n \nPractice exposing yourself to and managing your mental state in milder versions of the situation that is causing the strong emotion. Gradually increase the intensity of the situations you practice in.\n\n## Objectifying / Impermanence trick\n\nWhen you experience a strong emotion or urge: [3]\n1. Hold the emotion / urge in your attention without acting on it or analyzing it. Allow it to be present.\n2. Observe what the emotion / urge physically feels like\n3. Observe what the emotion / urge mentally feels like\n4. Reflect on how the emotion is just a “thing” / “object” that happens to currently be present in the world, just as any other “thing” / “object”\n5. Observe how the physical and mental sensations are evolving and changing over time.\n6. Reflect on the impermanence of the emotion\n\n## Depersonalization trick\nWhen you experience a strong emotion or urge: [3]\n1. Apply the impermanence trick.\n2. Take the “self” as your object of focus. Reflect on how the “self” is just a continuous stream of conscious experience - nothing more, nothing else.\n3. Ask yourself: “Who is having this emotion / urge?”\n\n## CBT approach (evaluating the perception & proxy goal)\nAsk the following three questions: [1]\n1. Is the perception that the proxy goal is not being met accurate?\n\nExample: I’m feeling angry at my girlfriend because I feel like we haven’t been spending enough time together. However, when I look at my calendar, I notice that we’ve actually been spending a fair amount of time together.\n\nEven if the perception is accurate, does the proxy goal itself make sense? Is it realistic and does it actually help the root desire?\n\nExample (unrealistic): I get along very well with almost everyone in my class, except for a few people. This makes me depressed because I feel like I need to be liked and approved of by everyone or else I’m a bad person.\n\nExample (irrelevant): I’m experiencing anxiety because my college GPA is 3.5. I need to get higher grades in order to be able to later succeed as a startup founder, and I need to succeed as a startup founder in order for people to respect me.\n\nEven if the perception is accurate AND the proxy goal makes sense, does experiencing the emotion help in any way with achieving the goal? What is the most effective way to channel this emotion towards achieving the goal?\n\nNoticing errors / distortions in proxy goal construction & evaluation: [1]\n\nOveremphasizing negative patterns:\n\n1. All-or-nothing thinking: If your (the other person’s) performance falls short of perfect (or another binary label), you see yourself (them) as a total failure.\n2. Overgeneralization: You see a single negative event / quality as a never-ending pattern.\n3. Mental filter: You pick out a negative detail in any situation and dwell on it exclusively, thus perceiving the whole situation is negative.\n4. Magnification / Catastrophizing: You exaggerate the importance of your errors / mistakes / bad qualities (or those of other people).\n5. Labeling and Mislabeling: Instead of describing your (the other person’s) error, you attach a negative label to yourself (the other person).\n\nSeeing negative patterns where there are none:\n\n1. Jumping to conclusions: You make a negative interpretation even though there are no definite facts that convincingly support your conclusion.\n2. Mind-reading: You arbitrarily conclude that other people are reacting negatively to you (or intentionally being malicious).\n3. Fortune teller error: You imagine that something bad is about to happen, and you take this prediction as fact even though it’s unrealistic\n4. Emotional Reasoning: You take your negative emotions as evidence for the truth. E.g. “I feel inadequate, therefore I must be a worthless person”; “I’m angry at you - this means you’ve done something bad!”\n5. Should statements: You try to motivate yourself by saying “I should do this” or “I must do that”. You direct should statements towards others: “He should to X”, leading to frustration.\n6. Personalization: You see yourself (another person) as the cause of some negative external event which in fact you (they) were not primarily responsible for - e.g. “My partner failed to pass her job interview, so I must be bad at giving interview preparation.”\n\nMistakenly deemphasizing positive patterns:\n1. Disqualifying the Positive: You reject your positive experiences (or another person’s positive actions / qualities) by insisting they “don’t count” for some reason or other.\n2. Minimization: You inappropriately shrink your (another person’s) successes or positive qualities until they appear tiny and inconsequential.\n\nGenerating specific emotions\n\n- Music\n\n## Resources\n[1] Feeling Good (Cognitive Behavioral Therapy)\n\n[2] Focusing - Eugene Gendlin\n\n[3] Buddhism and Modern Psychology - Robert Wright\n\n","n":0.027}}},{"i":81,"$":{"0":{"v":"Brain Tricks","n":0.707}}},{"i":82,"$":{"0":{"v":"Math","n":1}}},{"i":83,"$":{"0":{"v":"Oxford","n":1}}},{"i":84,"$":{"0":{"v":"Prelims","n":1}}},{"i":85,"$":{"0":{"v":"Statistics","n":1},"1":{"v":"# Buzzwords\n\nrandom sample = iid = independent and identically distributed\n\nsum of normal distributions is normal\n\nsum of jointly normal distributions is normal\n\n$var(aX + b) = a^{2}var(X)$\n\nunbiased estimator (sample variance) divides by $n-1$ to have unbiased estimator of the TRUE variance\n\nMLE = maximum likelihod estimator \n\nlikelihood function $L(\\theta|x_1.. x_n)$\n\n\nIf you parametrize $p = \\dfrac{1}{\\theta}$ in Geometric distribution and find MLE for $\\theta$ you will have unbiased MLE! The MLE for $p$ is not unbiased (its overestimating)\n\n# Introduction. Random samples. Summary Statistics, MLE\n\nProbability: in probability we start with a probability model $P$, and we deduce properties of $P$.\n\nE.g. Imagine flipping a coin 5 times. Assuming that flips are fair and independent, what is the probability of getting 2 heads?\n\nStatistics: in statistics we have data, which we regard as having been generated from some unknown probability model $P$. We want to be able to say some useful things about $P$.\n\n\n**Definition.** A random sample of size $n$ is a set of random variables $X_1 , . . . , X_n$ which are independent and identically distributed (i.i.d.).\n\noften you will compute the joint pmf of $X_1,...,X_n$. This joint pmf gives you the probability you observe the sample data $x_1,...,x_n$,\n\nIn probability we assume that parameters $λ$ and $µ$ in our two examples are known. In statistics we wish to estimate $λ$ and $µ$ from data.\n- What is the best way to estimate them? And what does “best” mean?\n- For a given method, how precise is the estimation?\n\n**Sample mean** and **Sample variance**, **sample standard deviation**\n\nDenominator in sample variance is $n-1$, so that the sample variance will be what is called an **unbiased** estimator of the population variance. If we divide by $n$ our sample variance would be underestimating the TRUE variance on average. **We need to divide by the degrees of freedom, not the number of samples!** [stack](https://stats.stackexchange.com/questions/406327/degrees-of-freedom-in-sample-variance)\n_\nGiven observations $x_1,...,x_n$ we can compute the observed values $x$ and $s^2$ .\n\nWe use sample mean, variance, standard deviation to estimate the TRUE unknown mean, variance, standard deviation.\n\n![summary_stats.png](assets/images/summary_stats.png)\n\n**MLE = Maximum Likelihood Estimation**\nMaximum likelihood estimation is a general method for estimating unknown parameters from data.\n\n\n**Definition.** Let $X_1,...,X_n$ have joint p.d.f./p.m.$f(x; θ)$. Given observed values $x_1,...,x_n$ of the **likelihood** of $θ$ is the function:\n$L(θ) = f (x; θ)$\n\n$L(\\theta)$ is the joint pmf, pdf of the observed data and is regarded as a function of $\\theta$ for fixed **$x$**.\n\nDefinition. The maximum likelihood estimate (MLE) is the value $\\hat{\\theta}$ that maximizes the likelihood $L(\\theta)$ (or log-likelihood).\n\n\nThe idea of maximum likelihood is to estimate the parameter $\\theta$ that gives the greates likelihood to the obsrvations $x_1,...,x_n$.\n\n\n\nEstimator:\n- A rule for constructing an estimate.\n- A function of the random variables $X$ involved in the random sample.\n- Itself a random variable.\nEstimate:\n- The numerical value of the estimator for the particular data set.\n- The value of the function evaluated at the data $x_1,...,x_n$.\n\n# Parameter estimation\n\n\n**Definition.** A **statistic** is any function $T(X)$ of $X_1,...,X_n$ that does not depend on $\\theta$.\n\n**Definition.** An **estimator** of $θ$ is any statistic $T(X)$ that we might use to estimate $θ$.\n\n**Definition.** $T(x)$ is the **estimate** of $θ$ obtained via $T$ from observed values $x$.\n\n\nWe can choose between estimators by studying their properties. A good estimator should take values close to the TRUE parameter $θ$.\n\n**Definition.** An estimator $T=T(X)$ is **unbiased** for $\\theta$ if $E(T) = \\theta$. This means that “on average” $T$ is correct.\n\n\n**Definition.** The mean squared error (MSE) of an estimator $T$ is defined by $MSE(T) = E[(T − θ)^2].$\n\n**Definition.** The bias $b(T)$ of $T$ is defined by $b(T) = E(T) − θ$.\n\nMSE is a measure of the **“distance”** between $T$ and the true parameter $θ$\n\nHowever $MSE(T)$ and $b(T)$ may depend on $\\theta$.\n\nNote $MSE(T) = var(T) + b(T)^2$ (bias variance trade-off)\n\nSo an estimator with small MSE needs to have small variance and small bias.\n\n\nMLEs are usually asymptotically unbiased, and have MSE decreasing like $1/n$ for large $n$.\n\n**USE MSE(T)** to compare different estimators for $\\theta$.\n\n# Accuracy of estimation: Confidence Intervals\n\nA crucial aspect of statistics is not just to estimate a quantity of interest, but to assess how accurate or precise that estimate is. One approach is to find an interval, called a confidence interval (CI) within which we think the true parameter falls.\n\n\n**Definition.** If $a(X)$ and $b(X)$ are two statistics, and $0 < α < 1$, the interval $(a(X), b(X))$ is called a confidence interval for $θ$ with confidence level $1 − α$ if, for all $θ$:\n\n$P(a(X) < θ < b(X)) = 1 − α$\n\nThe interval $(a(x), b(x))$ is called an interval estimate and the random interval $(a(X), b(X))$ is called an interval estimator.\n\nNote: $a(X)$ and $b(X)$ do **not** depend on $θ$.\n\nWe want small intervals and $P(a(X) < \\theta < b(X))$ to be large.\n\n\nBy the same argument as before, if $X_1 , . . . , X_n ∼ N(µ, σ_{0}^{2})$ with $σ_{0}^{2}$ known, then a level $1 − α$ confidence interval for $µ$ is\n\n$(\\bar{X}-\\dfrac{z_{\\alpha/2} \\sigma_{0}}{\\sqrt{n}}, \\bar{X}+\\dfrac{z_{\\alpha/2} \\sigma_{0}}{\\sqrt{n}})$\n\nThe more data I have the smaller interval I get!\n\nif variance is unknown use **SAMPLE VARIANCE** (division by $n-1$).\n\n**Interpretation of a Confidence Interval**\n\n- The parameter $\\theta$ is fixed but unknown.\n- If we imagine repeating our experiment then we’d get new data, **$x'$**\nsay, and hence we’d get a new confidence interval $a(x'), b(x')$. If we did this repeatedly we would “catch” the true parameter value about $95\\%$ of the time, for a $95\\%$ confidence interval: i.e. about 95% of our intervals would contain $θ$.\n- The confidence level is a coverage probability, the probability that the random confidence interval $a(X), b(X)$ covers the true θ. (It’s a random interval because the endpoints $a(X), b(X)$ are random variables.)\n\nYou always get confidence interval estimates. You cannot say $(523,3313)$ is 95\\% confidence interval. It either does or does not contain the TRUE parameter $\\theta$. You cant say which as $\\theta$ is unknown.\n\n\n**Confidence Intervals using the CLT**\n\n\n(estimate ± 2 estimated std errors)is an approximate 95% CI\n(estimate ± 3 estimated std errors)is an approximate 99.8% CI.\n\n**Example.** Estimating the TRUE variance and stadard deviation of the sample population.\n\n- Let $X_1,...,X_n ∼ N(\\mu, \\sigma^{2})$ be iid. Then $\\hat{\\mu} = \\bar{X}$ and estimate of the variance is $var(\\hat{mu}) = \\dfrac{\\sigma^{2}}{n}$ and standard deviation $SE(\\hat{\\mu}) = \\dfrac{\\sigma}{\\sqrt(n)}$\n\nThe variance and SE of an estimator $\\hat{\\mu}$ might themself depend on anothe paramater. We need to plug in $\\hat{\\sigma}$ with the Maximum likelihood estimator (divides by $n$) or other estimate (sample variance divides by $n-1$ and is **unbiased**).\n\n\n**Given a radnom normally distributerd sample, when constructing  a confidence interval you should use the sample variance. If you use the MLE to estimate the $\\sigma$ you will underestimate the variance and your CI would be shrinked**\n\n$\\hat{\\mu} \\pm z_{\\alpha/2}\\dfrac{\\hat{\\sigma}}{\\sqrt{n}}$\n\n\n# Linear Regression\n\nSuppose we measure two variables in the same population:\n- $x$: the **explanatory variable, predictor, feature, input**\n- $y$: the **response variable, output**\n\n\nA linear regression model for the dependence of $y$ on $x$ is:\n\n$y_{i} = \\alpha + \\beta x_{i} + \\epsilon_{i}$\n\n\nwhere:\n- $x_1,...,x_n, y_1...,y_n$ are known constants (data points)\n- $\\epsilon_1,..., \\epsilon_{n}$ are i.i.d. $N(0,\\sigma^{2})$ \"random errors\"\n- $α, β$ are unknown parameters.\nThe “random errors” represent random scatter of the points $(x_i, y_i)$ about the line $y = α + βx$, we do not expect these points to lie on a perfect straight line.\n\n\n**Note: a linear relationship like this does not necessarily imply that $x$ causes $y$.**\n\n**Goal: estimate $\\alpha$ and $\\beta$**\n\n$Y_i ~ N(\\alpha+\\beta x_i,\\sigma^{2})$\n\nMaximize the likelihood $L(\\alpha,\\beta|x_i,y_i)$. Same as minimizing the square error $S(\\alpha,\\beta) = \\sum_{i=1}^{n}(y_i-\\alpha-\\beta x_i)^2$!\n\nVectorized linear regression problem: Estimate $\\beta$ for $\\bf{y} = \\bf{X}\\beta + \\epsilon$ (intercept is included as a col of ones)\n\nVectorized solution $\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y$ it is an unbiased estimator!\n\n95\\% CI for the true parameter $\\beta$ is $(\\hat{\\beta} \\pm 1.96 SE(\\beta))$\n\nThe standard deviation of the true parameter $SE(\\beta)$ is usually unknown and we need to estimate it.\n\n$var(\\beta) = X^{T}X var(y) = X^{T}X \\sigma^{2}$\n\nYou can use the The MLE: $\\hat{\\sigma}^{2} = \\dfrac{1}{n}\\sum(y_i-x_i\\beta)$ to get 95\\% CI: $(\\hat{\\beta} \\pm z_{\\alpha/2}SE(\\beta))$ \n\nA better approach is to estimate $\\sigma^{2}$ using $\\dfrac{1}{n-p-1}\\sum(y_i-x_i\\beta)$ because this is an unbiased estimator (on average you are correct) and to base the confidence interval on a chi square-distribution rather than a normal distribution. (t-distribution in univariate case $x_i$ is one dimensional and p = 1, division by $n-2$ in this case)\n\n[example](https://stats.stackexchange.com/questions/29981/should-confidence-intervals-for-linear-regression-coefficients-be-based-on-the-n)\n\n# Assessing model fit\n\nHaving fitted a model = estimated the parameters $\\beta$.\n\nHaving fitted a model, we should consider how well it fits the data. A model is normally an approximation to reality: is the approximation sufficiently good that the model is useful?\n\n**Definition.** The i-th fitted value is $\\hat{y_i} = x_{i}\\hat{\\beta}$\n\nThe i-th residual is $e_{i} = y_{i}-\\hat{y_{i}}$\n\nThe RSS (residual sum of squares) is $RSS = \\sum e_{i}$\n\nThe RSE (residual standard error) is $RSE = \\sqrt{\\dfrac{RSS}{n-p}}$ (this is an estimate of the standard deviation $\\sigma$).\n\n\n**Potential problem: non-linearity**\n\nA **residual plot** is a useful graphical tool for identifying non-linearity: for simple linear regression we can plot the residuals $e_i$ against the fitted values $\\hat{y_{i}}$ . Ideally the plot will show no pattern. The existence of a pattern may indicate a problem with some aspect of\nthe linear model.\n\n\n**Potential problem: non-constant variance of errors**\n\nNon-constant variance is also called **heteroscedasticity**.\nCan see funnel-type shape in the residual plot.\n\nHow might we deal with non-constant variance of the errors?\n- One possibility is to transform the response Y using a transformation such as $log(Y)$ or $\\sqrt{Y}$ (which shrinks larger responses more), leading to a reduction in heteroscedasticity.\n- If you know how variances behave for $Y_i$ and think $var(Y_i)=var(\\epsilon_i) = \\sigma^{2}/w_{i}$ you can take the approach called weighted least squares minimizing $\\sum_{i=1}^{n}w_i(y_i-x_i\\beta)$\n\n**Potential problem: outliers**\n\nAn outlier is a point for which $y_{i}$ is far from the value $\\hat{y_i}$ predicted by the model.\n\n\nIf we believe an outlier is due to an error in data collection, then one solution is to simply remove the observation from the data and re-fit the model. However, an outlier may instead indicate a problem with the model, e.g. a nonlinear relationship between $Y$ and $x$, so care must be taken.\n\nstudentized residuals = standardized residuals greater than 3 is signal for outlier\n\n**Potential problem: high leverage points**\n\n(Trailstone group interview...)\n\nOutliers are observations for which the response $y_i$ is unusual given the value of $x_i$. \n\nOn the other hand, observations with high leverage have an unusual value of $x_i$ .\n\n**Definition** the leverage of the ith observation is $h_{i}$ where\n\n$h_{i} = \\dfrac{1}{n} + \\dfrac{(x_i - \\bar{x})^{2}}{\\sum (x_j-\\bar{x})^{2}}\n\n\nTake-away: Points with high residuals pull the regression line towards them more than points with lower residuals (the more $\\hat{y_i}$ are wrong the more the line goes towards them)\n\nPoints with unusual value of $x_i$ far from the mean pull ht regression line a lot too. These are called high leverage points.\n\n\nWhy does this matter? We should be concerned if the regression line is heavily affected by just a couple of points, because any problems with these points might invalidate the entire fit. Hence it is important to identify high leverage observations.\n\n\n# Data Analysis\n\nLinear regression is an example of supervised learning $Y ~ X$.\n\nIn unsupervised learning you have only $X$. In this case you want to answer questions:\n\n1. Can we find a way to visualize the data that is informative?\n2. Can we compress the dataset without losing any relevant information?\n3. Can we find separate subgroups (or clusters) of observations that de-\nscribe the structure of the dataset?\n\nUnsupervised learning can be more challenging than supervised learning,\nsince the goal is more subjective than prediction of a response variable.\n\nExploratory data analysis is unsupervised learning.\n\ndata matrix = design matrix\n\nFirst step in modelling - EDA\n\nInter Quartile Range (IQR) - the difference between the 1st and 3rd quartiles. This is a measure of ‘spread’ within the dataset.\n\nBox plot.\n\n![box_plot.png](assets/images/box_plot.png)\n\nPair plots\n\n3D interactive plots\n\n**Simulation** is a technique for generating pseudo-random values that have a particular distribution.\n\n\n# The Multivariate Normal Distribution\n\n![mvn.png](assets/images/mvn.png)\n\nGiven a sample of $n$ observations $N_{p}(\\mu,\\Sigma)$, the MLE-s are:\n\n![mle_mvn.png](assets/images/mle_mvn.png)\n\n# PCA\n\nPrincipal components analysis (PCA) finds a low-dimensional representation of the data that captures as much of the information in the dataset as possible.\n\nPCA key points:\n- each component coming from PCA is a linear combination of the variables\n- each components looks for maximum varaibility in the data\n\n**components choose a good way to choose a projection that separates the two clusters**\n\n![pca.png](assets/images/pca.png)\n\nPCA is maximization problem. Tofind componene $\\alpha_{1}$:\n\n$max \\alpha_{1}^{T}S\\alpha_{1}$ subject to $|\\alpha_{1}| = 1$ \n\nWe try to maximize the sample variance of the first component. Solve this using Lagrange Multipliers.\n\nUsing vector calculus you will get:\n- $\\alpha_{1}$ is the eigen vector  with largest eigen value $\\lambda_{1} = \\alpha_{1}^{T}S\\alpha_{1}$\n\nThe way PCA algo work is by doing an **eigendecomposition** of the sample variance matrix $S = VDV^{T}$, where $D$ is diagonal matrix with the eighen values and $V$ is orthonormal/orthogonal $VV^{T} = I$ and $V$ has the eigenvectors of $S$\n\n**Plotting**\n\nIf we define $Z$ to be an $n × p$ matrix containing the transformed data, such\nthat $Z_{ij}$ is the value of the $j$th principal component for the $i$th observation\n\n$\\bf{Z} = \\bf{XV}$\n\n\nWe can then plot the columns of Z against each other to visualize the data as\nrepresented by those pairs of principal components. The matrix Z is known\nas the scores matrix.\n\n**Biplots** shows pair of PC-s how they cluster the data. It uses the data projection\non the principal components.\n\nThe **total amount of variance** in the original data matrix X is the sum of\nthe diagonal entries in $S$.\n\nIt is common practice to plot the decreasing sequence of eigenvalues to visualize ths structure in the dataset. Such plots are sometimes refered to as **eigenspectrum plots** or **variance scree plots**, and usually they are scaled so each bar is percentage of the total variance.\n\n\n**Question**\n\nApply PCA to raw data or to transformed data?\n\n\nThe first principal component maximises the variance of a linear combination of variables. If the variables have very different levels of variability then maximizing the projection with the largest variance may tend to dominate the first principal component.\n\nFor this reason, it can sometimes make sense to **standardize** the variables before PCA\n\nThis can be achieved by applying PCA to the sample correlation matrix R, rather than the sample covariance matrix S.\n\n\nIt can be shown (see Exercises) that the PCA components derived from using S are not the same as those derived from using R, and knowledge of one of these sets of components does not enable the other set to be derived.\n\n## PCA via SVD\n\n![svd.png](assets/images/svd.png)\n\nExpress design matrix $X$ using SVD and rewrite $S=\\dfrac{1}{n-1}X^{T}X$.\n\nYou can win on computation time if $n << p$.\n\nCalculating the eigendecomposition of $XX^{T}$ scales like $O(n^3)$ which\nis much less than the eigendecomposition of $X^{T}X$ which scales like $O(p^3)$.\n\n\n## PCA as minimizing reconstruction error\n\nThere is another way to derive principal components analysis that uses the\nidea that we might try to find the low-dimensional approximation that is as\nclose as possible to the original data.\n\n**data compression**\n\n\n# Clustering\n\nPCA provides low dimensional represeantaion of the data and show groupings of observations when visualized. It does not provide **labelling**.\n\n**Clustering** refers to a very broad set of techniques for finding subgroups,\nor clusters, in a dataset.\n\n**K-means**\n\nTo perform $K$-means clustering we must first decide upon the number of\nclusters $K$. The algorithm will assign each observation to excatly one of the\n$K$ clusters.\n\n![k-means.png](assets/images/k-means.png)\n\n\nGoals:\n- minimize **within** distance between points in same cluster\n- maximize distance between observations from different clusters\n\nK-means algo explicitly tackles the first goal by finding a local minimum. Finiding global minimum would require goind through all partitions of $n$ elements in $k$ \nsubsets which is factoriel like (Stirling numbers).\n\nThe two goals above are actually equivalent [see](https://stats.stackexchange.com/questions/158210/k-means-why-minimizing-wcss-is-maximizing-distance-between-clusters)\n\nK means objective function:\n\n\n![k-means-obj.png](assets/images/k-means-obj.png)\n\n**Multiple starts**\n\nThe algorithm does not always give the same solution since the start point is random.\n\n$K = n$ is **overfitting**\n\n\n**Hierarchical clustering**\n![dendrongram.png](assets/images/dendrongram.png)\n\nDendrograms can be used to cluster observations into a discrete number of\nclusters or groups\n\nDifferent ways to create a hierarchy. Here we consider **agglomerative** clustering approache.\n\n1. Begin with $n$ observations and a measure of all the $n \\choose 2$ pairwise dissimilarities, denoted $d_{ij}$ for $i,j ∈ (1, . . . , n)$. These dissimilarities\ncan be represented in a lower diagonal matrix, denoted $D^{(n)}$.\n\n2. For $i = n, n - 1, . . . , 2$\n- (a) Find the pair of clusters with the smallest dissimilarity. Fuse\nthese two clusters.\n- (b) Compute the new dissimilarity matrix between the new fused cluster and all other $i-1$ remaining clusters and create an updated matrix of dissimilarities $D^{(n−1)}$ .\n\nDistance betwwen two clusters:\n\n**single linkage** = minimum (closest) distance betwwen elements from the two clusters\n\n**complete linkage** = maximum ...\n\n**group average** = $\\dfrac{|G1|}{|G2|} \\sum \\sum d_{ij}$\n\n# Problem Sheets\n\n**Sheet 1**\n\nQ1. Proove that $S^2 = \\dfrac{1}{n-1} \\sum(X_i - \\bar{X})^2$ is unbiased estimator of the TRUE variance $\\sigma^{2}$\n\nQ2.\nGiven data $X_{1} .. X{n}$\n\ni) - $Binomial(r,\\theta)$ MLE with known $r$ : $\\dfrac{\\bar{X}}{r}$\n\nii) Mle negative binomial (different representaion) $\\dfrac{r}{r+\\bar{x}}$\n\niii) $\\dfrac{r}{\\bar{x}}$\n\nQ3. \n\ni) $P(MM) = P(MM|I)P(I) + P(MM|NON)P(NON) = \\theta\\2 + (1-\\theta)\\4$\n\nii) $\\dfrac{n_1+n_2-n_3}{n_1+n_2+n_3}$\n\n\nQ4. \n\ni) mgf to prove normality, $var(aX + b) = a^{2}var(X)$\n\nii) ..\n\niii) $\\Phi(c) - \\Phi(-c)$\n\n\nQ5. $N(\\sum_{i=1}^{n}a_{i}\\mu_{i}, \\sum_{i=1}^{n}a_{i}^{2}\\sigma_{i}^{2})$\n\nProof $X_{1}+X_{2}$ is normal using moment generating functions\n\n\n**Sheet 2**\n\nQ1. $\\hat{p} = \\dfrac{1}{\\bar(x)}$\n\nIf you parametrize $p = \\dfrac{1}{\\theta}$ in Geometric distribution and find MLE for $\\theta$ you will have unbiased MLE! The MLE for $p$ is not unbiased (its overestimating)\n\nQ2. maximize likelihood\n\nQ3. $\\hat{p} = \\dfrac{\\sum x_i}{\\sum v_i]$\n\n\nQ5.\n\na) $\\dfrac{n}{143.5-nlog(\\alpha)}$\n\nb)\n\ni) by definition of distribution - just maximize the likelihood\n\nii) integrate the pdf from y to inf\n\niii rewrite the modulus (open it up to use ii) \n\n\n**Sheet 3**\n\nQ1. $ (\\bar{X}-\\dfrac{z_{0.025}}{\\sqrt(n)},\\bar{X}-\\dfrac{z_{-0.025}}{\\sqrt(n)})$  $(2\\times 1.96)^2$\n\nQ2. \n\ni) $p(1-p)$\n\nii) $(\\bar{x}\\pm z_{0,025}\\sqrt{(\\hat{p}(1-\\hat{p}))}/\\sqrt{n})$\n\niii) (0.48,0.76) neshto takova\n\nQ3. use 95\\% CI $(\\bar{x}\\pm \\dfrac{z_{0.025}}{\\sqrt{n}} \\sigma$\n\nwhere $\\sigma = \\sqrt{\\hat{theta}}$ using the MLE of Theta (poisson MLE)\n\nQ4.\n\nMLE is $max(X_i)$, then use $P(Max < x) = P(X<x)^n$\n\nQ5.\n\na) straight\n\nb) mean\n\nc) $L ~ N(\\mu,\\sigma^2)$, $R ~ N(\\mu,\\sigma^2)$ but are dependent! Need to compute the bivariate normal distribution.","n":0.019}}},{"i":86,"$":{"0":{"v":"Probability","n":1},"1":{"v":"Notes on Oxford [lectures]() and solutions/answers to all problems from this [book](). \n\n# Buzzwords\nSample space, events, probability measure. Sampling with or without replacement. Conditional probability = partition of sample space, law of total probability/total expectation, Bayes’ Theorem. Independence.\n\nvariance, covariance, zero covariance does not imply independence, in normal distributions zero covariance = independence\n\nDiscrete radom variablers, pmf = probability mass function, Marginal and conditional distributions, first and second order linear difference equations (fibonacci), random walk, Gambler's ruin.\n\nPoisson interpretation: The number of occurrences during a time interval of length $\\lambda$ is a Poisson-distributed random variable with expected value $\\lambda$.\n\nPo $(\\lambda)$ is approximation of Bin $(n,\\dfrac{\\lambda}{n})$ for large $n$, derive from pmf of binomial.\n\nPoisson limit therem, law of rare events, poisson approximation of binomial (less computationally expensive),\nexponential distribution is the poisson discrete equivalent\n\nnegative binomial is sum of geometric distributions, binomial is sum of bernoulli distributions\n\n$gamma(\\sum_{i}a_i,b)$ is sum of gamma distributions $gamma(a_i,b)$\n\nsum of $n$ exponential distributions $Exp(\\lambda)$ is $Gamma(n,\\lambda)$\n\nprobability generating function, branching processes, solution to probability of extinction $s = G(s)$, random sum formula $G_N(G_X(s)) = G_S(s)$ where $S = X_1 + X_2 + ... + X_n$, $X_i ~ X$ iid and independent from $N$. use it to prove $E(S) = E(N)E(X)$ (alternatively use total law of expectation) . This solves problems with stopping times too.\n\nuse pgfs to compute $P(X mod 2 = 0)$, hint $G(1), G(-1)$. \n\npgf helps to prove sum of independent distribution is another distribution\nuse $G_{X+Y}(s) = G_{X}(s)G_{Y}(s)$ and that pgf defines **uniquely** the distribution\n\nuse pgf's to compute distribution. $P(X = k) = \\text{k-th derivative} G_X(0)$, eval derivatives at 0, need to be easility differentiable\n\ncts random variables, cdf, pdf\n\npdf has similar properties as pmf but is NOT a probability.\n\nsample any cdf from uniform samples. $F_{X}^{-1}(U)$\n\n$E[X] = \\int_{0}^{\\infty}P(X>x)$, need $X$ to be non-negative random variable. Prove that using swap integrands (Tonelli's theorem).\n\nRandom sample, sums of independent random variables. Markov’s inequality, Chebyshev’s inequality, Weak Law of Large Numbers.\n\n[mixture distributions](https://actuarialmodelingtopics.wordpress.com/2017/10/02/examples-of-mixtures/) good for bayesian modelling\n\n - Eulers's formula (Riemann zeta function expressed with prime numbers) has cool probabilistic proof. Check sheet 2 last problem.\n\n$P(\\text{break stick into n pieces and have polygon}) = P(\\text{all pieces are less than } \\dfrac{1}{2}) = 1 - P(\\text{all points lie in one semi circle}) = 1- \\dfrac{n}{2^{n-1}}$\n\n# Chapter 1. Events and probability\n\n Set of all possible outcomes $\\Omega$ is called the *sample* space. S subset of $\\Omega$ is called an *event*. \n\n For events $A$ and $B$ we can do set operations:\n - $A ∪ B$ means at least one of the two occurs\n - $A ∩ B$ means both occur\n - $A - B$ means $A$ occurs but $B$ does not\n\n We assign probabilities $P(A)$ to events.\n\n Counting. Number of permutations of $n$ distinct elements is $n! ~ 2π n^{n+ 0.5} e^{−n}$ \n\nBinomial coefficient ${N\\choose k} = \\frac{N!}{(N-k)! k!}$.\n\nBinomial theorem expands $(x+y)^n = = (x + y)(x + y) · · · (x + y)$. Proof by counting.\n\nBijectionargument in counting problems\n\n**Example**\nHow many distinct non-negative intger-valued solutions of the equation $x_1 +... x_m = n$ are there?\n\n${n+m-1\\choose m-1}$ - use sticks argument\n\n\n**Lemma. Vandermonde’s identity** ${m+n\\choose k} = \\sum_j^k {m\\choose j}{n\\choose k-j}$\n\nUse Breaking things down argument\n\nCountable sets are those which you can label (i.e. map to the integer space), Uncountable sets cannot be labelled like $\\R$\n\n**Definition 1.5**. A probability space is a triple $(Ω, F, P)$ where\n1. $Ω$ is the sample space,\n2. $F$ is a collection of subsets of $Ω$, called **events**, satisfying axioms **F1 –F3** below,\n3. $P$ is a probability measure, which is a function $P : F → [0, 1]$ satisfying axioms **P1 –P3** below.\n\n\n**The axioms of probability**\n\n$F$ is a collection of subsets of Ω, with:\n- F1 : ∅ ∈ $F$. empty event is in $F$\n- F2 : If $A ∈ F$, then also $A^{c} ∈ F$. An event and its complementary are both in $F$\n- F3 : If {Ai , i ∈ I} is a finite or countably infinite collection of members of $F$, then $∪A_i ∈ F$. $F$ has the notion of **closure**.\n\n$P$ is a function from $F$ to $\\R$, with:\n- P1 : For all $A ∈ F, P(A) ≥ 0$.\n- P2 : $P(Ω) = 1$. All events have probability 1.\n- P3 : If {Ai , i ∈ I} is a finite $P$ or countably infinite collection of members of $F$, and $A_i ∩ A_j = ∅$ for $i != j$, then $P(∪A_i ) = \\sum P(A_i) $ **Distributivity** of union over intersection.\n\n**P3** would not be true if it was just for pairwise sets. The above is stronger!\n\n**Theorem 1.9**. Suppose that $(Ω, F, P)$ is a probability space and that $A, B ∈ F$. Then\n1. $P (A^c ) = 1 − P (A)$;\n2. If $A ⊆ B$ then $P(A) ≤ P (B)$.\n\n\n**Definition 1.11.** Let $(Ω, F, P)$ be a probability space. If $A, B ∈ F$ and $P(B) > 0$ then the **conditional probability** of $A$ given $B$ is\n\n$P(A|B) = \\frac{P(A ∩ B)}{P(B)}$\n\nProbability space is a powerful thing. You have all the axioms above to be true!\n\n**Lemma** If $(Ω, F, P)$, then for any event $B$, if you swap $P(A) with Q(A) = P(A|B)$ then $(Ω, F, Q)$ is a probability space too! That is if you condition your probability space on certain event you still have all the axioms.\n\n**Independece** Events $A$ and $B$ are independent if $P(A ∩ B) = P(A)P(B)$.\n\nA family of events is independend if $P(∩ A_i) = \\prod P(A_i)$\n\nPAIRWISE INDEPENDENT DOES NOT IMPLY INDEPENDENCE.\n\n$A$ and $B$ independent imply $A$ and $B^c$ are independent.\n\n\n**Theorem 1.20 (The law of total probability)**. Suppose $\\{B1 , B2 , . . .\\}$ is a partition of $Ω$ by sets from $F$, such that $P(B_i ) > 0$ for all $i ≥ 1$. Then for any $A ∈ F$,\n\n$P(A) = \\sum_{i≥1} P(A|B_i)P(B_i)$.\n\n(partition theorem)\n\n\n**Bayes theorem = Conditional probability + law of total probability**\n\n$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)} = \\dfrac{P(B|A)P(A)}{\\sum P(B|A_i)P(A_i)}$ \n\nSimpson’s paradox\n\n![simpson_paradox.png](assets/images/simpson_paradox.png)\n\n## Problems\n\nSolutions to  **1.11** from the book.\n\nQ1. Condition on first event and do linear differencing equation. Homogeneous and particular solution.\n\n$p_n = 1/6 + 2/3 p_{n-1}$. Can use indeuction too.\n\nNote this is a binomial distribution and we compute probability we have even outcome. Can expend $(1+x)^{n} + (1-n)^n$\n\nQ2. No. Finate spaces should be power of two.\n\nQ3. By induction. Use union operation is associative $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n\nQ4. By Q3 and $P(A_1 ∪ A_2 ∪ . . . ∪ A_n) = 1 − P ((A_1 ∪ A_2 ∪ . . . ∪ A_n)^{c} ) = 1 − P(A_1^{c} ∩ . . . ∩ A_n^{c})$\n\nQ5. Example of pairwise independence (3 events) that does not imply independence of all 3 events $P(A \\cap B \\cap C) \\neq P(A)P(B)P(C)$.\n\nQ6. Conditional probability + Bayes. 79/140, 40/61\n\nQ7. 3 spades sequences/all sequences $= \\dfrac{13.12.11}{13.12.50}$ \n\nQ8. Binomial distribution and expand Stirling.\n\nQ9. Contidional probability + Binomial disribution + Vandermonde’s identity\n\nQ10. Skip physics\n\nQ11. Law of total probability (parititon theorem). They want to get the eight element so can do manually with iteration. I don't see easy way to solve this difference equation by hand?\n\n#TODO Q12. Extra hard, did not solve it. [stack](https://math.stackexchange.com/questions/3277206/prove-0-1234567891011-is-a-normal-number), [normal numbers](https://en.wikipedia.org/wiki/Normal_number), [Champernowne constant](https://en.wikipedia.org/wiki/Champernowne_constant)\n\n\nQ13. Conditional probability + algebra iteration... Goal is to get differencing equation in each variable e.g. $f(c_{n+1},c_n,c_{n-1}) = 0$\n\nQ14. a) Inclusion-exclusion principle.\nb) $e^{-1}, 1-e^{-1}$\n\nQ15. Condition on k cards match. Then use incllusion exclusion principle.\n\nQ16. Conditional probability on when 8:45 and 9:00 trains come. $\\dfrac{e^{-1}}{2}+\\dfrac{e^{-2}}{4}+\\dfrac{e^{-4}}{4}$\n\nQ17. #TODO\n\nQ18 #TODO\n\nQ19. $n=6$\n\n# Chapter 2. Discrete random variables\n\nEncode information about an outcome using numbers\n\nDefinition 2.1. A **discrete random variable** X on a probability space $(Ω, F, P)$ is a function $X : Ω → R$\nsuch that\n\n- (a) $\\{ω ∈ Ω : X(ω) = x\\} ∈ F$ for each $x ∈ R$,\n- (b) $ImX := \\{X(ω) : ω ∈ Ω\\}$ is a **finite** or **countable subset** of $R$.\n\n(a) says $\\{ω ∈ Ω : X(ω) = x\\} = \\{X = x\\}$ lives in $F$, that is it is an event and we can assign probability.\n\n(b) is the definition of discrete.\n\nDefinition 2.3. A probability mass function (pmf) of a random variable is $p_{X}(x) = \\P(X = x)$ s.t.:\n\n- $p_X(x) ≥ 0$ for all $x$,\n- $\\sum p_X(x) = 1$\n\nClassic discrete distributions:\n- Bernoulli\n- Binomial\n- Geometric\n- Poisson\n\n**Definition 2.6** The expectation (or expected value or mean) of $X$ is\n\n\n$E[X] = \\sum_{x∈ImX} xP(X = x)$\nprovided that $\\sum_{x∈ImX}|x|P (X = x) < inf$, otherwise it does not exist. We require **absolutely convergent** series sum.\n\nThe expectation of $X$ is the ‘average’ value which $X$ takes.\n\n**Theorem.** $E[f(X)] = \\sum_{x∈ImX} f(x)P(X = x)$\n\n**Linearity of expectation**:\n- $E[aX + b] = aE[X] + b$\n- $E[X+Y] = E[X] + E[Y]$\n\n**Definition 2.11.** For a discrete random variable $X$, the variance of $X$ is defined by:\n\n$var(X) = E[(X − E[X])^2] = E[X^2] - (E[X])^2$, provided that this quantity exists.\n\nVariance gives size of fluctuations around the expectation.\n\n$var(Y) = var(aX + b) = a^2var(X)$\n\n\n**Theorem 2.14** (Law of total expectation = Partition theorem for expectations).\n\n$E[X] = \\sum_{B} E[X|B]P(B)$\n\n\nJoint pmf is $p_{X,Y}(x,y) = P(X=x,Y=y) = P(\\{X=x\\} \\cap \\{Y=y\\})$\n\nMarginal distribution exists for joint distribution and is just integrating/summing out one of the variables $p_{X}(x) = \\sum_{y} p_{X,Y}(x,y)$\n\n**Theorem 2.23** If $X$ and $Y$ are independent, then $E[XY] = E[X]E[Y]$. Reverse is NOT true.\n\n**Definition** $cov (X,Y) = E[(X − E[X])(Y − E[Y])] = E[XY] - E[X]E[Y]$.\n\n**i.i.d. = independent and identically distributed**\n\n# Problems\n\nSolution from chapter 2.6 in book.\n\nQ1. $f(x) = \\dfrac{x!}{(x-k)!}$\n\nQ2. $\\dfrac{r}{p}$\n\nQ3. Discrete RV with zero variance then, the RV is eqaul to the mean. In cts RV, $X=E[X]$ almost surely (degenerate distribution).\n\nQ4. $\\alpha < -1$ need convergence, $c = 1/\\zeta(-\\alpha)$ Rieman zeta function\n\nQ5. Lack of memory proerty of Geometric distribution. *Kvot bilo bilo*.\n\nQ6. \n1. Proof by visualization.\n2. $3(\\dfrac{2}{3})^{k}$\n3. coupun collection problem, linearity of exp. $3*(1+1/2+1/3)$\n\nQ7. harmonic series $log(n)$\n\nQ8. #TODO\n\nQ9. Expected tosses till see $n$ consecutive heads $\\dfrac{1-p^n}{p^n(1-p)}$. \nWrite it using Markov chain state technique and goal is to compute $e_0$, with boundary condition $e_{H..HH} = e_n = 0$\nTo solve it you need to take difference of consecutive equations and do telescoping sum like idea.\n\nQ10. #TODO\n\n# Chapter 3. Difeerence equations and random walks\n\n**Theorem 3.3** The general solution $(u_n)_{n>=0}$ to a difference equation:\n\n$\\sum_{j=0}^{k} a_{j}u_{n+j} = f(n)$\n\ncan be written as $u_n = v_n + w_n$ where:\n\n- $v_n$ is a particular solution to the equation (resembles $f(n)$)\n- $w_n$ solves the homogeneous equation $\\sum_{j=0}^{k}a_{j}w_{n+j} = 0$\n\nTo solve the homogeneous equation when you have $k>1$ you need to get the auxilary equation - substitute in the homogenous equat $u_n = x^{n}$ and the roots of it gives you the general form of the solution to the homogeneous equation. For $k=2$ it looks like: $w_n = A_1 x_{1}^{n} + A_2 x_{2}^{n}$\n\nIf $x_1=x_2$, then try $w_n = (A + Bn)x_{1}^{n}$\n\nTo solve the particular equation try solutions similar to $f(n)$ if it fails try the next most complicated thing. \n\n**Gamblers ruin:**\n\n$u_n = pu_{n+1} + qu_{n-1}$, where $u_n = P(bankruptcy)$ if gambler has $n$ money. Boundary conditions: $u_0 = 1, u_M = 0$.\n\nThis is second order difference equation. If you remove the boundary $M$ you need to take limits $\\lim_{M->inf}u_n^{(M)} = P(\\text{hit 0 before )M}$.\n\n\n![gamblers_ruin.png](assets/images/gamblers_ruin.png)\n![gamblers_hit0.png](assets/images/gamblers_hit0.png)\n\n$P(ever hit 0)$ is $\\lim_{M->inf}u_n^{(M)}$. To prove that formally you need to use that for an **increasing** set of events $A_1 ⊆ A_2 ⊆ A_3 ⊆ ... ⊆ A_M ...$ you have $P(\\cup A_k) = lim_{k->inf} P(A_k)$. \n\n\nGamblers ruin for expectation $e_n =$ number of steps to hit absorbing stage. \n\n$e_n = p(1+e_{n+1}) + q(1+e_{n-1})$, with bc $e_0 = e_M = 0$\n\n# Problems\n\nSolution to problems from Chapter 10.5\n\nQ1. $P(two iid random walks end in the same position)$ can  be expressed using two expressions. Think about the two walks separately and combined as in one walk.\n\nQ2. Gamblers ruin with different parameters.\n\nQ3. Condition on first step and use gamblers ruin solution for $n = 1,-1$. $\\dfrac{1}{N}$\n\nQ4. Skipped, to do property need to take limit as $M->inf$ and define $a_n^{M}$\n\n#TODO finish questions, might need to read chapter. It is more comprehensive than the lecture notes\n\n\n# Chapter 4 Probability generating functions\n\nThe probability generating function of a discrete random variable is a power series representation of its pmf. It is defined only for discrete random variables.\n\n$G_X(s) = E[s^{X}] = \\sum_{k} s^kP(X = k)$\n\nwhere $s\\in \\R$ such that the expectation exists.\n\n\n**Theorem 4.2**. The distribution of $X$ is uniquely determined by its probability generating function, $G_X$ .\n\nThe $k$th derivative of $G$ evaluated at zero gives $P(X = k)$.\n\n**Theorem 4.3** If $X$ and $Y$ are independent, then $G_{X+Y}(s) = G_X(s)G_Y(s)$\n\n\nUse the above two theorems to prove that sum of $n$ independent Bernoulli random variables is a Binomial random variable.\n\nSum of $n$ iid Poisson rvs is Poisson rv with parameter = sum of parameters.\n\n**Calculate moments** by taking derivative of the pgf and evaluate at $s = 1$\n\n**Theorem 4.8.** Let $X_1 , X_2 , ...$ be i.i.d. non-negative integer-valued random variables with p.g.f. $G_X (s)$.\nLet $N$ be another non-negative integer-valued random variable, independent of $X_1, X_2 ,...$ and with p.g.f. $G_N(s)$. Then the p.g.f. of $\\sum_{i=1}^{N} X_i$ is $G_N(G_X(s))$.\n\nThis chain trick appears in branching processes. Start with one individual which gives birth to many children. In generation $n$ each individual in the population gives children. The pgf of total number of individuals in generation $n+1$ satisfies $G_{n+1}(s) = G(G_n(s))$ Where $G$ is the pgf of random variable giving number of children for 1 individual. By induction you can express $G_{n+1}(s)$ as nested $G(s)$ n times.\n\n\n**Extintion probability.**\n\n$s = G(s)$ is solution to probability of extinction!\n\n$q = \\sum_{k}q^k P(X=k) = G(q)$, where we condition on the number of children of the first individual.\n\nThis equation always have solution at 1. However you cannot solve it for $q$.\n\nYou need to go this way: $q = P(\\cup_{n} \\{X_n = 0\\}) = lim_{n->inf} P(X_n = 0) = lim_{n->inf}G_n(0)$ using the fact about increasing sequences of events.\n\nIt turns out that the question of whether the branching process inevitably dies out is determined by the mean number of children of a single individual. Then we find that there is a positive probability of survival of the process for ever if and only if $µ > 1$. (Theorem 4.15 proves it)\n\n\n# Problems\n\nProblems from Chapter 4.5 from the book\n\nQ1. $P(X = k) = u_{k-1}-u_kS\n\nQ2. $\\dfrac{1}{6^7}({13\\choose 6} - 49)$, need to count number of ways you can sum 7 number and equal to 14 then subtract when any of the numbers is greater than 6 (could be 7 or 8 only, sum 6 numbers is minimum 6).\n\nQ3. Geometric distribution with $p=1/3$. Get pgf of geo + some arithmetic.\n\nprobability of winning are sum of geometric series. Mean durtion of game is 3.\n\nQ4. Sum of $N$ **alternative** geometric distributions. \n\nQ5.\n- a) $G_N(G_X(s))$ where $N ~ \\text{Alternative Geometric}$ and $X ~ Ber(1/2)$, differentiate pgf many times and evaluate at 0\n- b) \n\nQ6. \n- a) $np$, $np(1-p)$\n- b) $\\dfrac{1+(1-2p)^n}{2}$, eval pgf at -1\n- c) evaluate pgf at smart points [trick](https://math.stackexchange.com/questions/458549/probability-that-number-of-heads-flipped-is-divisible-by-3)\n\nIn this case the pgf evaluated at 3rd non trivial roots of unity $w^3 = 1$ is\n$G_{X}(w) = 1 - w^2 P(X \\% 3 = 1) - w P(X \\% 3 =2)$, the pgf is known at $w_1, w_2$, so you can compute the probabilities.\n\n\nQ7. Use $G_{X+Y}(s) = G_{X}(s) G_{Y}(s)$. Then use bayesien theorem to prove the condinitonal probability  is binomial distribution.\n\nQ8. Skipped\n\nQ9. Coupon-collecting problem, geometric distributions, pgf is known.\n\nQ10. derivative of pgf evaluated at 1 is mean - directly from derivative of infinite sum series. for second prove need to use $\\phi(1) = 1$.\n- $\\dfrac{a}{a+b-ab}$ two conditional probabilities from A and B points of view.\n- distribution is something like geometric\n- condition expectation. $e_{A} = (1-b)e_{B} + 1$, similarly for $B$. Answer $\\dfrac{2-a}{a+b-ab}$\n\nQ11. \n- a) $F = \\sum_{i = 1}^{N} I_{i}$ sum of independent Bernoulli. $G_F(s) = G_N(G_I(s))$\n- b) directly from pgf definition, pgf denies distribution uniquely\n- c) $G_N(s) = G_{F+S}(s) = G_{F}(s) G_{S}(s) = G_{N}(\\dfrac{s+1}{2})^2$ last eq come from a).\n\n# Chapter 5. Continuous random variables\n\nRecall that random variable is a function which maps $\\Omega$ the outcome space to $\\R$. Discrete random variables have $Im(X)$ to be countable set.\n\n\n**Definition 5.2.** The cumulative distribution function (**cdf**) of a random variable $X$ is the function $F_X : \\R → [0, 1]$ defined by\n$F_X(x) = P(X \\leq x)$.\n\n$F_X$ is non-decreasing.\n\nCTS random variable:\n\n![cts_rv_defn.png](assets/images/cts_rv_defn.png)\n\n**WARNING: $f_X(x)$ IS NOT A PROBABILITY.**\n\nFor cts random variable $P(X = x) = 0$ for any $x \\in \\R$\n\n![expectation_cts.png](assets/images/expectation_cts.png)\n\nIndependece in cts random variables is defined through the pdf:\n\n![independence_cts.png](assets/images/independence_cts.png)\n\nBut can be defined through cdf too.\n\n\n$var(X + Y) = var(X) + var(Y) + 2cov(X,Y)$\n\n**NB** Standard normal random variables $X$ and $Y$ are independent if and only if their covariance is 0. This is a nice property of normal random variables which\nis not true for general random variables, as we have already observed in the discrete case.\n\n# Problems\n\nAnswers on problems from chapter 5.8 in the book.\n\nQ1. $E(X) = 0$ (substitution + symmetry), $var(X) = 2c^{-2}$ - integration by parts + substitution.\n\nQ2. for $w \\in \\N$ for Poisson $X$ we have $P(X \\geq w) = 1 - \\dfrac{\\Gamma(w,\\lambda)}{\\Gamma(w)}$, [see](https://math.stackexchange.com/questions/467341/question-about-connection-between-poisson-and-gamma-distributions)\n\nThe number of occurrences before time λ is at least w if and only if the waiting time until the wth occurrence is less than λ.\n\nQ3. Skipped\n\nQ4. [trick](https://math.stackexchange.com/questions/1248334/p-d-f-of-the-absolute-value-of-a-normally-distributed-variable)\n$P(|X| \\leq x) = P(X \\leq x) - P(X \\leq -x)$\n\n$E(Y) = \\sqrt{\\dfrac{2}{\\pi}}$\n\n$var(Y) =  1 - \\dfrac{2}{\\pi}$, integrate by parts, use pdf integrates to 1\n\nQ5. shows how to sample uniform distribution\n\nQ6. sample any other distribution from uniform distribution\n\nQ7. Integration question ,need to swap integrands (use Tonelli's theorem)[trick](https://math.stackexchange.com/questions/1690740/prove-that-ex-int-0-infty-pxx-dx-int-0-infty-1-f-xx)\n\nQ8. skipped, basic principles\n\nQ9. use indicator function?\n\nQ10. cdf is $1- e^{-\\dfrac{y+2}{1-y}}$, take derivative to find pdf\n\nQ11. integrate $arctan(x)$ from zero to inf to get the constant, $(2/\\pi)arctan(x)$\n\nQ12. need to define pdf of the angle theta and the distance from center needle to the strips. (can simulate $\\pi$ from this problem.)\n[wiki](https://en.wikipedia.org/wiki/Buffon%27s_needle_problem)\n\nQ13. $P(\\text{break stick into n pieces and have polygon}) = P(\\text{all pieces are less than} \\dfrac{1}{2}) = 1 - P(\\text{all points lie in one semi circle}) = 1- \\dfrac{n}{2^{n-1}}$\n\n\nQ14. cdf is $\\dfrac{y}{3+y}$\n\n# Chapter 6. Random samples and the weak law of large numbers\n\n**Definition 6.1.** Let $X_1,X_2 , ... , X_n$ denote i.i.d. random variables. Then these random variables are\nsaid to constitute a random sample of size $n$ from the distribution.\n\n\nLet $X_1,X_2 , ... , X_n$ denote i.i.d. random variables with mean $\\mu$ and variance $\\sigma^2$. Then $X_n = \\sum_{i=1}^{n}X_i/n$ has $E(X_n)= \\mu$ and $var(X_n) = \\dfrac{\\sigma^2}{n}$.\n\n![wlln.png](assets/images/wlln.png)\n\nTheorem 6.6. [Markov's inequality](https://en.wikipedia.org/wiki/Markov%27s_inequality) gives upper bound for a random variable to be larger than certain threshhold.\n\nTheorem 6.7. [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality) tells you how far from the mean a random variable can deviate.\n\n\n# Distributions\n\n\n![discrete_distributions.png](assets/images/discrete_distributions.png)\n\n![cts_distributions.png](assets/images/cts_distributions.png)\n\n\n# Problem sheets answers\n\n**Sheet 1**\n\nQ1. $\\dfrac{11!}{2!}, 2\\dfrac{10!}{2!}, \\dfrac{6!}{2!}$\n\nQ2. $(100!)^{2}$\n\nQ3. $\\dfrac{1}{6^9}$\n\nQ4. ${k\\choose r}$, then condition on largest element.\n\nQ5. basic set theory holds for probability of events\n\nQ6. a) $P(B \\cup C \\cap A^{c})$. b)141\n\nQ7. a) 23, $\\dfrac{365...(365-n+1)}{365^n}$\nb) $1 - (\\dfrac{364}{365})^{n}$\n\nQ8. Inclusion exclusion principle\n$P(\\text{no coorect hook}) = P(\\cup_{i=1}^{n} A_{i}) = 1 - \\dfrac{1}{2!} + \\dfrac{1}{3!} ...$\n\nnumber of arrangements of no correct hook = \n$n! \\times P(\\text{no correct hooks})$\n\nd) $P(\\text{exactly r correct hooks}) = \\dfrac{\\text{arrangements with exacly r correct}}{\\text{all arrangements}} = \\dfrac{{n\\choose r}(n-r)!\\sum_{k}^{n-r}\\frac{(-1)^k}{k}}{n!}$\n\nRemark. probability no correct hooks converges to $1-e^{-1}$\n\n**Sheet 2**\nQ1. Example that pairwice independence does not imply independence\n\nQ2. $\\dfrac{95}{95+2 \\times 99.5} ~ 32%$\n\nQ3. \n- a) $\\dfrac{m}{m+n}$\n- b) $\\dfrac{m}{m+n}\\dfrac{m-1}{(m+n-1)} + \\dfrac{n}{m+n}\\dfrac{m}{m+n-1}$\n- c) Bayes from a) and b)\n\nQ4 Use Bayes. Clarification: $p$ proportion are conservative, $1-p$\nare liberal.\n\nQ5. \n- a) ${26\\choose 13}\\dfrac{1}{2^{26}}$\n\n- b) ${26\\choose 13}^2$ $/$ ${52\\choose 26}$\n\nSecond is bigger. Intution is that in a) you reset every time, and in b) you are pulled towards balance.\n\nQ6. Euler's formula for Riemann's zeta function.\n- a) $\\dfrac{1}{k^{s}}$\n- b) directly from definition $P(AB) = P(A)P(B)$, but for infinite number of events (pairwise dependence is not enough) \n- c) if an integer is not divisibel by any prime number it muist be 1!\n\n**Sheet 3**\n\nQ1. $np$\n\nQ2. proof by visualization\n\nQ3. \n- a) $(1-p)^k$\n- b) memoryless propery of geometric distribution\n\nQ4. Complete th exponent and use sum of pmf is 1.\n\nQ5. \n- a) for large $n$ binomial becomes poisson\n- b) $e^{-\\lambda}$\n\nQ6. 6\n\nQ7. Coupon collector problem.\n- a) $\\dfrac{n-1}{n}$, geometric($\\dfrac{n-1}{n}$)\n\n- b) Geo($\\dfrac{n-k+1}{n}$)\n\n- c) harmonic series\n\n**Sheet 4**\n\nQ1. $\\dfrac{n+1}{2}, \\dfrac{(n+1)(4n+1)}{12}$\n\nQ2. zero covariance does not imply independence\n\nQ3. \n- a) independent so can just multiply\n- b) sum of poisson independen is poisson (derive from pgf)\n- c) binomial with $p = \\dfrac{\\lambda}{\\lambda + \\mu}$\n- use c\n\nQ4.\n- bayes, in numerator $P(X=k,Y=n+1-k)$\n- b) $(1-p)^{2k}(2p-p^2)$, use $p_k = p_{\\ge k} - p_{\\ge k+\n\nQ5. \n- $n\\times e^{-\\lambda}$\n- $P(Bin(m,p) = k)$, sum of binomial times poisson which turns out to be $Po(\\lambda p)$\n\n\nQ6. from axioms?\n\nQ7. first and second order difference equations. solvable using particular and homogeneous solutions. For particular solutions - strategy try next most complex thing. \n\n\n**Sheet 5**\n\nQ1. $2q_n+p_n = 1, q_n = \\dfrac{p_{n-1}+q_{n-1}}{2}$ $\\dfrac{1}{3}$ first order recurence equation.\n\nQ2. 19\n\nQ3. \n- a) $n$\n- b) $\\dfrac{n+1}{2n}$\n- c) $\\dfrac{M}{2}$, $e_n = n(M-n)$\n\nQ4.\n- a) sum of geometric series\n- b) Take derivatives and evaluate at 1 $E(X) = p'_{X}(1) = \\dfrac{1}{p}$.\n$var(X) = \\dfrac{1-p}{p^2}$\n\nQ5.\n- a) condition on first two throws, second order recurence.\n- b) last 3 tosses are fixed, $\\dfrac{1}{8}r_{n-3}$\n- c) take derivative of probability generating function\n- d) hmmmm #TODO, maybe need two probabilities $p_n, q_n$ and use trick like in Q1?\n\nQ6. \n- a) $\\dfrac{N(N-1)}{2}$, need to use $e_1$ from Q3.\n- b) always need to go the whole way around, by symmetry the answer is $\\dfrac{1}{N-1}$\n\n**Sheet 6**\n\nQ1.\n- a) $s^a$\n- b) $s^n p_{Y}(s^m)$\n\nQ2.\n- a) negative binomial - number of trials up to and including the $k$-th success.\n- b) negative binomial is sum of geometric distributions! $p_{Geo}(s)^m = (\\dfrac{ps}{1-(1-p)s})^m$\n\nQ3.\n- a) law of total expectation, or use Theorem 4.8 $G_Z(s)=G_N(G_X(s))$ and play around with pgf derivatives evaluated at 1.\n- b) $\\lambda p$\n- c) no, $E[Z]$ is not longer $E[N]E[X_1]$, your proof in a) using law of total expecation uses the independence\n\nQ4. $\\dfrac{G_X(-1)+1}{2}$\n\nQ5. $G(s) = 1/12 + 2s/3 + s^2/4$, $G(G(s))$, $P(extinction after 2 minutes)=G(G(0))$ \n\n\nQ6.\n- a) $2p$, $G(s) = ps^2+1-p$\n- b) $\\dfrac{1-p}{p}$\n- c) #TODO, maybe need to revisit notes, or look Miro's solution\n\n**Sheet 7**\n\nQ1. pictures of distributions\n\nQ2. check if the integral of these functions exists\n\nQ3.\n- a) $\\dfrac{1}{2}$, $\\dfrac{1}{12}$\n- b) $\\dfrac{a}{b}$\n\nQ4.\n- a) $e^{-\\lambda x}$\n- b) $e^{-\\lambda a} - e^{-\\lambda b}$\n- c) Bayes theorem + a) memoryless property of exponential distribution\n- d) take arcsin and use that $X$ is exponential\n- e) exponential, with lower rate\n- f) **exponential distribution is the continuous version of the geometric distribution**, capped exp is geometric. [proof](https://math.stackexchange.com/questions/2087662/proving-that-the-discrete-exponential-distribution-is-geometric-distribution)\n\nQ5.\nsubstract mean, divide by std to get standard normal\n- a) 0.454\n- b) 0.921 - 0.454\n- c) $0.921^20 + 20 \\times 0.921^19 \\times(1-0.921)$\n\nQ6.\n- dont need pdf to get expecation and variance [see](https://math.stackexchange.com/questions/2473968/radius-of-a-circle-w-uniform-distribution)\n- to get pdf just take derivate of cdf, $\\sqrt\\dfrac{x}{\\pi}$\n\nQ7.\n- a) $F_X(X)$ is uniform distribution, just go through definition $F_X(F_{X}^{-1}(x)) = x$\n- b) $F_X(x)$ - gi>ves a way to generate $X$ from uniform distribution $U$\n- c) thats exponential distribution, get the cdf, find its inverse and apply b)\n\n**Sheet 8**\n\nQ1. Integrate, choose constants which would make the integrals equal to 1.\nfor independence check with marginal distributions $f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y)$\n\nQ2\n- a) $P(min(X_i)>t) = \\prod_{i}P(X_i>t) ~ Exp(\\sum_{i}\\lambda_{i})$\n- b) treat using indicator variable $\\sum_{i}P(T_i > 1) = \\sum_{i}e^{-\\lambda_{i}}$\n- c) $P(M < median) = 0.5$\n\nQ3. integrate $x^2$ from 0 to 1. Area under the shaded region..\n\nQ4. chebyshev\n\nQ5. chebyshev\n\nQ6. \n- a) $\\dfrac{1}{4}$, $\\dfrac{3}{16}$\n- b) 0 when $|i-j| > 1$, $\\dfrac{1}{16}$ otherwise\n- c) use b, $\\dfrac{n}{4}$,  $\\dfrac{7n}{16}$\n- d)  $\\dfrac{n}{4}$,  $\\dfrac{3n}{16}$\n\nQ7 .WLLN","n":0.016}}},{"i":87,"$":{"0":{"v":"Linear Algebra II","n":0.577}}},{"i":88,"$":{"0":{"v":"Linear Algebra","n":0.707},"1":{"v":"\n# Mаtrices\n\nMatrix Multiplication, Inverse , Orthogonal Matrix, Gaussian Elimination\n\n- tidy way to represent linear equations\n- coefficients of the variables are the entries of the matrix\n- matrix properties defines if and how many solutions the linear system has\n- Matrix Multiplication is not commutative! It is $O(m*n*p)$ cost to multiply an $m x n$ matrix with an $n x p$ matrix.\n- Set of square matrices under addition/multiplication is a Ring and a Group but is not Abelian (i.e. not commutative)\n- Only square matrices can be inverted, rectangular matrices cannot be inverted\n- For rectangular matrices we have the pseudo-inverse (the product is almost the same as the identity matrix)\n- The inverse of a matrix is unique, if it exists\n- A matrix is orthogonal if the inverse is equal to the transpose. $A*A^{T} = I = A^{T}*A$\n- In orthogonal matrix, multiplying each tw rows (or columns) has dot produc zero (perpendicular). Multiplying the same vector gives the unit vector.\n- Multiplying by orthogonal $A$ rotates and/or reflects vectors, but doesn’t change their lengths or angles between them.\n- Gaussian elimination is the method you use to solve linear equations systematically. 1. Swap equation, to find the leftmost non-zero entry.\n2. Scale the equation, so that the leftmost non-zero entry is 1. (divide by the coefficient)\n3. Subtract appropriate multiples of the first equation from all other equations to eliminate the leftmost non-zero entry in those equations.\n4. Now only one row has a non-zero entry in the first column. We will able to tell teh value of the first variable once we know the values of all other variables.\n5. You will get a triangular matrix, where the first row has a non-zero entry in the first column, the second row has a non-zero entry in the second column, and so on.\n-  A system of linear equations $Ax = b$. The augmented matrix is $A|b$.\n- EROs (elementary row operations) on the augmented matrix $A|b$ are:\n  1. Swap two rows\n  2. Scale a row by a non-zero constant\n  3. Add a multiple of one row to another row\n- Each ERO is invertible and does not change the set of solutions\n- Echelon form of a matrix is triangular, where the first non-zero entry in each row is 1, and all entries below it are 0. In Gaussian elimination, you bring a matrix using EROs to echelon form.\n- **Theorem** An invertible $n x n$ matrix can be reduced to $I_{n}$ using EROs.\n- **Theorem** I $A$ is invertible, then $Ax = b$ has a unique solution for every $b$. The solution is $A^{-1}b$\n\n\n# Calculating the inverse\n\n-- To calculate he inverse of a **square** matrix A, augment the identity and row reduce the augmented matrix to echelon form. If the echelon form is the identity, then the inverse is the other half of the augmented matrix.\n- Idea is each EROs, can be represented by multiplying by an **elementary** matrix $E_{i}$.\n- $E_k...E_2E_1A = I$\n- so the product of the elementary matrices is the inverse of $A$.\n- applying  EROs to identity records their combined effect\n- $O(n^3)$","n":0.045}}},{"i":89,"$":{"0":{"v":"PartC","n":1}}},{"i":90,"$":{"0":{"v":"PartB","n":1}}},{"i":91,"$":{"0":{"v":"PartA","n":1}}},{"i":92,"$":{"0":{"v":"Probability","n":1},"1":{"v":"\nOxford notes for Part A probability.\n\n# Buzzwords\n\nProbability space $(\\Theta, F, P)$ = modelling an experiment. $\\Theta$ is the space of outcomes, $F$ is the space of events, $P$ gives a measure of probability.\n\n\nMemoryless property of geometric and exponential distributions = kvot bilo bilo.\n\nclose connections between the Poisson distribution and the exponential distribution - think Poisson processes.\n\n$X_n \\rightarrow X$ almost surely (with probability 1) implies convergence in probability implies convergence in distribution.\n\nWLLN\n\nWeak law of large numbers is limit in probability. Proven using Chebyshev's inequality (requires finite variance)\n\nThis assumption of finite variance is not necessary (just makes proves easier). Large or infinite variance will make the convergence slower, but the LLN holds anyway.\n\nStrong Law of Large Numbers (SLLN) is almost sure convergence.\n\nCLT, converges in distribution. The fluctuations of the mean $X_n$ around $\\mu$ is of order $1/\\sqrt{n}$\n\nrisk pooling (used in CLT) $S_n = X_1 + ... X_n$, where $X_1 ... X_n$ are different portfolios (iid assumption!). \n\nBinomial with small probability and many events is poisson\n\nBinomial = sum of Beornoulli. If fixed $p$, then by CLT Binomial converges to Normal distribution. Consider $W_n ~ Bin(n,p_n)$, where $p_n$ converges to 0. Assume the expected total number of successes stays approximately the same $np_n = \\lambda$. Then Binomial converges to Poisson. \n\n\ngenerating functions are used to compute moments and pmfs. Take kth derivative and evaluate at 0 or 1.\n\nPgf-s used for discrete random variables $E(z^{X})$.\n\nMgf-s used for continuous random variables $E(e^{tX})$.\n\nMgfs and Pgfs define uniquely the distribution of random variables.\n\nFor $M_{X_1},M_{X_2} ... M_{X_n}$ if $M_{X_n} \\rightarrow M_{X}$ then $X_{n} \\rightarrow X$ in distribution as $n \\rightarrow \\infty$.\n\n\nProof WLLN CLT using mgfs and above fact\n\nChebyshev’s inequality, which is the application of Markov’s inequality to the random variable $(X − \\mu)^2$.\n\nMarkovs inequality is about bouding the tails of non-negative distributions.\n$P(X>a) \\leq \\dfrac{E(X)}{a}$\n\n\nFact: For standard normal random variable $P(X > 3) \\approx 10^{-3}$\n\nMarkov chain is a sequence of variables satisfying the Markov property\n\n$P(X_{n+1} = i_{n+1} | X_n = i_n ,..., X_0 = i_0) = P(X_{n+1} = i_{n+1} | X_n = i_n)$.\n\n\nTo describe a time homogeneous Markov chain you need the initial distribution of $X_0$ and a **transition matrix** $P = (p_{ij})_{i,j \\in I}$\n\nThe matrix $P$ is indexed by the state space $I$. $P$ has non-negative entries and each row sums to 1.\n\nMarkov chains are memoryless.\n\nChapman-Kolmogorov equations\n\nirreducible chains, communicating classes, closed class, periodicity of classes\n\nrecurrence, transience, mean return time, null recurrence, positive recurrence\n\nnull recurrent = mean return time is infinite but probability of goiing back infinitely many times is 1.\n\nin a class all states are either positive recurrent or null recurrent, or transient\n\nRandom walk on $\\Z^{d}$ is a irreducible chain with period 2.\n\niid is subcase of Markov chain\n\nErgodic theorem is generalization of SLLN\n\n# Chapter 1. Recap Prelims\n\n\n- (i) $Ω$ is a set, which we call the sample space.\n- (ii) $F$ is a collection of subsets of $Ω$. An element of $F$ is called an event.\n- (iii) The probability measure $P$ is a function from $F$ to [0, 1]. It assigns a probability to each event in $F$.\n\nA **random variable** is a function defined on $Ω$. Maps events to numbers.\n\ncdf, pdf, discrete and cts random variables, variance, covariance\n\nindependence, $P(A \\cup B \\cup C) = P(A)P(B)P(C)$. pairwise indepence is weaker than independence.\n\n\nIf $X ∼ Gamma(rX , λ)$ and $Y ∼ Gamma(rY , λ)$ are independent, then we have $X + Y ∼ Gamma(rX + rY , λ)$. As a special case, if $X_1 , X_2 , . . . , X_n$ are i.i.d. with $Exp(λ)$ distribution,\nthen $X_1 + X_2 + · · · + X_n$ has $Gamma(n, λ)$ distribution.\n\nMemoryless property of geometric and exponential distributions.\n\n\nIf $X ∼ Poisson(λ)$ and $Y ∼ Poisson(µ)$ are independent, then $X + Y ∼ Poisson(λ + µ).$\n\n\n\n# Chapter 2. Convergence of random variables and limit theorems.\n\nWe need to formalise the notion that two random variables $X$ and $Y$ are close.\n\n**Modes of convergence**\n\nLet $X_1, ... X_n, X$ be random variables. We say:\n\n$X_n$ converges to $X$ **almost surely (with probability 1)** if\n$P(X_n \\rightarrow X \\text{ as } n \\rightarrow \\infty) = 1$\n\n$X_n$ converges in **probability** to $X$ if for every $\\epsilon > 0$: $P(|X_n - X| < \\epsilon) \\rightarrow 1 \\text{ as } n \\rightarrow \\infty$.\n\n$X_n$ converges to $X$ in **distribution**(weakly) if $F_n(x) \\rightarrow F(x)$ as $n \\rightarrow \\infty$ for all $x$ where $F(x)$ is continuous.\n\nThese formulations are in decresing order of strength. The last one does not require distributions to be the same! This is really a definition about convergence of distributions, not about convergence of random variables.\n\n**There are many situations in which a sequence of discrete random variables converges to a continuous limit.**\n\nWLLN in convergence in probability.\n\n$S_n  = X_1 + ... X_n$, where $X_1, ... X_n$ are iid with finite variance. Then:\n\n$P(|\\dfrac{S_n}{n}  - \\mu| < \\epsilon) \\rightarrow 1$ as $n \\rightarrow \\infty$\nproove it using Chebyshevs inequality.\n\nCLT - convergence in distribution (standard normal). The fluctuations of $S_n = X_1 + ... + X_n$ around $n\\mu$ is of order $\\sqrt{n}$.\n\nBinomial with small probability and many events is poisson\n\nBinomial = sum of Beornoulli. If fixed $p$, then by CLT Binomial converges to Normal distribution. Consider $W_n ~ Bin(n,p_n)$, where $p_n$ converges to 0. Assume the expected total number of successes stays approximately the same $np_n = \\lambda$. Then Binomial converges to Poisson. \n\n# Chapter 3. Generating functions\n\nThe generating function of random variable $X$ is $G(z) = E(z^{X})$\n\nUsing generating dunctions you can compute moments and the pmf of $X$. Take kth derivative and evaluate at 0 or 1.\n\n**Theorem 3.2** (Uniqueness theorem for probability generating functions). If X and Y have the same generating function, then they have the same distribution.\n\n\nGenerating functions for independen rvs: $G_{X+Y}(z) = G_{X}(z)G_{Y}(z)$\n\nGenerating functions used for random sums: $G_{S}(z) = G_{N}(G_{X}(z))$\n\n**Probability genrating functions** are used for discrete random variables!\n\n\n**Moment generating function** of a random variable is $M_X{t} = E(e^{tX})$\n\nMgf is Pgf with  $z = e^t$\n\nWe use $e^{t}$ because we can expand around $t=0$. If we expand around $z=0$ we would no longer have power series in the general case for cts variables.\n\nAssuming the mgf exists (the expecation is finite), Taylor expansion:\n\n$M_{X}(t) = \\sum_{k=0}^{\\infty} \\dfrac{t^{k}E(X^k)}{k!}$\n\n$M_{X}^{(k)}(0) = E(X^{k})$\n\n\nMgfs and Pgfs define uniquely the distribution of random variables.\n\nFor $M_{X_1},M_{X_2} ... M_{X_n}$ if $M_{X_n} \\rightarrow M_{X}$ then $X_{n} \\rightarrow X$ in distribution as $n \\rightarrow \\infty$.\n\n\nFact For standard normal random variable $P(X > 3) \\approx 10^{-3}$.\n\n**Characteristic functions**\n\nReplace in mgf $t$ with $it$\n\n$\\phi_{X}(t) = E(e^{itX}) = E(cos(tx)) + iE(sin(tX))$.\n\nResults for mgf hold for cgf too.\n\n**Mgf vs Cgf**\n\nCgf does not require exponentially decaying tails (it is all on complex plain). Mgf to proove CLT uses exponential decay (finite expectation).\n\nMgf could be used to show bounds of tails (using Markov inequality, see section 3.3). CGf just does not makes sense as we use complex numbers and cannot compare them.\n\n# Chapter 4. Joint distributions of continuous random variables\n\n\n\n# Chapter 5. Markov chains\n\nLet $X = (X_0 , X_1 , X2 , ...)$ be a sequence of random variables taking values in some state space $I$. The process $X$ is called a Markov chain if for any $n \\geq 0$ and any $i_0 , i_1 , . . . , i_{n+1} \\in I$,\n\n$P(X_{n+1} = i_{n+1} | X_n = i_n ,..., X_0 = i_0) = P(X_{n+1} = i_{n+1} | X_n = i_n)$.\n\nThe Markov chain is called **(time) homogeneous** if in addition $P(X_{n+1} = j| X_{n} = i)$ does not depend on $n$. Then $p_{ij} = P(X_{n+1} = j| X_{n} = i)$ and these are called **transition** probabilities of the chain.\n\nTo describe a time homogeneous Markov chain you need the initial distribution of $X_0$ and a **transition matrix** $P = (p_{ij})_{i,j \\in I}$\n\nThe matrix $P$ is indexed by the state space $I$. $P$ has non-negative entries and each row sums to 1.\n\nMarkov chains are memoryless.\n\nWe can say that “the future is independent of the past, given the present”.\n\n**Theorem 5.2. (Chapman-Kolmogorov equations)**\n\n- To reach from $i$ to $k$ in $m+n$ steps try middle: $p_{ik}^{m+n} = p_{ij}^{m} p_{jk}^{n}$\n- To reach from $i$ to $j$ in $n$ steps: $p_{ij}^{n} = (P^n)_{i,j}$ \n\n**Theorem 5.3** If $\\lambda$ is the initial distribution, then the distribution of $X_n$ is $\\lambda P^{n}$.\n\n\n**Class structure** of state space of Markov chains.\n\nWe say state $i$ communicates with state $j$ if we can reach from $i \\rightarrow j$, that is $p_{ij}^{n}$ is positive for some $n$ (after certain number of steps we can reach from $i$ to $j$) and vice verca.\n\nA class of states $C$ is **closed** if the probability to go out of the class is 0.\n\nIf the class $\\{i\\}$ is closed then we call it an **absorbing state**.\n\nA state $I$ is **irreducible** if all states communicate (can reach from elsewhere to everywhere).\n\n**Period** of a state $i$ is the least number of steps after which you will be back and back and back ... in $i$, that is the gcd of the set $\\{ n \\geq 1: p_{ii}^{n}\\}$, if it does not exist (never go back, p_{ii}^{n} = 0)then the period is not defined.\n\n$i$ is called **aperiodic** if this g.c.d. is 1. Example: go back in 1, 3, 7 steps\n\n**Fact. All states in a communicating class have the same period.**\nIn particular, if a chain is irreducible, then all states have the same period.\n\n**Hitting probabilities**\n\nDefine $h_{i}^{A} = P_{i}(X_{n} \\in A \\texttt{for some} n \\geq 0)$, the hitting\nprobability of $A$ starting from state $i$. If $A$ is a closed class we call that the **absorbtion probability**.\n\n**Recurrence and transience**\n\n$P_{i}(X_{n} = i \\texttt{for some } n \\geq 1) = p \\leq 1$ equivalently\n$P_{i}(\\texttt{git } i \\texttt{ infinitely often}) = 0$. The state $i$ is called transient.\n\n$P_{i}(X_{n} = i \\texttt{for some } n \\geq 1) = 1$ equivalently\n$P_{i}(\\texttt{git } i \\texttt{ infinitely often}) = 1$. The state $i$ is called recurrent.\n\n**Theorem 5.9** In a recurrent class: either all states are recurrent or all are transient.\n\nEvery recurrent class is closed. Every finite closed class is recurrent.\n\n\nThe theorem tells us that recurrence and transience are quite boring for finite chains: state $i$ is recurrent if and only if its communicating class is closed. But infinite chains are more interesting! An infinite closed class may be either transient or recurrent.\n\n**Therem** State $i$ is recurrent iff $\\sum_{n=0}^{\\infty}p_{ii}^{(n)} = \\infty$\n\n**Random walk in $\\Z^{d}$**\n\nConsider a simple symmetric random walk on the $d$-dimensional integer lattice. This is a Markov chain with state space $\\Z^{d}$ and transition probabilities $p_xy = 1/(2d)$ if $|x − y| = 1$, and $p_xy = 0$ otherwise. The chain is irreducible, with period 2.\n\nFor $d=1,2$ the chain is recurrent (probability of go back to 0 infinitely often is 1), and for $d \\geq 3$ the cahin is transient.\n\n**Mean Return Time to a state**\n\n$m_{i} = E(\\texttt{start from }i\\textt{ and go back to }i) = 1 + \\sum p_{ij}k_{j}^{i}$\n\nwhere $k_{j}^{i}$ is the mean hitting time of $i$ starting from $j$\n\nIf $i$ is transient then $m_i = \\infty$ (return time to itsleft is infinite with positive with probability).\n\nIf $i$ is recurrent and $m_i = \\infty$, we say $i$ is **null recurrent**\n\nIf $i$ is recurrent and $m_i < \\infty$, we say $i$ is **positive recurrent**\n\nIf the chain is irreducible, we can therefore call the whole chain either transient, or null recurrent, or positive recurrent.\n\n\n# Chapter 6. Markov chains: stationary distributions and convergence to equilibrium\n\nLet $\\pi = (\\pi_{i} , i \\in I)$ be a distribution on the state space $I$.\nWe say that $\\pi$ is a **stationary distribution**, or invariant distribution, or equilibrium distribution, for the transition matrix $P$ if $\\pi P  = \\pi$\n\n$\\pi$ is a **left** eigenvector of the matrix $P$ with eigenvalue 1.\n\nThat is for all $j$ $\\pi_{j} = \\sum_{i}\\pi_{i} p_{ij}$\nzzz\nStationary distributions are those for which after we move using transition matrix, the distribution does not change.\n\n\n**Theorem 6.1 (Existence and uniqueness of stationary distributions)**. Let $P$ be an irreducible transition matrix.\n- (a) $P$ has a stationary distribution if and only if $P$ is positive recurrent.\n- (b) In that case, the stationary distribution π is unique, and is given by $\\pi_{i} = 1/m_{i}$ for all $i$ (where $m_{i}$ is the mean return time to state $i$ defined at (5.12)).\n\n**Theorem 6.2 (Convergence to equilibrium)**. Suppose $P$ is irreducible and aperiodic, with stationary distribution $\\pi$. For a Markov chain with transition matrix $P$ and any initial distribution $P(X_n = j) \\rightarrow \\pi_{j}$ as $n \\rightarrow \\infty$ for all $j$.\n\n**Theorem 6.3 (Ergodic theorem)**. Let $P$ be irreducible. Let $V_i(n)$ be the number of visits to state $i$ before time $n$.\n\nThen $\\dfrac{V_i(n)}{n} \\rightarrow \\dfrac{1}{m_i}$ almost surely as $n \\rightarrow \\infty$.\n\nThe ergodic theorem concerns the “long-run proportion of time” spent in a state.\n\nIn the positive recurrent case, the theorem says the long-run proportion of time   $\\dfrac{V_i(n)}{n}$ spent in a state $i$ is the stationary probability of that state $\\dfrac{1}{m_i} = \\pi_{i}$.\n\n\nIn the null-recurrent or transient case, $1/m_{i} = 0$, so the ergodic theorem says that with probability 1 the long-run proportion of time spent in a state is 0.\nzz\nWe can see the ergodic theorem as a generalisation of the strong law of large numbers.\nThe ergodic theorem can be seen as extending this to the case where Xn is not i.i.d. but is a Markov chain. IID is stronger asumption that Markov propery.\n\n\n**Intuition** about the convergence theorems.\n\n\nThe idea will be that after a long time, a Markov chain should more or less “forget where it started”. There are essentially two reasons why this might not happen: \n\n- (a) periodicity; for example if a chain has period 2, then it alternates between, say, “odd” and “even” states; even an arbitrarily long time, the chain will still remember whether it started at an “odd” or “even” state. \n\n- (b) lack of irreducibility. A chain with more than one closed class can never\nmove from one to the other, and so again will retain some memory of where it started, forever\n\nThus for convergence to equillibrium (which does not depend on initial distribution) we require aperiodicity and irreducibility\n\n\n# Poisson processes\n\nA Poisson process is a natural model for a stream of events occuring one by one in continuous time, in an uncoordinated way.\n\nExample:\n- the process of times of detections by a Geiger counter near a radioactive source (a very accurate model)\n- the process of times of arrivals of\ncalls at a call centre (often a good model)\n- the process of times of arrivals of buses at a bus\nstop (probably an inaccurate model; different buses are not really uncoordinated, for various\nreasons\n\n**Definition** A *counting process* is a random process $N_t$ for $t \\in [0,\\infty]$, where time $t$ is continuous and $N_t$ is a random variable which takes values in $\\{1,2,3...\\}$ and $N_s < N_t$ for $s < t$.\n\n**Arrival process**\n\nIf $N_t$ describes an arrival process, then $N_t = k$ means that there hae been $k$ arrivals in the time interval $[0,t]$.\n\nIn fact we can describe the process by the sequence of arrival times, which we might call “points” of the process. Let $T_k = inf\\{t ≥ 0 : N_t ≥ k\\}$ for $k ≥ 0$. inf = infimum!\n\n$T_0 = 0$, $T_k$ is the *k-th arrival time*.\n\nAlso $Y_k = T_k - T_{k-1}$ $k \\geq 1$ and $Y_k$ is the **interarrival** time between $k-1$ and $k$.\n\n\nFor $s < t$, we write $N(s, t]$ for $N_t − N_s$ and we call this the increment process $N$ on the interval $(s,t]$ = number of arravals between $s$ and $t$.\n\n\n**Definition 7.1 (Definition of Poisson process via exponential interarrival times).** $(Nt , t ≥ 0)$ is a Poisson process of rate $\\lambda$ if its interarrival times $Y_1 , Y_2 , Y_3,...$ are i.i.d. with $Exp(\\lambda)$ distribution.\n\n**Definition 7.2 (Definition of Poisson process via Poisson distribution of increments).** $(Nt , t ≥ 0)$ is a Poisson process of rate $\\lambda$ if:\n- N_0 = 0\n- If $(s_1,t_1), ... (s_k,t_k)$ are disjoint intervals $\\R_{+}$, then the increments $N(s_i,t_i]$ are independent. The number of points falling in disjoint intervals is independent.\n- For any $s < t$, the increment $N(s,t]$ has Poisson distribution $\\lambda(t-s)$\n\nThe two definitions are equivalent. The key idea is that the memoryless property for the exponential distribution and the independent increments property are telling us the same thing.\n\n\nTo get more intuition for the relation between Poisson increments and exponential interarrivals, one can also think about a related discrete-time process.\n\n- (1) If $X_n ∼ Binomial(n, λ/n)$, then $X_n → Poisson(λ)$ as $n \\rightarrow \\infty$. (See Example 2.9.)\n- (2) If $Y_n ∼ Geometric(λ/n)$, then $Y_n/n → Exp(λ)$ as $n \\rightarrow \\infty$. (See Example 2.3.)\n\nSo we can see this exponential/Poisson relationship in the Pois-\nson process as a limit of the geometric/binomial relationship which is already familiar from sequences of independent trials\n\n\n**Theorem 7.1 (Superposition of Poisson processes)**. Let $L_t and $M_t$ be independent Poisson processes of rate $λ$ and $µ$ respectively. Let $N_t = L_t + M_t$. Then Nt is a Poisson process of rate $λ + µ$.\n\n\n**Theorem 7.2 (Thinning of a Poisson process).** Let $N_t$ be a Poisson process of rate $λ$. “Mark” each point of the process with probability $p$, independently for different points. Let $M_t$ be the counting process of the marked points. Then $M_t$ is a Poisson process of rate $pλ$.\n\n\n# Problem sheets\n\nProblem sheet 3.\n\nQ1. closed classes: $\\{3\\},\\{1,5\\}$, $\\{1,2,5\\}$ (1-indexd)\nclosed classes are always communicating classes.\n\n$(1/5,1/10,7/20,1/5,3/20)$, for second pard need chapman kolmogorv $9/16$\nraise $P^2$\n\nQ2. $p_{ii} = 1 - (\\dfrac{N-i}{N})^2 - (\\dfrac{i}{N})^2, p_{i,i-1} = (\\dfrac{i}{N})^2, p_{i,i+1} = (\\dfrac{N-i}{N})^2$\n\nQ3. states  6 and not 6. need to get eigenvalue decomposition and raise the transition matrix to power n. Or condition of last state $p_n = p_{n-1} \\dfrac{4}{5} \\dfrac{1}{5}$\n\n$P(\\texttt{return} 1 \\texttt{after n steps})$ = $\\dfrac{1}{5}(1-P(\\texttt{return to} 6)$\n\nQ5. basic logic\n\nQ6.\n- a) write transition matrix\n- b) write all equations $e_8 = e_{10}/2+e_{6}/2+1$ etc., answer is 2, alternatively consider state one $\\{0,10\\}$ and state two $\\{2,4,6,8\\}$. Currently we are in state two. 0.5 probability to go from state two to one and 0.5 to stay in same state. By geometrix distribution you get expectation 2.\n- c) write all equations (conditioning) $p_8 = p_{10}/2 + p_{6}/2$ etc.\n- d) use c) + bayes equality, for later part need to compute probability win given we reach 10 starting from 2,4,6.\n\nQ7.\n- a) $p_{i}+q_{i} = 1$ and condition of first step\n- b) $h_{i} = 1 - u_{1}\\sum_{k=1}^{i}\\gamma_{k}$\n- c) the maximal solution (look at theorem)\n\nQ9. uniform distribution by symmetry","n":0.018}}},{"i":93,"$":{"0":{"v":"Probability puzzles","n":0.707},"1":{"v":"Solutions to all probability puzzles mobile app.\n\nCatalan numbers $C_n$ is the number of monotonic lattice paths along the edges of a SQUARE grid with n × n square cells, which do not pass above the diagonal.\n\n[Bertrand ballot problem](https://en.wikipedia.org/wiki/Bertrand%27s_ballot_theorem) for p > q (RECTANGLE proble) probability of strictly in creasing win is $(p-q)/(p+q)$\n\n\n**Bounded Gamblers ruin**\n\n\n**Unbounded Gamblers ruin**\n\nAssume probability $p$ to go up and probability $q$ to go down. Let $k_i$ be the expected hitting time of 0 from state $i$ and $h_{i}$ be the \nprobability of hitting 0 from state $i$ at some point in time.\n\nIf $p < q$ then and $h_{i} = 1$ and $k_{i} = \\frac{1}{q-p} * i$ \n\nIf $p > q$ then $h_{i} < 1$ and hence $k_{i} = infinity$ \n\nIf $p = q$ then $h_{i} = 1$ but $k_{i} = infinity$\n\nWhen probability of hitting 0 is less than 1, then always mean hitting time is infinity as there is small chance to never hit 0 so mean hit time is infinity.\n\nWhen probability of hitting 0 is 1, then mean hitting time can be infinity or not.","n":0.075}}},{"i":94,"$":{"0":{"v":"Machine Learning","n":0.707},"1":{"v":"# Articles\n[ML algos](https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-for-2023-843dba11419c)\n\n# Kaggle\n\n- [Repo](https://github.com/ngocuong0105/kaggle)\n- [[machine learning.Kaggle tricks]]\n\n\nSteps:\n1. EDA\n2. Feature engineering\n3. Build ML model\n4. Model validation\n\n\n**[random_state parameter](https://scikit-learn.org/stable/glossary.html#term-random_state:~:text=random_state-,%C2%B6,-%C2%B6)**\n\nMany machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run. You use any number, and model quality won't depend meaningfully on exactly what value you choose.\n\n**Underfitting vs overfitting**\n\nControl the balance between these two using max leaves and max tree depth parameters in decision trees.\n\n# MLOps\n\n[Weights and Biases](https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases/notebook)","n":0.114}}},{"i":95,"$":{"0":{"v":"Experiments","n":1}}},{"i":96,"$":{"0":{"v":"Stratification","n":1},"1":{"v":"# Stratification\nStratification is the process of dividing members of the population into homogeneous subgroups before sampling. In statistical surveys, when subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) independently. \n\nStratification is a common technique used in Monte Carlo sampling to achieve variance reduction.\n\nGoal estimate: $E(Y)$. The standard Monte Carlo approach is to sample $Y_{i}$ and compute $E(Y)$ as $\\frac{1}{n}\\sum Y_{i}$ which has variance $var(Y)/n$\n\nThe basic idea of stratification is to divide the sampling region into strata, sample within each stratum separately  (each has mean $\\bar{Y}_{i}$) and then combine results from individual strata together to give an overall estimate, which usually has a smaller variance than the estimate without stratification. The final estimate you get is $\\sum w_{i}\\bar{Y}_{i}$\n\nThey should have the same expected value but lower variance when the means differ across strata.\n\nThe intuition is that the variance of Y can be decomposed into the within-strata variance and the between-strata variance, and the latter (between strata) is removed through stratification.\n\n\nA good stratification is the one that aligns well with the un-derlying clusters in the data. By explicitly identifying these clusters as strata, we essentially remove the extra variance introduced by them.\n\n\n\n# Stratification in online experiments\n\nIn the online world, because we collect data as they arrive over time, we are usually unable to sample from strataformed ahead of time. \n\nFor example, if $Y_i$ is the number of queries from a user $i$, a covariate $X_i$ could be the browser that the user used before the experiment started.\n\n\nIn experiments (e.g t-test) you compare the means of two groups ($\\bar{Y}+_{c}$ and $\\bar{Y}+_{t}$). We want to see variance reduction in the delta between the two groups, $\\bar{Y}+_{c} - \\bar{Y}+_{t}$.\n\n![](assets/images/delta_exp.png)\n\n\nIt is important to note that by using only the pre-experiment information, the stratification variable X (e.g which browser) is independent of the experiment effect. This ensures that the stratified delta is unbiased.","n":0.057}}},{"i":97,"$":{"0":{"v":"CUPED","n":1},"1":{"v":"\n# Experiment setup\n\ngoal is to estimate the `delta = mean(Y_c) - mean(Y_t)`\n\n![](assets/images/exp_setup1.png)\n\n![](assets/images/exp_setup2.png)\n\nWe want this estimate to have small varaince and to be unbiased.\n\n*In this framework, the key to variance reduction for the difference in mean lies in reducing the variance of the means\nthemselves.*\n\n\n# What is CUPED?\n\nCUPED is a well-established variance reduction technique for experiments [paper](https://www.exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf)\n\nUnlike methods like outlier removal or winsorization, it doesn’t sacrifice any data integrity.\n\nIt can be used together with winsorization, which enhances its effectiveness.\n\nIt requires pre-experimentation data - which we generally have in Bunsen.\n\nFor a rather unpredictable event like ad clicks, we get ~5% reduction in standard deviation (which is ~10% reduction in variance, and experiment run time). For something like sessions, that 5% can increase to something like 30%.\n\n\n# Predicted uncertainty is not uncertainty. \n\nSo the variance in your experiment metric should only take into account the unpredictable part of your measurement. If you predict that:\n- guv A will have 5 sessions during the experiment, and \n- guv B will have 20 sessions,\n\nAnd when you actually measure the sessions, you get:\n- guv A has 4 sessions (-1 from prediction)\n- guv B has 21 sessions (+1 from prediction),\n\nThen the variance, or the uncertainty, in your measurement can be calculated on the residuals of your predictions, rather than on the measurements themselves. This gives var([-1, +1]), rather than var([4, 21]), which is a great reduction.\n\n[notebook](https://drive.google.com/file/d/1YYSY9IgZzkp8q9U2ujPc2wuSRVtlgcqq/view?usp=drive_link)\n\n[presentation](https://docs.google.com/presentation/d/1_ae5aQ12v0ykqCLF3JB3urdgHeZuwags/edit#slide=id.p1)\n\n\n# Control Variates\n\nFrom paper:\n\n![](assets/images/control_variates.png)\n\n\n![](assets/images/control_variates_1.png)\n\n\n# Control Variates in online experiments\n\n**The difficulty of applying it boils down to finding a control variate $X$ that is highly correlated with $Y$ and at the same time has known $E(X)$.**\n\n![](assets/images/control_variates_2.png)\n\n\n![](assets/images/control_variates_3.png)\n\n*cv* stands for control varaites\n\n# Control Variates vs Stratification\n\nSee [[machine learning.experiments.Stratification]]\n\n\nThese are two techniques that both utilize covariates to achieve variance reduction. The stratification approach uses the covariates to construct strata while the control variates approach uses them as regression variables.\n\nThe former uses discrete (or discretized) covariates, whereas control variates seem more naturally to be continuous variables.\n\nControl Variates is an extension of stratification, where the covariate can be an indicator variable showing the belonging of a sample to a stratum.\n\n\nWhile these two techniques are well connected mathematically, they provide different insights into understanding why and how to achieve variance reduction. The stratification formulation has a nice analogy with mixture models, where each stratum is one component of the mixture model. Stratification is equivalent to separating samples according to their component memberships, effectively removing the between-component variance and achieving a reduced variance. A better covariate is hence the one that can better classify the samples and align with their underlying structure. On the other hand, the control variates formulation quantifies the amount of variance reduction as a function of the correlation between the covariates and the variable itself. It is mathematically simpler and more elegant. Clearly, a better covariate should be the one with larger (absolute) correlation.\n\n# CUPED IN PRACTICE\n\nA simple yet effective way to implement CUPED is to use the same variable from the pre-experiment period as the covariate. You need to have pre-experiment data (have not applied treatment), not good when you work with new users.\n\nThe more correlated covariate with the target the larger variance reduction.\n\n\nAcross a large class of metrics, our results consistently showed that using the same variable from the pre-experiment period as the covariate tends to give the best variance reduction. In addition, the lengths of the pre-experiment and the experiment periods also play a role. Given the same pre-experiment period, extending the length of the experiment does not necessarily improve the variance reduction rate. On the other hand, a longer pre-period tends to give a higher reduction for the same experiment period.\n\n\n- margarida's implementation\n```sql\n-- winsorization 90th %tile + CUPED\n\n  WITH bucketed_assignment_log AS (\n      -- macro assign_cohorts start\n      SELECT cohort_id,\n             user_id_encid,\n             MIN(first_assignment_time) - INTERVAL '5 SECOND' AS first_assignment_time\n        FROM dl_bunsen_filtered.assignment_log_metric_analysis_v2\n       WHERE experiment_run_id = 11052\n         AND (is_bot = FALSE OR is_bot IS NULL)\n         AND first_assignment_time BETWEEN '2023-07-25' -- wildcards are timestamp strings, e.g. '2022-07-28 12:00:00'\n           AND '2023-09-25' -- note that the single quotes are required\n       GROUP BY 1, 2\n      -- macro assign_cohorts end\n  ),\n\n       connections_data AS (SELECT connections.user_id_encid,\n                                   connections.object_id,\n                                   datapipe_timestamp AS event_time\n                              FROM dl_bunsen_mad.connections_sessionized_bot_labeled AS connections\n                             WHERE TIMESTAMP 'epoch' + datapipe_event_time / 1000000 * INTERVAL '1 second' BETWEEN '2023-07-25'\n                                 AND '2023-09-25'\n                               AND connections.dt BETWEEN (DATE_TRUNC('day', '2023-07-25'::TIMESTAMP) - INTERVAL '1 DAY')\n                                 AND (DATE_TRUNC('day', '2023-09-25'::TIMESTAMP) + INTERVAL '1 DAY')\n                               AND connection_type IN ('photo_uploaded')\n                               AND is_bot = 'false'\n                             GROUP BY 1, 2, 3),\n\n       sum_per_day_per_user AS (SELECT cohort_id,\n                                       bucketed_assignment_log.user_id_encid,\n                                       first_assignment_time,\n                                       COUNT(DISTINCT object_id) AS sum_per_day_per_user\n                                  FROM bucketed_assignment_log\n                                  INNER JOIN connections_data\n                                      ON bucketed_assignment_log.user_id_encid = connections_data.user_id_encid\n                                          AND\n                                         connections_data.event_time BETWEEN bucketed_assignment_log.first_assignment_time\n                                             AND '2023-09-25'\n                                 GROUP BY 1, 2, 3),\n\n       percentile_reviews AS (SELECT user_id_encid,\n                                     sum_per_day_per_user,\n                                     PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY sum_per_day_per_user)\n                                     OVER () AS percentile_90\n                                FROM sum_per_day_per_user),\n\n       censored_photos AS (SELECT user_id_encid,\n                                  CASE\n                                      WHEN sum_per_day_per_user <= percentile_90 THEN sum_per_day_per_user\n                                      ELSE percentile_90\n                                      END AS photos_censored\n                             FROM percentile_reviews),\n\n       sum_per_user AS (SELECT cohort_id,\n                               user_id_encid,\n                               first_assignment_time,\n                               COALESCE(photos_censored, 0) /\n                               (DATEDIFF(SEC,\n                                         TO_DATE('2023-07-25', 'YYYY-MM-DD'),\n                                         TO_DATE('2023-09-25', 'YYYY-MM-DD')) / 86400.\n                                   )::FLOAT AS avg_sum_per_user\n                          FROM bucketed_assignment_log\n                          LEFT JOIN censored_photos\n                              USING (user_id_encid)),\n\n       get_theta AS (SELECT cohort_id,\n                            user_id_encid,\n                            X,\n                            Y,\n                            avg_X,\n                            avg_Y,\n                            var_X,\n                            SUM((X - avg_X) * (Y - avg_Y) / (N - 1)) OVER () AS cov_XY,\n                            cov_XY / var_X                                   AS theta\n                       FROM (SELECT cohort_id,\n                                    sum_per_user.user_id_encid,\n                                    COALESCE(num_photos_in_last_year, 0) AS X,\n                                    avg_sum_per_user                     AS Y,\n                                    COUNT(*) OVER ()                     AS N,\n                                    AVG(X) OVER ()                       AS avg_X,\n                                    AVG(Y) OVER ()                       AS avg_Y,\n                                    VARIANCE(X) OVER ()                  AS var_X\n                               FROM sum_per_user\n                               LEFT JOIN data_science.contribution_summary\n                                   ON sum_per_user.user_id_encid = contribution_summary.user_id_encid\n                                       AND contribution_summary.dt =\n                                           (SELECT MAX(dt) FROM data_science.contribution_summary)) AS aux\n                      GROUP BY cohort_id,\n                               user_id_encid,\n                               X,\n                               Y,\n                               avg_X,\n                               var_X,\n                               avg_Y,\n                               N),\n\n       get_Y_cuped AS (SELECT cohort_id,\n                              user_id_encid,\n                              Y - theta * (X - avg_X) AS Y_cuped\n                         FROM get_theta\n                        GROUP BY cohort_id,\n                                 user_id_encid,\n                                 X,\n                                 Y,\n                                 avg_X,\n                                 theta)\n\nSELECT cohort_id,\n       COUNT(user_id_encid) AS sample_size,\n       AVG(Y_cuped)         AS metric_value,\n       STDDEV(Y_cuped)      AS standard_deviation\n  FROM get_Y_cuped\n GROUP BY cohort_id\n ORDER BY cohort_id;\n\n\n```","n":0.033}}},{"i":98,"$":{"0":{"v":"All in one","n":0.577},"1":{"v":"# Basics\n\nExperiments are created by **randomly** splitting your traffic into separate cohorts, where each cohort gets a different variation of a treatment. Experiments can establish causality because the randomly assigned cohorts are effectively the same except for the applied treatment, so any differences seen in metric results are a result of the applied treatment. However, because the cohorts are sampled, they may not be reflective of the entire population, which introduces uncertainty (this is where statistics comes in!). \nTo simplify the results of the statistics, we have created the Scorecard.\n\n\n3 types of experiments:\n- Simple AB allows you to compare multiple variants against the status quo. A Simple A/B does not run a formal A/A test.\n- A/B allows you to compare multiple variants against the status quo.\n- Rollout allows you to compare one single new variant - usually the new feature - against the status quo.\n- SEO\n\n**Beaker**\n\nThis is Yelp's platform to create experiments. You can create experiments in these two subplatforms:\n- RequestBucketer (cohort-based)\n- Bunsen (parameter-based)\n\nBoth allow you to run experiments, the difference is that you manage and instantiate them in a different way. Bunsen tracks performing metrics automatically. RequestBucketer requires your own or third-party solution.\n\n\n## A/B test\n\nA/B testing is a method of comparing multiple variants against each other to determine which one performs better.\n\nA typical A/B experiment includes one A/A test and multiple A/B tests.\n- A/A test: a test to determine the time needed to conduct the A/B test. The A/A test itself runs for 7 days.\n- A/B test: a test to compare treatment with current status quo. It runs for the time determined by the A/A test.\n\n## Rollout\nA feature rollout is the process of introducing a new feature to a set of users and analyzing the impact. By rolling out in phases, it gives a company an opportunity to fully test the UI and user experience.\n\nIt includes one optional A/A test and multiple rollout runs.\n- A/A test: a test to determine the time needed to conduct the A/B test. It is only needed if tracking a metric. The A/A test itself runs for 7 days.\n- Rollout run: a process of splitting traffic into status quo and one variant. There is no requirement on how long a rollout run should take.\n\n\n## Simple AB\n\nSimple AB is the fastest option for most experiments that do not have specific needs for an A/A test. Power Analysis occurs during the A/B run, removing the week long A/A Run requirement.\n\nIt includes optional smoke tests and one A/B test.\n- Smoke test: a short test to uncover implementation issues at an early stage of the experiment pipeline. It need not run for more than 1-2 days.\n- A/B test: compares treatment cohorts against status quo. Automatically determines duration of the experiment run (at least 7 days).\n\nUnsupported Use Cases:\n- Experiments with diversion keys other than guv , user_id_encid and business_id_encid.\n- Swimlanes.\n- Manual A/A tests.\n\n## SEO\nSEO experiments are used to test features that impact web organic traffic levels coming into yelp. They use page_id as a diversion key instead of guv, therefore a new feature is introduced to a set of \"pages\" versus a set of \"users\". The SEO experiment tests impact on both msite and www pages for Yelp. An SEO Experiment tests features to identify what optimizes SEO and therefore should lead to an increase in organic traffic.\n\n\nA typical SEO experiment includes two tests, an A/A test followed by an A/B test on the same set of users.\n- The first A/A test is used to collect baseline value, define the required number of samples and randomly selects samples for the experiment.\n- The second A/B test runs after the first A/A run has reached full power, and uses the baseline and selected samples to compare treatment cohorts with current status quo.\n\nSpecifics of SEO experiments:\n\nThese experiments run not on users but on website pages and number of pages in cohort is not stable - some pages can be a part of an experiment at some point of time (not necessarily during all period when it was active) BUT they can’t switch between cohorts. \nThe main goal of SEO experiments is to test how certain changes on webpage can impact our traffic from Google, in other words: how Google react on changes (title tags, internal linking, content, etc.) we make on a page - will they rank us higher and send us more traffic or maybe we’ll start losing traffic. \n\n## What types of runs make up an SEO experiment?\nEach SEO experiment is made of exactly one SEO A/A test followed by one SEO A/B test. Each individual test can be rerun if issues occur.\n\nSEO A/A: A/A testing is used to test two identical versions of each other. A/A testing is done to:\n\n- Collect the metric value of the status quo cohort in order to perform power analysis, calculating the number of samples needed for the experiment to conclude.\n- Collect the required number of samples into the assignment log for A/B run to use. Any new samples that the A/B run collects but were not present during the A/A run will not be considered for analysis.\n\nSEO A/B: A/B testing is a method of comparing multiple variants against each other.\n\n- The A/B run measures how the samples collected during A/A run react to different treatments.\n- Any new samples that appeared during AB run but were not present during A/A will not be included in analysis.\n\n\n## Steps to run SEO\n\n\n\n# Scorecard\n\n\n\n## Guardrail alerts\n\n\nGuardrails are important company wide metrics that we aim to protect against while experimenting.\n\n\n\n# A/B Testing Intuition Busters\n\n[Paper](https://drive.google.com/file/d/1O0HxZprNGDpzD27Aiqjm5XxAp2FQigca/view?usp=drive_link)\n\n","n":0.033}}},{"i":99,"$":{"0":{"v":"Time Series","n":0.707},"1":{"v":"[Forecast evaluation](https://www.arxiv-vanity.com/papers/2203.10716/)\n\n[Kaggle resources](https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/372926)\n\n","n":0.577}}},{"i":100,"$":{"0":{"v":"Explainable AI - SHAP","n":0.5},"1":{"v":"\n# [A unified approach to interpreting model predictions](https://drive.google.com/file/d/16EB_r2xMpIwWTbmqSCzr_ajXOIg26oTb/view?usp=drive_link)\n\n[SHAP docs](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)\n\n- explainable AI\n- accuracy vs interpretability\n- SHAP (SHapley Additive exPlanations)\n- new notion: **any explanation of a model’s prediction as a model itself, which we term the explanation model**\n- related to partial dependency plots (way to compute 'importance' of a group of features)\n\n\n**Definition.**  The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler **explanation model**, which we define as any interpretable approximation of the original model.\n\n**Definition 1:** Additive feature attribution methods have an explanation model that is a linear function of binary variables:\n\\[ g(z') = \\phi_0 + \\sum_{i=1}^{M} X_i \\phi_i z_i', \\quad \\text{where} \\quad z' \\in \\{0, 1\\}^M, \\, M \\, \\text{is the number of simplified input features, and} \\, \\phi_i \\in \\mathbb{R}. \\]\n\n\n**Definition 0:** Additive feature attribution methods have an explanation model that is a linear function of binary variables:  \n\n\nPaper important things:\n- Definition 1 Additive feature attribution methods\n- Shapley regression values \n- Shapley sampling values \n- Theorem 1: There is a unique additive feature attribution method that satisfies the properties of local accuracy, missingness, and consistency.\n- Section 3 provides the solution to that theorem\n- main problem is to approximate SHAP values\n- SHAP values can be very complicated to compute (they are NP-hard in general)\n- The paper describes two model-agnostic approcimate methods to compute SHAP:\n    - Spaley sampling values (already known)\n    - Kernel SHAP (novel)\n\n\nOne of the fundamental properties of Shapley values is that they always sum up to the difference between the game outcome when all players are present and the game outcome when no players are present.\n\n\n\n**SHAP Values:**\n\nSHAP (SHapley Additive exPlanations) values are a unified measure of feature importance in machine learning models. The key idea behind SHAP values is to distribute the prediction value among features, considering all possible feature combinations. Specifically, for a prediction $f(x)$ in a model, SHAP values allocate the contribution of each feature $i$ to the prediction by averaging over all possible feature combinations, taking into account their interactions. Mathematically, SHAP values are defined as:\n\n\n$$\n\\phi_i(f) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{{|S|!(|N|-|S|-1)!}}{{|N|!}} [f(S \\cup \\{i\\}) - f(S)]\n$$\n\nwhere $\\phi_i(f)$ represents the SHAP value of feature $i$ for the prediction function $f$, $N$ is the set of all features, and $S$ is a subset of features excluding feature $i$. The sum is taken over all possible subsets $S$.\n\n\nSHAP values provide a consistent, locally accurate, and globally fair attribution method, enabling interpretable analysis of complex machine learning models.\n\n\n# SHAP Values\n","n":0.047}}},{"i":101,"$":{"0":{"v":"Outliers","n":1},"1":{"v":"It is important to consider detecting and handling outliers in your data to prevent them from negatively impacting your model. Outliers can cause issues such as skewing the distribution of the data and leading to poor model performance.There are several techniques that can be used to detect and handle outliers, including IsolationForest, OneClassSVM, EllipticEnvelope, and LocalOutlierFactor. You can also consider removing or interpolating outlier data points as a way to address this issue. By addressing outliers in your data, you can help ensure that your model is better able to generalize to new, unseen data.\n\n# PCA\nUse PCA to detect outliers in the dataset. PCA in particular can show you anomalous variation which might not be apparent from the original features: neither small houses nor houses with large basements are unusual, but it is unusual for small houses to have large basements. That's the kind of thing a principal component can show you.\n\nBox plot the y-values over pca components and look at the extreme points.\n\n","n":0.078}}},{"i":102,"$":{"0":{"v":"Metrics","n":1},"1":{"v":"\n# F1 score\n\nHarmonic mean between precision and recall. Precision measures how precise is you pick up positive predictions (TP/FP).\nRecall is how many of all true predictions you have predicted to be positive (TP/AP).\n\nTP = true positive predictions\n\nFP = forecastes as positive\n\nAP = actual positive\n\n\nYou can pick one positive prediction and it to be corrrect. This will have high precision but low recall as you have not captured many true predictions.\n\nF1 brings recall and precision in one number and we take the harmonic mean because precision and recall are rates.\n\nHarmonic mean is used when you have multiple rates and you need to compute the **average rate**.\n\n\n**NB**\n- F1 score measures how good are your positive predictions\n- F1 score is skewed towards classifiers which would overforecast positive predictions\n- F1 score is 0.67 if you predict only ones and is 0 if you predict only zeroes.\n- F1 equalized the importance of precision and recall. In practice these represent different mis-classifications.\n- F1 ingnores True Negatives and is misleading for imbalannced classes\n- F1 is not symmetric - if you change labels and forecasts from 1 to 0 and vice versa your F1 score changes\n\nExample:\n\nActual: 0, 1\n\nPrediction: 1, 1\n\nF1 = 0.67, precision is 0.5, recall is 1\n\nAfter swap:\n\nActual: 1, 0\n\nPrediction: 0,0\n\nF1 = 0\n\n# SMAPE\n\n[Symmetrics mean absolute percentage error](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error)\n\n- Overforecasts are better than under forecasts (A: 100, F: 120 is better than A: 100, F: 80).\n- SMAPE is a ratio metric. Hence if you adjust your forecast you better use a multiplication factor rather than additive factor (adding constant number (e.g 5) to a forecast of 100 and to a forecast of 10 is totatally different).\n- small actual values and forecasts can have big impact on the metric (usually data is nosier with small values/quantitites) \n- one possible trick to optimize SMAPE is to predict the relative change rather than the actual values (relative change works with normilized data).\n\nExample why you need target engineering/relative change fortecasts:\n```\nLet's take one example of two cities and two months.\nFirst month:\nCity A = 5\nCity B = 100\n\nSecond month:\nCity A = 1 (-80% retrocession)\nCity B = 105 (+5% growth)\n\nNow suppose your model is predicting the second month, and the predictions are:\nPred A = 6 (+20%)\nPred B = 110 (+10%)\n\nFrom these predictions, you had a low MAE (missed both by 5), but a high SMAPE for A and very low for B.\n\nSMAPE = SMAPE(city A) + SMAPE(city B) = 5/7 + 5/215\n\ncity A contributes to the error 30 times more than city B. Smaller city hase huge effect on the SMAPE.\n\nNow suppose you use rate of change as Target\nSMAPE = SMAPE(city A) + SMAPE(city B) = 1/1 + 0.05/0.15 = 1 + 1/3\n```\n\n**The example above shows that small cities can have huge impact on SMAPE and last value model should generrally be used for them**\n\n","n":0.047}}},{"i":103,"$":{"0":{"v":"Loss Functions","n":0.707},"1":{"v":"\n# Huber Loss\n[Wikipedia](https://en.wikipedia.org/wiki/Huber_loss#:~:text=absolute%20value%20function).-,Pseudo%2DHuber%20loss%20function,less%20steep%20for%20extreme%20values.)\n\nHuber loss is a combination of L1 and L2 loss functions. It is less sensitive to outliers in data than the squared error loss.\n\npseudo huber loss allows you to control the smoothness and therefore you can specifically decide how much you penalise outliers by, whereas huber loss is either MSE or MAE\n\n## How to choose the hyper parameter delta?\nHuber loss will clip gradients to delta for residual (abs) values larger than delta. You want that when some part of your data points poorly fit the model and you would like to limit their influence. Also, clipping the grads is a common way to make optimization stable (not necessarily with huber).\n[StackExchange](https://stats.stackexchange.com/questions/465937/how-to-choose-delta-parameter-in-huber-loss-function)\n\n\n# L1 vs L2 regularization\n\n\nThe best advice in data science is always try both and see what produces better CV and LB.\n\nI think L1 shines when we approach \"the curse of dimensionality\" (i.e. when the number of train rows is small compared with number of train columns). I think a rule of thumb is when number of features (i.e. columns) is anywhere near 1/10th (or larger) the number of training samples (i.e. rows) we approach the \"the curse of dimensionality\".\n\n","n":0.072}}},{"i":104,"$":{"0":{"v":"Logistic Regression","n":0.707},"1":{"v":"# Logistic regression\n\nLogistic regression is actually a **classification method**.\n\n**Logistic loss = - Log likelihood**\n\nThe decision boundary of logistic regression is always a straight line. Logistic regression is considered a **generalized linear model**. Linear regression expresses the log-odds in terms of a linear function of the inputs $x$. Log-odds is a way to compute the probability of classifying as 0 or 1.\n\n## Linear decision boundary\n\n![](assets/images/linear_decision_boundary.png)\n\n# Logistic Regression as a Neural Network\n\nBinary classification of an image - flatten all pixels into one vector\n\n![](assets/images/binary_class.png)\n\n\nIn NN the design matrix would have dimension $nxm$ - it is easier that way (transposed of ML design matrix)\n\n![](assets/images/nn_notation.png)\n\n**Logistic regression is just applying sigmoid function to the linear model, $\\hat{y} = \\sigma(wx+b)$ and using logistic loss function.**\n\n![](assets/images/logistic_tregression.png)\n\n![](assets/images/log_reg_defn.png)\n\nNote in logistic regression we predict $\\hat{y} = \\sigma(wx+b)$ which represents a probability between 0 and 1. We use the ogistic losss function so that we have a convex cost function. No matter where you initialize, you would reach the global minimum.\n\n","n":0.079}}},{"i":105,"$":{"0":{"v":"Linear Regression","n":0.707},"1":{"v":"\nHere I discuss how violation of linear regression assumptions affect model explainability and foresting performance.\nIt is a practical guide on the caveats of the Linear Model and how to deal with them - particularly useful in forecasting-focused settings like Kaggle. It is assumed the reader is familiar with the mathematics behind the model.\n\nLinear model's assumptions are:\n- The target variable is linear in the predictor variables. Violation is also known for mispecified model\n- The errors are normally distributed, independent, and homoscedastic (constant variance).\n- The errors are independent of the features (no endogeneity).\n- The features are not multi-collinear (not highly correlated).\n\n\nExample when each of these assumptions is violated:\n- Non-linear relationship: $y = 2x^2 + noise$\n- Correlated errors: time series data with autocorrelated residuals\n- Heteroscedasticity: variance of errors increases with the value of x (leverage effect, further from mean pull the line more)\n- Endogeneity: omitted variable that affects that affects both x and y\n\n\n\n\n# Multi-Collinear Features\n\n- **Multicolinearity affects model interpretability and not so much forecasting performance.**\n\nExperiment:\n- Run LR `y ~ x1`\n- Run LR `y ~ x1+x2`, where these two are highly correlated\n\nResults\n```\nUncorrelated features correlation:\n\nUncorrelated case model summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.929\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     1275.\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           5.50e-58\nTime:                        08:19:04   Log-Likelihood:                -78.314\nNo. Observations:                 100   AIC:                             160.6\nDf Residuals:                      98   BIC:                             165.8\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          5.0443      0.054     93.689      0.000       4.937       5.151\nX1             2.1139      0.059     35.712      0.000       1.996       2.231\n==============================================================================\nOmnibus:                        3.154   Durbin-Watson:                   2.216\nProb(Omnibus):                  0.207   Jarque-Bera (JB):                3.133\nSkew:                           0.105   Prob(JB):                        0.209\nKurtosis:                       3.841   Cond. No.                         1.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nHighly correlated features correlation:\n          X1        X2\nX1  1.000000  0.999931\nX2  0.999931  1.000000\n\nHighly correlated case model summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.939\nModel:                            OLS   Adj. R-squared:                  0.938\nMethod:                 Least Squares   F-statistic:                     751.5\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           9.10e-60\nTime:                        08:19:04   Log-Likelihood:                -63.278\nNo. Observations:                 100   AIC:                             132.6\nDf Residuals:                      97   BIC:                             140.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.9343      0.047    105.551      0.000       4.842       5.027\nX1             6.8089      4.486      1.518      0.132      -2.095      15.713\nX2            -4.7588      4.475     -1.064      0.290     -13.639       4.122\n==============================================================================\nOmnibus:                        0.401   Durbin-Watson:                   2.118\nProb(Omnibus):                  0.818   Jarque-Bera (JB):                0.103\nSkew:                          -0.034   Prob(JB):                        0.950\nKurtosis:                       3.141   Cond. No.                         174.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n**Insights:**\n- Forecasts of both model are good and in practice you don't need to drop the correlated features if you care about forecasts\n- Remember that the forecast $\\hat{y}$ is the orthogonal projection of the real value y on the subspace spanned by the design matrix $X$. If the design matrix has correlated features there are multiple ways to express the same $\\hat{y}$. Hence different values for the $\\hat{beta}$ would give the same forecast.  \n- When there are correlated features you will see large variance in the beta estimates, also mathematically $var(\\hat{beta})=(XX^{T})^{-1}\\sigma$ will just be very large when the inside matrix is not positive definite.\n- large beta variance, means low t-statistic, which means high p-value\n- hence with correlated features the model is not interpretable but the sum is good forecasts.\n\n\n**The Geometry**\n- When two variables are highly correlated, they both almost point in the same “direction” in the predictor space.\n- The model tries to allocate credit (and adjust slope) between them for explaining changes in y.\n- Many combinations of coefficients can fit the same plane equally well.\n         \n**Effect on Model Fit** \n\n- The overall plane/surface that regression fits (ŷ = intercept + b1*X1 + b2*X2) can still be the same, though b1 and b2 may be weird or unintuitive (like 378 and -377). Forecasts are still good.\n- The regression is “sure” about the sum effect, but not about the individual effects.\n     \n**Unstaple coefficients**\n- Opposite signs and very large in absolute values to offset each other.\n- For forecasting, this generally does not hurt test-set predictive performance as long as your train/test data comes from the same distribution and the correlation patterns are stable. It can, however, make your model more sensitive to changes in the feature distribution (\"unstable predictions\" with changing data).\n\n# High Leverage Points\n\n- identify outliers in the design matrix $X$ (not in target variable $y$)\n- Property of: Only the feature/design matrix X  \n- Definition: Measures how far an observation's x-values are from the mean of all x-values, i.e., how \"unusual\" its X-row is.\n- Mathematically: The diagonal elements $h_i$ of the “hat” matrix $H=X(X^TX)^{-1}X^T$.\n- Note: Leverage is completely independent of $y$.\n     \n- outliers in the design matrix $X$ can lead to the picture below:\n- if a row $x_i$ deviates from the mean of $X$ it will have large leverage and pull the regression line\n![high_leverage_point](./assets/images/high_leverage_point.png)\n\n\nNOTE:  In regression, leverage is a property of observations (rows, i.e. data points), not of features (columns, variables). \n- You need to check rows that are with high leverage not features/columns!\n\n```python\n# Model Spec\n# Generate 8 normal data points\nX1 = np.random.normal(0, 1, 8)\nX2 = np.random.normal(0, 1, 8)\ny = 2*X1 + 3*X2 + np.random.normal(0, 0.5, 8)\n\n# Add a high-leverage point (extreme X1)\nX1_leverage = np.append(X1, 10)\nX2_leverage = np.append(X2, 1)\ny_leverage = np.append(y, 4)\n```\n\n**Results**\n\n```\nModel with leveraged point\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.526\nModel:                            OLS   Adj. R-squared:                  0.368\nMethod:                 Least Squares   F-statistic:                     3.328\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):              0.107\nTime:                        09:31:52   Log-Likelihood:                -18.040\nNo. Observations:                   9   AIC:                             42.08\nDf Residuals:                       6   BIC:                             42.67\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0550      1.093      0.965      0.372      -1.620       3.730\nX1             0.0434      0.282      0.154      0.883      -0.647       0.734\nX2             3.9627      1.857      2.134      0.077      -0.580       8.505\n==============================================================================\nOmnibus:                        0.114   Durbin-Watson:                   1.807\nProb(Omnibus):                  0.944   Jarque-Bera (JB):                0.321\nSkew:                           0.113   Prob(JB):                        0.852\nKurtosis:                       2.103   Cond. No.                         9.95\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nModel without leveraged point\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.972\nModel:                            OLS   Adj. R-squared:                  0.961\nMethod:                 Least Squares   F-statistic:                     88.04\nDate:                Tue, 08 Jul 2025   Prob (F-statistic):           0.000127\nTime:                        09:31:52   Log-Likelihood:                -5.0804\nNo. Observations:                   8   AIC:                             16.16\nDf Residuals:                       5   BIC:                             16.40\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2898      0.299      0.968      0.377      -0.480       1.059\nX1             2.0443      0.233      8.772      0.000       1.445       2.643\nX2             2.1176      0.528      4.008      0.010       0.759       3.476\n==============================================================================\nOmnibus:                        3.584   Durbin-Watson:                   2.783\nProb(Omnibus):                  0.167   Jarque-Bera (JB):                1.223\nSkew:                          -0.958   Prob(JB):                        0.543\nKurtosis:                       3.002   Cond. No.                         4.46\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n# Residuals and Cook's Distance\n\n- identify outliers in target variable $y$\n- Studentised residuals and cook's distance use leverage and residual error to find outliers in $y$\n\n\n![alt text](assets/images/y_outlier.png)\n\n# Homo or Hetero\n- Homoscedacity is the assumption that residuals are with equal variance. \n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndef generate_data(seed=42, n1=50, n2=50):\n    np.random.seed(seed)\n    x1 = np.linspace(0, 10, n1)\n    y1 = 2 * x1 + 1 + np.random.normal(0, 1, n1)\n    x2 = np.linspace(10, 20, n2)\n    y2 = 2 * x2 + 1 + np.random.normal(0, 69, n2) # High variance, Different mean\n    return x1, y1, x2, y2\n\ndef fit_single_model(x, y):\n    model = LinearRegression().fit(x.reshape(-1, 1), y)\n    y_pred = model.predict(x.reshape(-1, 1))\n    return model, y_pred\n\ndef fit_two_models(x1, y1, x2, y2):\n    model1 = LinearRegression().fit(x1.reshape(-1, 1), y1)\n    y1_pred = model1.predict(x1.reshape(-1, 1))\n    model2 = LinearRegression().fit(x2.reshape(-1, 1), y2)\n    y2_pred = model2.predict(x2.reshape(-1, 1))\n    return model1, y1_pred, model2, y2_pred\n\ndef plot_results(x1, y1, x2, y2, X_all, y_pred_all, y1_pred, y2_pred):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(x1, y1, label='Set 1 (Low variance)', color='blue')\n    plt.scatter(x2, y2, label='Set 2 (High variance)', color='red')\n    plt.plot(X_all, y_pred_all, label='Fit on all points', color='black', linewidth=2)\n    plt.plot(x1, y1_pred, label='Fit on Set 1', color='blue', linestyle='--')\n    plt.plot(x2, y2_pred, label='Fit on Set 2', color='red', linestyle='--')\n    plt.legend()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Illustration of Heteroscedasticity')\n    plt.show()\n\ndef calculate_mse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n\ndef main():\n    # Generate data\n    x1, y1, x2, y2 = generate_data()\n    X_all = np.concatenate([x1, x2]).reshape(-1, 1)\n    y_all = np.concatenate([y1, y2])\n\n    # Fit models\n    model_all, y_pred_all = fit_single_model(np.concatenate([x1, x2]), y_all)\n    model1, y1_pred, model2, y2_pred = fit_two_models(x1, y1, x2, y2)\n\n    # Plot\n    plot_results(x1, y1, x2, y2, np.concatenate([x1, x2]), y_pred_all, y1_pred, y2_pred)\n\n    # MSE calculations\n    mse_all = calculate_mse(y_all, y_pred_all)\n    y_sep_pred = np.concatenate([y1_pred, y2_pred])\n    mse_sep = calculate_mse(y_all, y_sep_pred)\n\n    print(\"MSE (fit to all data):\", round(mse_all))\n    print(\"MSE (fit separately):\", round(mse_sep))\n\n\nmain()\n```\n</details>\n\nIt reduces a bit the forecasting accuracy.\n\n![alt text](./assets/images/heteroscedacity.png)\n\n\n\n<details>\n<summary> <b>Summary of the results:</b> </summary>\n\n```\n=== Regression summary: All Data ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.059\nModel:                            OLS   Adj. R-squared:                  0.050\nMethod:                 Least Squares   F-statistic:                     6.194\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):             0.0145\nTime:                        08:37:53   Log-Likelihood:                -516.21\nNo. Observations:                 100   AIC:                             1036.\nDf Residuals:                      98   BIC:                             1042.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.2053      8.499      0.377      0.707     -13.661      20.072\nx1             1.8295      0.735      2.489      0.015       0.371       3.288\n==============================================================================\nOmnibus:                       21.818   Durbin-Watson:                   2.191\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               73.588\nSkew:                          -0.605   Prob(JB):                     1.05e-16\nKurtosis:                       7.025   Cond. No.                         23.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Regression summary: Set 1 (Low variance) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.975\nModel:                            OLS   Adj. R-squared:                  0.975\nMethod:                 Least Squares   F-statistic:                     1903.\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):           2.81e-40\nTime:                        08:37:53   Log-Likelihood:                -66.142\nNo. Observations:                  50   AIC:                             136.3\nDf Residuals:                      48   BIC:                             140.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          1.0644      0.258      4.120      0.000       0.545       1.584\nx1             1.9420      0.045     43.622      0.000       1.853       2.032\n==============================================================================\nOmnibus:                        0.453   Durbin-Watson:                   1.942\nProb(Omnibus):                  0.798   Jarque-Bera (JB):                0.608\nSkew:                           0.156   Prob(JB):                        0.738\nKurtosis:                       2.559   Cond. No.                         11.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Regression summary: Set 2 (High variance) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.021\nMethod:                 Least Squares   F-statistic:                  0.001247\nDate:                Thu, 10 Jul 2025   Prob (F-statistic):              0.972\nTime:                        08:37:53   Log-Likelihood:                -275.16\nNo. Observations:                  50   AIC:                             554.3\nDf Residuals:                      48   BIC:                             558.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         33.7690     44.502      0.759      0.452     -55.707     123.245\nx1            -0.1028      2.911     -0.035      0.972      -5.956       5.751\n==============================================================================\nOmnibus:                        4.219   Durbin-Watson:                   2.211\nProb(Omnibus):                  0.121   Jarque-Bera (JB):                3.105\nSkew:                          -0.535   Prob(JB):                        0.212\nKurtosis:                       3.587   Cond. No.                         79.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n</details>\n\nThese results show how when you fit one for all it has low $R^2$ mainly due to the large errors by high variance data. You will miss the signal in the low variance data (p-value for x1 is higher in the fit all model).\n\n# Loss Function, Likelihood, Pearson Correlation\n\n- Under normality and homoscedasticity assumptions, the OLS loss function is equivalent to the negative log-likelihood of the Gaussian distribution.\n- Also it is equivalent to the Pearson correlation coefficient between the predicted and actual values.\n\n\n# Independent Errors Assumption\n\n- The assumption that the errors are independent of each other.\n- statsmodels has a fix for p-values and varaince estimae (still fits normal OLS) but gives different summary results. \n\n```python\nX_const = sm.add_constant(x)\nols_model = sm.OLS(y_ar1, X_const).fit()\nrobust_model = ols_model.get_robustcov_results(cov_type='HAC', maxlags=1)\n```\n\n# Computation and Optimization\n\n**Solving `Ax = b`**\n\n## Direct Inverse\n- Strict inverse using QR decomposition, Q is orthogonal matrix, R is upper triangular.\nYou can solve $Rx = Q^{T}b$ using back substitution. Recursively solving for each element of x starting from the last row up to the first row.\n- $X(X^{T}X)^{-1} y$ is $O(p^3) + O(p^{2}*n)$ run time (matrix inversion is $O(p^3)$)\n- Use Gram-Schmidt  to build QR decomposition \n- $O(p^2*n)$ run time\n- Gram-Schmidt is doing regression on each feature one by one, orthogonally projecting the residuals onto the next feature.\n- Your book \"Elements of Statistical Learning\" has the exact algo.\n\n- In theory you can use Gaussian Elimination to calculate the inverse of A. \nThis is when you \"attach\" the identity matrix to A and do row operations to convert A to identity matrix, the identity matrix will become A inverse.\nThese operations are echelon form and are $O(p^3)$ run time.\n\n## Pseudo Inverse\n- Produces more stable results and is strictly better than direct inverse in terms of forecasts\n- The big-O run time could be larger for well-defined invertible matrices, but in practice it is faster.\n- pseudoinverse instead of blowing up values close to 0 it suppresses them naturally giving stable results\n- **SVD*** decomposition is used to compute the pseudoinverse\n- `A = U @ SIGMA @ V^T -> A^+ = V @ SIGMA^+ @ U^T`\n- U and V are orthonormal matrices\n\n\n### Decomposition Methods\n- LU decomposition: $A=LU$ where L is lower triangular and U is upper triangular. Used to solve Ax=b by solving Ly=b and then Ux=y (simpler than taking the inverse directly)\n- Cholesky decomposition (subcase of LU decomposition): $A=LL^T$ for symmetric positive definite matrices.\n- QR decomposition: $A=QR$ where Q is orthogonal and R is upper triangular. Used to solve Ax=b by solving Rx=Q^Tb.\n- SVD decomposition: $A=U\\Sigma V^T$ where U and V are orthogonal and Σ is diagonal. Used for pseudoinverse and dimensionality reduction.\n\n\n\n#TODO check how to implement Gram-Schmidt in numpy/scipy.\n\nI tested in practice, the Gram-Schmidt was slower, though.\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```python\niimport numpy as np\nimport time\nfrom numpy.linalg import inv, solve\nfrom scipy.linalg import qr\n\n# Function for normal equation (direct inversion)\ndef normal_equation(X, y):\n    XtX = X.T @ X\n    Xty = X.T @ y\n    beta = inv(XtX) @ Xty\n    return beta\n\n# Function for QR decomposition\ndef qr_solution(X, y):\n    Q, R = qr(X, mode='economic')\n    beta = solve(R, Q.T @ y)\n    return beta\n\n# Simulation parameters\nn = 50000   # number of samples (can increase for stress testing)\np = 200    # number of features\n\nnp.random.seed(0)\nX = np.random.randn(n, p)\ny = np.random.randn(n)\n\n# Run and time the normal equation\nstart = time.time()\nbeta_normal = normal_equation(X, y)\ntime_normal = time.time() - start\nprint(f\"Normal Equation Time: {time_normal:.4f} seconds\")\n\n# Run and time the QR decomposition\nstart = time.time()\nbeta_qr = qr_solution(X, y)\ntime_qr = time.time() - start\nprint(f\"QR Decomposition Time: {time_qr:.4f} seconds\")\n\n# Check the difference in solution (should be very small!)\ndiff = np.linalg.norm(beta_normal - beta_qr) # sum(abs(diff))^(0.5)\nprint(f\"L2 norm of difference between solutions: {diff:.2e}\")\n\n# Output\n'''\nNormal Equation Time: 0.0389 seconds\nQR Decomposition Time: 0.5719 seconds\nL2 norm of difference between solutions: 5.85e-16\n'''\n```\n</details>\n\n\n# Feature Selection\n- Forward or backward selection methods are O(n^2)\n- Can use regularization for feature selection\n- In practice if you have many features that are noisy or collinear, regularization will help with forecasting performance.\n- however if you choose the **top correlated** with the target features that could outperform fitting regularized LR on all features,\nsince you'd try to fit noise. Lasso might remove the noisy features but it still struggles.\n\n# Regularization\n\nRegularization can be used in two ways: \n- to reduce the variance of the model\n- to perform feature selection\n\nFeature selection and regularization can be connected\n- They can be seen as L0 regularization (penalizing the number of non-zero coefficients)\n- L1 and L2 regularization are convex problems. The loss with regularization is a SUM OF CONVEX functions!\n- Minimizing the loss functions is equivalent to solving a \nconstrained optimization problem = minimizing OLS + L1/L2 constraint on the coefficients.\n- when you think of the constrained problems you can draw the OLS loss and the L1/L2 constraints and see where they intersect.\n- L2 constraint is a circle, L1 is a diamond. \nThe corners of the diamond are more likely to intersect with the OLS loss contours leading to sparse solutions.\nIn multiple dimensions the L1 constraint is a hypercube with many corners.\n\n**Solutions:**\n\nIf X is orthonormal, then the solutions to lasso and ridge are:\n- lasso: sign(beta) * max(|beta|-lambda,0)  where beta is the solution to OLS\n- ridge: beta / (1+lambda)\n\n## Ridge\n- Ridge (L2) regularization is a scaling regularization (divides the OLS solution by a factor (1+lambda))\n- Ridge regularization is PCA regression with soft-threshold. It shrinks the coefficients of the principal components.\nIt shrinks more the coefficients with low variance (less important components)\n- Ridge is equivalent to maximizing the posterior mean with prior beta ~ Normal(0,rho). then lambda = sigma/rho (sigma is the error variance)\n\n\n## Lasso\n- Lasso (L1) regularization \n- sparse solutions (think of the constraint problem is a diamond)\n- convex problem - no closed form solution - use coordinate descent, stochastic descent or LARS\n- Lasso is equivalent to maximizing the posterior mode with prior beta ~ Laplace(0,b). then lambda = sigma/b (sigma is the error variance)\n\n\nSay you have MSE with constraint |beta| > 5, then this is NOT convex - you could be on the wrong side of the parabola.\n\n\n\n# Log Scale\n\nMake better visualizations when there is an outlier in the target variable.\n\nNote how the y axis does not have equally distributed points. from 10^1 to 10^2 is the same length as from 10^2  to 10^3 \n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample data: 20 points lying roughly on a line, plus one big outlier\nnp.random.seed(0)\nx = np.arange(1, 21)\ny = 2 * x + 1 + np.random.normal(scale=2, size=x.size)\ny[-1] = 1000  # outlier\ndf = pd.DataFrame({'x': x, 'y': y})\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n# Linear scale scatter plot\nsns.scatterplot(x='x', y='y', data=df, ax=axs[0], color='b', marker='o')\naxs[0].set_title(\"Linear scale\")\naxs[0].set_xlabel(\"x\")\naxs[0].set_ylabel(\"y\")\n\n# Log scale scatter plot\nsns.scatterplot(x='x', y='y', data=df, ax=axs[1], color='r', marker='o')\naxs[1].set_yscale('log')\n# equivalent to:\n# axs[1].scatter(df['x'], np.log(df['y']), color='r')\n\naxs[1].set_title(\"Log scale (y-axis)\")\naxs[1].set_xlabel(\"x\")\naxs[1].set_ylabel(\"y (log scale)\")\n\nplt.tight_layout()\nplt.show()\n```\n</details>\n\n- When all points are close together (except for the outlier), the outlier dominates the linear y-range, which squeezes the other data against the axis.\n- With log scale, both the regular points and the outlier are shown in proportion, so you can see ALL points—even values that differ by orders of magnitude.\n\n\n![alt text](./assets/images/log_scale_viz.png)\n","n":0.019}}},{"i":106,"$":{"0":{"v":"Kaggle winning solutions","n":0.577},"1":{"v":"My notes on past winning solutions in Kaggle competitions from [notebook](https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions/notebook).","n":0.302}}},{"i":107,"$":{"0":{"v":"Kaggle Tricks","n":0.707},"1":{"v":"[Hands-on course in Kaggle](https://www.kaggle.com/learn)\n\n\n\n\n# All is in the right scaling\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Make reproducible\nnp.random.seed(42)\n\n# Generate data\nx = np.linspace(1, 100, 100)\ny = 2 * x + 5 + np.random.normal(0, 10, size=x.shape)\n\n# Add outlier\ny[50] += 30   # Outlier at index 50\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1,2,1)\nplt.scatter(x, y, alpha=0.7, label='data')\nplt.title('Original')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n```\n**Can you catch the outlier?**\n\n![alt text](./assets/images/catch_out.png)\n\n\n**Same as above but with  np.log(y)**\n\n![alt text](./assets/images/catched_out.png)\n\n# Videos\n\n[Kaggle Grandmaster Interviews](https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/372629)\n\n---\n\n[Chris Deotte interview](https://www.youtube.com/watch?v=QGCvycOXs2M)\n\nThere is alwasy a data property, barrier - breakthrough in the competition which makes the difference in the leaderboard. Find a feature which could change your modelling approach.\n\nTo do very very good you need to make a breakthrough. To have a breakthrough the secret is in the EDA. Make lots of plots, check correlation between things and eventually you get an intuition. It is all about the data.\n\nKey components in Kaggle competitions:\n- have a fast pipeline where you can run experiments (need to set a good local validation)\n- RIGID EDA, to get intuition of the data\n\nSee something in the plots and you add a feature, split your model, modify the feature.\n\n---\n[Beyond Feature Selection](https://www.youtube.com/watch?v=gdyuCqmLMUg&t=944)\n\n**Is more features better? should I just get more and more features?**\n\nMore features is generally better. As long as your features bring novel information it is good.\n\nDecision trees have built in Feature Selection. for each split you choose the best split - it is a feature selection algorithm.\n\n**So do we really need FS when training Decision Trees (LGBM, XGBOOST)**. The answer is yes. You still need. It does happen your model on the sample data might pick up noisy feature. So you still need FS.\n\nFeature selection can reduce the number of features and you can get your AUC/Score up by a few points.\n\n`scikit-learn`\n\n- Univariate analysis: \n    - variance threshold\n    - F-test (ANOVA)\n    - Forward selection in simple Ridge regression\n- RFE (recursive feature elimination) a Kaggler says it works well\n\n\nThere is some bias in the feature selection. If you do FS for Xgboost, then the features you et woud be good for the xgboost but might not be for Neaural Nets\n\n`Beyond scikit-learn`\n\n- Boruta\n    - Wrapper\n    - Loss based\n    - Subset\n- ReliefF\n    - Filter method\n    - Distance based\n    - Ranking\n\n**Characteristics of Feature Selection Algorithms**\n- Feature space\n    - Univariate F-Score\n    - Multivariate (RFE, L1-based, Tree-based)\n- Direction\n    - Forward\n    - Backward\n    - Ranking: F-score, L1-based\n\n**Most of the time in Kaggle competitons I spent 90% of the time on FS, FE, prepping the data.**\n\n\n**Feature engineering is usually better that Feature Selection**\n\n---\n[CPMP talk](https://www.youtube.com/watch?v=VC8Jc9_lNoY&t=408s)\n\n\nTarget Engineering - transform the target and make predictions on it to match the competition metric.\n\nSMAPE for example is assymetric metric. If you transfor the target with log, you get a metric which is almost like MAE.\n\nUnderstand the problem that is being solved. Understand the metric. Understand the data.\n\nHow good a feature is depends on the competition metric!!! Feature importance, correlations, etc. are not that important.\n\nSetup good Cross validation local setup. You CV score should correlate with the public LB.\n\n**Track the Train-Test error GAP** Large gap indicates overfit. When your CV score is increasing and the gap is not incrasing a lot, then you are not overfitting.\n\nFor xgboost/LGBM no need to worry about one hot encoding, or nan values. They handle it well.\n\n---\n\n[Feature engineering deck](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750)\n\nCategorical features:\n- large cardinality creates sparse data\n- one hot encoding\n- hash encoding (deals with new varaibles, may introduce collisions), avoids extremely sparse data\n- label encoding (for every categorical variable give a unique numerical ID)\n- count encoding (sensitive to outliers, may add log transform, replace unseen varaibles with 1)\n- LabelCount encoding (rank categorical varaibles by count in the train set, no collisions)\n- target encoding - encode cat varaibles by their ratio of target (becareful to avoid overfit, nested CV)\n- use NN to create dense embeddings from categorical variables\n- give NaN valies an explicit encoding instead of ignoring  (use them only when train and test NANs are coused by the same, or your local CV proves it holds signal)\n- expansion encoding (create multiple categorical varaibles from a single variable)\n\nNumerical features:\n- round numerical variables (form of lossy compression, retain most significant features of the data)\n- sometimes too much precision is just noise\n- binning (by quantiles, plot data to log for good interval binning)\n- scaling (standard Z, MinMax, Root, Log scaling)\n- impute missing (mean, median (robust to outliers), KNN,) add boolean column\n- interactions (substraction, addition, multiplication, divison) Ignore human intuition!, weird interactions can give significant improvement\n- create statistic on a row of data (number of NaNs, Number of 0s, Number of negative values, Mean, Max, Min, Skewness, etc)\n- temporal variables (dates, needs backtesting)\n- turn single features, like day_of_week into two coordinates on a circle\n- for TS, instead of total spend, encoed things like spend in last week, month, etc\n- hardcode categorical features like: date_3_days_before_holidays: 1\n- spacial variables (find closeness between big hubs)\n\nNeural Networks & DL\n- NN claim end-to-end automatic feature engineering\n- FE dying field? No - moves the focus to architecture engineering\n- encode order of samples in dataset\n\n*Applied Machine Learning is basically feature engineering, Andrew Ng*\n\n---\n[Winning gold is easy](https://www.youtube.com/watch?v=XBJ2f68LuO4&t=38s)\n\nA LOT OF TIPS AND TRICKS in this video.\n\nIf something does not work, does not stick to it.\n\nto get a gold model you need to try crazy ideas. Most would not work, but if you get one good one you can get the gold.\n\nTrick for feature engineering:\n- add 1 noisy features\n- compute feature importance for all features\n- features which have higher rank than the noisy feature should generally be good features\n- if your noisy feature is too high in rank, your model is overfitting on noisy features and gives them high rank\n- you need to go through the top ranked features and see which of them your model is overfitting \n\n\n**Feature importance**\n\nDetect overfitting features\n- remove the feature and if CV is not hurt your are good\n- if removing the feature hurts CV, then use transformation on the feature\n\nFeature engineering is a bit of an art. Feature X and Feature Y might be good only if you use both (as they have interation).\n\n\n\nIf your model has many features (e.g xgboost) try using small colsample. Usually you would use 80-90%. But when having MANY features, this way your trees would be almost independent and not correlated.\n\nWorking with NN or linear models, when a feature you have NAN, add additional boolean column.\n\nNN are making feature engineering implicitly.\n\n----\n\n[Giba talk, tips for feature engineering and selection](https://www.youtube.com/watch?v=RtqtM1UJfZc&t)\n\n---\n\n[Kaggle Grandmaster and startup](https://www.youtube.com/watch?v=A8oBphPOliM)\n\n- CPU/RAM management is 1st priority\n- Work on a single model as long as you can (1 week for other models)\n- ML is just function approximation of the real world\n- Tree methods ARE sensitive to noisy/useless features (spend enough time in Feature selection...)\n- In kaggle you do not always have CV LB correlation (you have absolute noise in the LB, or the dataset is very small)\n- Every time the dataset is small - you can do 5 Fold CV, repeated with 20 different CV splits. Models bagged 20 times - 2000 models in total to train just 1 model.\n\n---\n\n[When averaging models in forecasting tasks help](https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/394975)\n\n# LB CV Scores\n\n- Compute correlation between LB and CV scores!\n- Local CV setup to be as close as possible to the Kaggle API.\n\n# Model augmentation\n- Bagging\n    - same model trained on different folds\n    - same model trained on different seeds (if data is very noisy)\n    - same model trained on different features\n    - same model trained on different hyperparameters\n    - different models take weighted average/mean (careful for MAE usually it hurts)\n- Stacking\n- Boosting: train on the errors\n- Use output of one model to be features of another model\n- Detrending\n\n\n# Powerful features\n\n- unbiased feature\n`\nfeatureX - groupby(FeatureY)[FeatureX].mean()\n`\n\n- previous FCT error feature (difference between FCT and TARGET), kind of boosting method\n- rolling aggregates - mean, median, min, max, quantiles, std, kurtosis, skew\n- intercept and slope instead of mean aggregate, that is run linear regression on past samples within a window and put the intercept and slope as features\n- random feature trick\n- neighbors_target_mean_500: The mean TARGET value of the 500 closest neighbors of each row\n\n\n\n# Adversarial Validation\n\nAdversarial Validation is a very clever and very simple way to let us know if our test data and our training data are similar; we combine our train and test data, labeling them with say a 0 for the training data and a 1 for the test data, mix them up, then see if we are able to correctly re-identify them using a binary classifier.\n\nIf we cannot correctly classify them, i.e. we obtain an area under the receiver operating characteristic curve (ROC) of 0.5 then they are indistinguishable and we are good to go.\n\nHowever, if we can classify them (ROC > 0.5) then we have a problem, either with the whole dataset or more likely with some features in particular, which are probably from different distributions in the test and train datasets. If we have a problem, we can look at the feature that was most out of place. The problem may be that there were values that were only seen in, say, training data, but not in the test data. If the contribution to the ROC is very high from one feature, it may well be a good idea to remove that feature from the model.\n\n\n[kaggle post](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n\n\n# Check for covariate shift in train and test\n\n- adversarial validation\n- KS test for the features to see if they are from the same distribution\n\n# Check for target shift in train and test\n\nGood to understand th dynamics of the target variable. If the target variable is changing over time, then you need to be careful with the train/test split. You need to make sure the train and test are from the same time period.\n\n- KS Test\n- T-test for mean shift\n\n\n# Hypothesis Testing / Normality tests\n\nI think KS and normality tests are very sensistive for large datasets. If you have 10k+ observations, then you would get a very low p-value for any test. So you need to be careful with the p-value.\n\nAny small shifts would be detected and would reject the null hypothesis.\n\nSo these tests are useful for small sample sizes. For large sample sizes you'd use the CLT and the t-test.\n\n\n# Kolmogorov-Smirnov test\n\n- The two sample Kolmogorov-Smirnov test is a  nonparametric test that compares the cumulative distributions of two data sets(1,2).\n- The test is nonparametric. It does not assume that data are sampled from Gaussian distributions (or any other defined distributions).\n\n\n## One sample Kolmogorov-Smirnov test\n\nIt takes the difference between the empirical distribution of the sample and the theoretical distribution. Looks at the supremum which should converge to 0 almost surely.\n\n## Two sample Kolmogorov-Smirnov test\nIt takes the difference between the empirical distributions of two samples. Looks at the supremum which should converge to 0 almost surely.\n\nKolmogorov distribution shows the rate of this convergence.\n\n**In practice, the statistic requires a relatively large number of data points to properly reject the null hypothesis.**\n\n1k + points should be ok with error less than 1% (see wiki page).\n\n\n- caveat is that this test is very sensistive, if the median, varaince,shape changes it would have low p-value\n\n**Interpreting the P value**\n\nThe P value is the answer to this question:\n\nIf the two samples were randomly sampled from identical populations, what is the probability that the two cumulative frequency distributions would be as far apart as observed? More precisely, what is the chance that the value of the Komogorov-Smirnov D statistic would be as large or larger than observed?\n\nIf the P value is small, conclude that the two groups were sampled from populations with different distributions. The populations may differ in median, variability or the shape of the distribution. \n\n\n\n## T-test\n\n- parametric method, used when the samples satisfy the conditions of normality, equal variance and independence.\n\nThis test assumes that the populations have identical variances by default.\n\n\n\n## Shapiro–Wilk test\n\nNormality test\n\n```python\nfrom scipy.stats import shapiro\n```\n\nThe null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed.\n\n\n- Better than KS one sample test as there you need to know the parameters of the distribution i.e. mean and variance. Cannot standaradize the data, as you would be going into a circle. you can't standardize by using estimated parameters and test for standard normal; that's actually the same thing.\n- good with small sample sizes ~ 50 observations\n- has more power compared to other normality tests, meaning it can detect deviations from normality more effectively.\n- Not Sensitive to Outliers\n\n\n# Likelihood-ratio test\n\n\n# Change point detection in Time Series\n\nChange point detection marks the locations where the underlying properties (statistical characteristics e.g. mean and variance) of the time series shift abruptly.\n\n- target engineering and prediction switching\n- target splitting, find its components\n\n\n# LGBM, XGBM , CATBOOST\n\n- important hyperparams are usually: learning_rate, num_iterations, colsample, sample, max_depth, n_leaves, early_stopping,  device type\n- feature importance: use num_splits to select features, used as a guide for useful and overfitting features\n- plot correlation between features\n- get_split_value_histogram for each feature (lgbm): do that for the most important features\n- get_leaf_output\n- feature binning of continuous features (max_bin)\n\n\n\n# Feature Selection\nObviously you should only do this if it helps your CV.\n\n**All tricks by Chris Deotte:**\n\n\n- forward feature selection (using single or groups of features)\n- recursive feature elimination (using single or groups of features)\n- permutation importance\n- adversarial validation\n- correlation analysis\n- time consistency\n- train/test distribution analysis\n- random feature trick\n- remove low variance features\n\nOne interesting trick called \"time consistency\" is to train a single model using a single feature (or small group of features) on the first month of train dataset and predict isFraud for the last month of train dataset. This evaluates whether a feature by itself is consistent over time. 95% were but we found 5% of columns hurt our models. They had training AUC around 0.60 and validation AUC 0.40. In other words some features found patterns in the present that did not exist in the future. Of course the possible of interactions complicates things but we double checked every test with other tests.\n\n\n\n\nWhatever feature selection you do **you should separete feature selection from model training.** This avoid data leakage. First do model selection and then model training.\n\n**Wrapper methods**: these are model dependent\n- forward selection\n- backward selection\n- recursive feature elimination (greedy optimization)\n\n**Filter methods**: these are model independent, and involve preprocessing\n- Pearson correlation\n- LDA - linear discriminant analysis\n- ANOVA\n- Chi-Square\n\nThese methods does not remove multicollinearity.\n\n**Embedded methods**: regularization\n- Ridge, Lasso\n\n## Recursive Feature Elimination\n\n1. Rank the features according to their importance\n2. Remove one/or more features - the least important - and build a machine learning algorithm utilizing the remaining features.\n3. Calculate a performance metric of your choice\n4. If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature. \nCan run permutation importance on held out test set [scikit-learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html)\n\nRepeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n\n\n## MDI\n\nTree-based models provide an alternative measure of feature importances based on the mean decrease in impurity (MDI). Impurity is quantified by the splitting criterion of the decision trees (Gini, Log Loss or Mean Squared Error). However, this method can give high importance to features that may not be predictive on unseen data when the model is overfitting Permutation-based feature importance, on the other hand, avoids this issue, since it can be computed on unseen data.\n\nFurthermore, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.\n\nThis problem stems from two limitations of impurity-based feature importances:\n- impurity-based importances are biased towards high cardinality features;\n- impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set (when the model has enough capacity).\n\n## Permutation importance\n\n```python\nfrom sklearn.inspection import permutation_importance\n```\nPermutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is **for a particular model.**\n\nWe measure the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature. A feature is “important” if shuffling its values increases the model **error**, because in this case the model relied on the feature for the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction. \n\nTo highlight which features contribute the most to the generalization power of the inspected model permutation importance should be computed on validation set.\n\n\nSklearn permutation_importance takes as input a fitted model. Thus based on the selected features/weights during training it computes the importance of the features. These features importance show how good are these features for this particular model. \n\n**Another take on feature importance is: to do permutation importance before model training.** This should be done manually (I don't know about a library)\nAdvatange is that is more acurate, disadvantage is that you loose a lot of time in training.\n\n\n**NB**\nWhen two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.\n\nOne way to handle this is to cluster features that are correlated and only keep one feature from each cluster.\n\n\n## Model Class Reliance\n[Paper](https://arxiv.org/pdf/1801.01489.pdf)\n\n\n# Dynamic label smoothing\n\n# Throw complex models on high weight data\n\n# 2 submisssions = bet both on heads and tails\n\n[tricks on Jane Street comp](https://www.kaggle.com/competitions/jane-street-market-prediction/discussion/224079)\n\n\n# Random forecast\n- produce ranomized forecasts multiple time. This is your baseline on what noise is in the competition.\n\n\n# Correlation Screening\n- compute correlation of each feature with the target\n","n":0.019}}},{"i":108,"$":{"0":{"v":"Imbalanced dataset","n":0.707},"1":{"v":"\n# Implications\n1. If the train and test sets are both imbalanced with the same imbalance ratio, you do not need to balance your train data.\n2. If the train set is imbalanced and the test set is balanced, balancing your train data using oversampling would improve your model.\n3. Undersample the majority class when working with an imbalanced dataset only if you want to train faster. Bear in mind that your model score would decrease.\n4. If you decide to change the balance ratio of your train set, then MAE and MSE errors would be almost constant, whereas F1 scores, precision, and recall can vary up to 2%.\n4. Use F1 score **macro-averaged** to give equal weight to all classes in your data.\n5. The default F1 score metric in scikit-learn gives big weight to the majority class, resulting in overoptimistic predictions when working with an imbalanced dataset.\n\n\n\nCommon pitfalls\n\n- Never test on the oversampled or undersampled dataset.\n- If we want to implement cross validation, remember to oversample or undersample your training data during cross-validation, not before!\n- Don't use **accuracy score** as a metric with imbalanced datasets (will be usually high and misleading), instead use f1-score, precision/recall score or confusion matrix\n\nImbalanced dataset techniques:\n\n- random under-sampling = get small subsample of the overrepresented class. Main risk is of great infromation loss.\n- SMOTE (synthetic minority over-sampling technique)\n- cost-sensitive loss function\n\n# What is the imbalanced dataset problem?\n\nWithout loss of generality, we assume that the minority or rare class is the positive class, and the majority class is the negative class. If we apply most traditional (cost-insensitive) classifiers on the dataset, they will likely to predict everything as negative (the majority class). This was often regarded as a problem in learning from highly imbalanced datasets.\n\nCreating a traditional-cost-insensitive classifier has two assumptions:\n- goal is to maximize accuracy/minimize loss\n- train and test datasets have the same distributions\n\n**Under these assumtions, predicting everything negative is the correct thing to do.**\n\nThus, the imbalanced class problem becomes meaningful only if one or both of the two assumptions above are not true:\n- the cost of different types of error is not the same/missclasification cost is not equal\n- the train and test data are different\n\n## When changing the cost-function works\nIn the case when the misclassification cost is not equal, it is usually more expensive to misclassify a minority (positive) example into the majority (negative) class, than a majority example into the minority class (otherwise it is more plausible to predict everything as negative). That is, FN > FP. In this case you should have a loss function which penalizes FN more than FP.\n\n\n## When over/undersampling works\nIn case the class distributions of training and test datasets are different (for example, if the training data is highly imbalanced but the test data is more balanced), an obvious approach is to sample the training data such that its class distribution is the same as the test data (by oversampling the minority class and/or undersampling the majority class).","n":0.045}}},{"i":109,"$":{"0":{"v":"Google Rules of Machine Learning","n":0.447},"1":{"v":"\nThis is a PDF doc containing their best practices for ML projects, concentrating on engineering aspects across the project lifecycle, rather than the details of any particular model or task. It's great for people who have experience building ML models for research/academia but haven't built a production system.\n\n[Rules](https://drive.google.com/file/d/1DF-XwtEVfqSyCK9_OJ1NWiSwvuXpTGAq/view?usp=share_link)","n":0.144}}},{"i":110,"$":{"0":{"v":"Feature Engineering","n":0.707},"1":{"v":"\nQuick [guide](https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection/blob/master/A%20Short%20Guide%20for%20Feature%20Engineering%20and%20Feature%20Selection.md#451-recursive-feature-elimination) for feature engineer and selection.\n\n\n\n# Feature engineering\n\n**TL;DR**\n\n- transformation of current features (taking squares, log etc)\n- apply interactions (multplication and ratio of two other features). When you make recipies it does make sense to add the ratio of the amounts of two ingredients\n- group transforms (you get features that aggregate information across multiple rows grouped by some category, e.g. the average income of a person's state of residence)\n- building-up and breaking-down features (ID 123-45-6789, addresses '8241 Kaggle Ln., Goose City, NV')\n- K-means (use cluster as a categorical feature)\n- cluster distane feature\n- rescaling features (`preprocessing` module in scikit-learn )\n- PCA\n- target encoding for categorical features\n- one-hot encoding (ok if you do not have many different values in a single category)\n\n**Feature importance**\n\nRank features using a feature utility metric called \"mutual information\". The **mutual information (MI)** between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?\n\nMutual information = entropy (amount of information learned).\n\n\n- MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\n- It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\n- The actual usefulness of a feature depends on the model you use it with. \n\n```python\nfrom sklearn.feature_selection import mutual_info_regression\n```\n\n**Interaction plots** using the hue parameter in seaborn. These plots shows you interactions between pairwise features.\n\n\n**General tips:**\n- Linear models learn sums and differences naturally, but can't learn anything more complex.\n- Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n- Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n- Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n- Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.\n\n## K-means\n\nThe motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\n\n\n**K means is sensitive to scale**. Features with larger values will be weighted more heavily.\n\n\n**Cluster distance feature!**\nThe k-means algorithm offers an alternative way of creating features. Instead of labelling each feature with the nearest cluster centroid, it can measure the distance from a point to all the centroids and return those distances as features.\n\n## PCA\n\nClustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data.\n\nPCA is typically applied to standardized data.\n\nThe whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\n\nPCA makes this precise through each component's percent of explained variance.\n\nPCA is a linear dimensionality reduction technique.\n\nPCA use-cases:\n- Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n- Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n- Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n- Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n\n**PCA tips:**\n- PCA only works with numeric features, like continuous quantities or counts.\n- PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n- Consider removing or constraining outliers, since they can have an undue influence on the results.\n\n\n## Categorical features\n\nWe need to encode categorical features into numerical features.\n- group aggregation strategy (use aggregate of the y values to encode the categorical features) e.g. mean encoding\n\nThis strategy is also known as **target encoding**. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\n\nProblem with aggregate encoding:\n1. missing categories in the test set need to be imputed somehow\n2. rare categories would have uncertain calculated aggregated statistics\n\n**Smoothing** in encoding idea.\n\nThe idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.\n```\nencoding = weight * in_category + (1 - weight) * overall\n```\n\n```\nweight = n / (n + m)\n```\nUse **m-estimate** to calculate the weight.  The parameter $m$ determines the \"smoothing factor\". Larger values of $m$ put more weight on the overall estimate, the smoother it is.\n\n```python\nfrom category_encoders import MEstimateEncoder # scikit-learn-contrib package\n```\n\n**Use Cases for Target Encoding**\n- High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\n- Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.\n\n**Tip**\nWhen  using a target encoder it's very important to use separate data sets for training the encoder and training the model. Otherwise the results can be very disappointing!\n\n","n":0.031}}},{"i":111,"$":{"0":{"v":"Diff and diff analysis","n":0.5},"1":{"v":"\n# Diff-in-Diff (Difference-in-Differences)\n\nWhy?\n\nDiff-in-diff is used to estimate the causal effect of a treatment when the treatment is not implemented as a randomized control trial (e.g. A/B testing).\n\nWithin Ad Market Dynamics we primarily use Diff-in-Diff to measure the impact of geo experiments.\n\nWhen to use Diff-in-Diff\nTreatment is not randomly assigned (e.g. Geo experiment ✅  A/B testing ❌)\nOther things were happening while treatment was in effect\nYou can't control for all the potential confounders\nAssumptions for Diff-in-Diff\nTrend in control group approximates what would have happened in the treatment group in the absence of the treatment\ni.e. if the treatment was not implemented, then the two groups would have experienced the same changes. \nHow to calculate Diff-in-Diff\n\n\n\n\nAn Example:\n\nDiff-in-Diff for the treatment is calculated as follows: result is 15. \n\nReference\n- Amazing Youtube video on Diff-in-Diff\n- Blog post with examples and case studies using Diff-in-Diff\n\n\n\n\n\n","n":0.086}}},{"i":112,"$":{"0":{"v":"Deep Learning","n":0.707},"1":{"v":"[The Bible](https://drive.google.com/file/d/1l7_bQJkxtEnFyPXm0y8U8Wy18dY5fqbY/view?usp=share_link)\n\nUseful [repo](https://github.com/janishar/mit-deep-learning-book-pdf) with some exercises","n":0.408}}},{"i":113,"$":{"0":{"v":"DT RF GB","n":0.577},"1":{"v":"\n\n![xgboost_obj.png](assets/images/dt_rf_boosting.png)\n\n\nTo learn a decision tree model, we take a greedy approach:\n1. Start with an empty decision tree (undivided\nfeature space)\n2. Choose the ‘optimal’ predictor on which to split and\nchoose the ‘optimal’ threshold value for splitting by\napplying a splitting criterion\n3. Recurse on on each new node until stopping\ncondition is met\nFor classification, we label each region in the model\nwith the label of the class to which the plurality of the\npoints within the region belong.\n\n\n![decision_trees.png](assets/images/decision_trees.png)\n\n\nFor regression you need:\n- pure nodes (no two leaves with same value, e.g lemons)\n- Output in $\\R$ usually the average of the training points contained in the region\n\n\n\nThe learning algorithms for decision trees in regression tasks is:\n1. Start with an empty decision tree (undivided feature space) \n2. Choose a predictor $j$ on which to split and choose a threshold value $t_j$ for splitting such that the weighted average MSE of the new regions as smallest possible:\n\n$argmin_{j,t_j} (\\dfrac{N_{1}}{N}) MSE(R_{1}) + \\dfrac{N_2}{N}MSE(R_2)$\n\nwhere $N_i$ is the number of training points in $R_i$ and $N$ is the number of points in $R$.\n3. Recurse on on each new node until **stopping condition** is met\n\nStop conditions:\n- max depth\n- minimum number of points in each region\n- compute purity gain and stop when the gain is less than some pre-defined threshold\n\n$Gain(R) = MSE(R) - (\\dfrac{N_{1}}{N}) MSE(R_{1}) - \\dfrac{N_2}{N}MSE(R_2)$ \n\n***See section feature importance*\n\n## Expressiveness of DT\n\nClassification trees approximate boundaries in the feature space that separate classes. \n\n**Regression trees = composition and linear combinations of step functions.**\n\nRegression trees split the feature space into parts where each is a constant value.\n\n![expr_dt.png](assets/images/expr_dt.png)\n\n\nTo capture a complex decision boundary (or to approximate a complex function), we\nneed to use a large tree (since each time we can only make axis aligned splits). Large trees have large variance and overfit.\n\n## Bagging\n\nAdjust for the high variance using **bagging**. Run multiple times DT and average out the results.\n\nThe same idea can be applied to high variance models:\n1. (Bootstrap) we generate multiple samples of training\ndata, via bootstrapping. We train a full decision tree on\neach sample of data.\n2. (Aggregate) for a given input, we output the averaged\noutputs of all the models for that input.\n\nFor classification - use voting.\n\nBagging = Boostrap + Aggregating\n\nNote that bagging enjoys the benefits of\n1. High expressiveness - by using full trees each model is able to approximate complex functions and decision boundaries.\n2. Low variance - averaging the prediction of all the models reduces the variance in the final prediction, assuming that we choose a sufficiently large number of trees.\n\nDrawback of bagging (and other ensemble methods) is that the averaged model is not interpretable.\n\n# Out-of-bag error\n\nThe bootstrap step in Bagging has small out of bag sample (with replacement sampling) To compute OOB error:\n\n\n1. Find all models (or trees, in the case of a random forest) that are not trained by the OOB instance.\n2. Take the majority vote(average for regressions) of these models' result for the OOB instance, compare it to the true value of the OOB instance.\n3. Compile the OOB error for all instances in the OOB dataset. Average out OOBs.\n\n## Random forests\n\nIn practice, the ensembles of trees in Bagging tend to be highly correlated.\n\nSuppose we have an extremely strong predictor, $x_j$ , in the training set amongst moderate predictors. Then the greedy learning algorithm ensures that most of the\nmodels in the ensemble will choose to split on $x_j$ in early iterations.\n\nThat is, each tree in the ensemble is **identically distributed**, with the expected output of the averaged model the same as the expected output of any one of the trees.\n\n**Bagging improvement**. The variance of the mean $B$ identical but not independent decision trees $X_1 ... X_B$ is $\\dfrac{1}{B^2}(\\sum var(X_i) + \\sum_{i!=j} cov(X_i,X_j)) = \\dfrac{1}{B}\\sigma^2+\\dfrac{B-1}{B}\\rho\\sigma^2 = \\rho\\sigma^2 + \\dfrac{1-\\rho}{B}\\sigma^2$, where $\\rho$ is pairwise correlation. \n\nThe larger $B$ the lower variance. As $B → \\infty$,bagging variance is bounded by correlation term.\n\n**Random Forest** is a modified form of bagging that creates ensembles of independent decision trees.\n\nTo de-correlate the trees, we:\n1. train each tree on a separate bootstrap sample of the full training set (same as in bagging)\n2. for each tree, at each split, we randomly select a set of J predictors from the full set of predictors. From amongst the $J$ predictors, we select the optimal predictor and the optimal corresponding threshold for the split. [random subspace method](https://en.wikipedia.org/wiki/Random_subspace_method) reduces correlation between estimators.\n3. ensemble all trees using average/majority function\n\nRandom forest models have multiple hyper-parameters to tune:\n1. the number of predictors to randomly select at\neach split\n2. the total number of trees in the ensemble\n3. the minimum leaf node size\n\nTuning random forests:\n\nUsing out-of-bag errors, training and cross validation can be done in a single sequence - we cease training once the out-of-bag error stabilizes.\n\n**Weak spot of RF:**\n\nWhen the number of predictors is large, but the number of relevant predictors is small, random forests can perform poorly.\n\nIn each split, the chances of selected a relevant predictor will be low and hence most trees in the ensemble will be weak models.\n\n**On number of trees in enseble algo**\n\nIncreasing the number of trees in the ensemble generally does not increase the risk of overfitting. Again, by decomposing the generalization error in terms of bias and variance, we see that increasing the number of trees produces a model that is at least as robust as a single tree.\n\nHowever, if the number of trees is too large, then the trees in the ensemble may become more correlated, increase the variance.\n\n**Variable importance for RF**\n\nRecord the prediction accuracy on the oob samples for each tree.\n- Randomly permute the data for column $j$ in the oob samples the record the accuracy again (make it unmeaningful).\n- The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable $j$ in the random forest. \n\n**Xgboost** has an implementation of random forests [link](https://xgboost.readthedocs.io/en/stable/tutorials/rf.html).\n\n# Gradient Boosting\n\nRandom forests and boosted trees are really the same models; the difference arises from how we train them.\n\nBy increasing the number of trees $B$ and reducing the pairwise correlation $\\rho$ we try to reduce the variance of the model. \n\nRather than reducing variance, we can aim to reduce bias of simple trees and make them more **expressive**. Boosting is another ensemble method which achieves that.\n\nAdd weak models $T_{i}$ additively and iteratively to ensemble a linear combination $T = \\sum w_{i}T_{i}$  whis is expressive.\n\n\n![gradient_boost.png](assets/images/gradient_boost.png)\n\nEach simple model $T_i$ we add to our ensemble model $T$, models the errors of the previous version of $T$.\n\nNote that gradient boosting has a tuning parameter, $\\lambda$ - step size(learning rate). \n\nGoal is to minimize objective function, computationally you compute partial derivatives set them to 0 and compute the stationary point. If the objective function is convex, then the stationary point is the min.\n\n\nSubtracting a $\\lambda$ multiple of the gradient from $x$, moves $x$ in the opposite direction of the gradient (hence towards the **steepest decline**) by a step of size $\\lambda$. \n\n![learning_rate.png](assets/images/learning_rate.png)\n\nChoosing $\\lambda$\n- if $\\lambda$ is a constant, then tune through cross validation\n- variable learning rate $\\lambda = g(|f'(x)|)$, function of the gradient of $f(x)$. Around the optimum, when the gradient is small, the learning rate should be small\n\n\n# XGBoost\n\n[Tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n\nXGBoost is a more regularized form of Gradient Boosting. XGBoost uses advanced regularization (L1 & L2), which improves model generalization capabilities. XGBoost delivers high performance as compared to Gradient Boosting. \n\nExtreme grading boosting refers to pushing the limits of computation rather than modelling approach.\n\nOur model is written in the form:\n\n$\\hat{y}_i = \\sum_{k=1}^{K} f_{k}(x_{i}), f_k \\in F$\n\nwhere $K$ is the number of trees, $f_{k}$ is a function in the functional space $F$, and $F$ is the set of all possible CARTs.\n\nThe objective fundtion is given by:\n\n$obj(\\theta) = \\sum l(y_i,\\hat{y}_i) + \\sum \\omega(f_{k})$ where $\\omega(f_k)$ is the complexity of the tree $f_k$ (regularization)\n\n\nWe add the trees $f_k$ additevely/iteratively. For example we learn $f_{t}$ by optimizing:\n\n![xgboost_obj.png](assets/images/xgboost_obj.png)\n\nOne important advantage of this definition is that the value of the objective function only depends on $g_i$ and $h_i$. This is how XGBoost support custom loss functions. \nWe can optimize every loss function, including logistic regression and pairwise ranking, using exactly the same solver that takes $g_i$ and $h_i$ and as input!\n\nLater they define $f_t(x)$ using $w_q(x)$ which are the scores= predictions in each leaf. So the goal becomes to find the best splits and the best scores in each leaf.\n\n**Remember** that our goal is to learn $f_t()$ at learning step $t$.\n\n\n**Model Complexity**\n\nXgboost regularization term tries to minimize the number of leaves in tree as wells as the sum of scores in all leaves.\n\nSCORE of a leaf = prediction of a leaf!!! I read the definition from the paper and this is what it means. Also it can be seen in the tutorial as well as confirmed in [stackexchange](https://stats.stackexchange.com/questions/351872/understanding-regularization-in-xgboost)\n\nEssesntially the final prediction is the sum of predictions/scores of all different trees and xgboost wants to have small contribution by each of the trees. It penalizes when one tree have large prediction.\n\n\n## XGBoost paper\n[Paper](https://drive.google.com/file/d/1wFl7VZuxz1_yPL8p3XNTNLfqNO2_h3EQ/view?usp=share_link)\n\n**Model definition**\n\n![](assets/images/xgboost_defn.png)\n\n![](assets/images/xgboost_loss.png)\n\nThe regularized term balances between:\n- number of leaves in each tree $T$, the smaller number the larger values in each leaf\n- the sum of values in each leaf, the smaller the better (we do not want a tree which has very largerge values)\n\n![](assets/images/xgboost_derivation.png)\nTree boosting has been shown to give state-of-the-art resutls on many standard classification benchmarks. XGBoost is a scalable machine learning system for tree boosting.\n\nThis is how we compute the gain of a node:\n\n![Alt text](assets/images/xgboost_gain.png)\n\n\nXGBoost  major contributions:\n- highly scalable parallel end-to-end tree boosting system\n- regularized learning objective (regularization term is L2 on the leaf scores,i.e. gains). Regularization term in decision trees/boosting trees is measuring  what is the minimum amount of gain that would reduce the loss by the same amount\n- novel sparsity-aware algorithm for nan-values (learn all non-nans, then try put nans on left and on right, chooses whatever gives better gain)\n- weighted quantile sketch for effiecient proposal calculation (this is algo for the first step in approximate split finding). One of the key problems in tree learning is to find the best split threshold.\n\n**Control overfitting**\nWhen you observe high training accuracy, but low test accuracy, it is likely that you encountered overfitting problem.\n\nThere are in general two ways that you can control overfitting in XGBoost:\n\n- The first way is to directly control model complexity.\n    - This includes max_depth, min_child_weight and gamma.\n    - max_depth, min_samples_leaf, min_samples_split in DecisionTreeClassifier in SkLearn (if you have just 1 sample in a leaf or split obviously you are overfitting)\n- The second way is to add randomness to make training robust to noise.\n    - This includes subsample and colsample_bytree.\n    - You can also reduce stepsize eta. Remember to increase num_round when you do so.\n\n## Implementation resources\n\n- [Scikit-Learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html)\n- [XGBoost parameter](https://xgboost.readthedocs.io/en/stable/parameter.html)\n- [Hyper parameter tuning (Bayesian)](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)\n\n\n**Limitation of additive tree learning.**\n\nSince it is intractable to enumerate all possible tree structures, we add one split at a time. This approach works well most of the time, but there are some edge cases that fail due to this approach. For those edge cases, training results in a degenerate model because we consider only one feature dimension at a time. \n\n**NOTE!!!!**\n\n[Xgboost order of features MATTERS](https://beverly-wang0005.medium.com/pitfall-of-xgboost-order-of-features-e651628ab3b7)\n\n\n## Can Gradient Boosting Learn Simple Arithmetic? \n\n[Article](https://forecastegy.com/posts/can-gradient-boosting-learn-simple-arithmetic/)\n\nSee if xgboost can learn simple arithmetic: addition, subtraction, multiplication, division between 2 random variables.\n\n- Learns well addition and subtraction, but not multiplication and division.\n-  need any random disturbance, e.g., some noise or subsample=0.8 would help to kick it off the equilibrium and to start learning.\n- need to add some noise in the data to make it learn multiplication and division\n- if you know there is a strong interaction you need to explicitly add it as a feature, so that your model does not struggle to find thresholds and splits to capture the interaction.\n\n*\"But it's a nice example of data with \"perfect symmetry\" with unstable balance. With such perfect dataset, when the algorithm is looking for a split, say in variable X1, the sums of residuals at each x1 location of it WRT X2 are always zero, thus it cannot find any split and is only able to approximate the total average.\"*\n\n\n# LightGBM\n[Paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n\n\n# Catboost\n\n\n# Adaboost\n\nConnect gradient boosting for regression to a boosting algorithm often used for classification.\n\nUnfortunately error function is not differentiable!\n\n$Error = \\dfrac{1}{N} \\sum I(y_n != \\hat{y}_n)$\n\nReplace the error function with a differentiable function that is a good indicator of classification error. Exponential loss:\n\n$ExpLoss = \\dfrac{1}{N} \\sum exp(y_n \\hat{y}_n)$\n\n\n\n![adaboost.png](assets/images/adaboost.png)\n\n![adaboost_lambda.png](assets/images/adaboost_lambda.png)\n\n# Implemetations\n\nThere are few implementations on boosting:\n- XGBoost: An efficient Gradient Boosting Decision \n- LGBM: Light Gradient Boosted Machines. It is a library for training GBMs developed by Microsoft, and it competes with XGBoost \n- CatBoost: A new library for Gradient Boosting Decision Trees, offering appropriate handling of categorical features\n\n\n# Feature importance\n\n[Article](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c) that explains how enthropy, information gain and decision splits are computed and chosen.\n\nThis is a measure of how much each feature contributes to the model. It is all about coming up with a **Variable selection criterion**.\n\nDecision trees use a top-down, greedy method called recursive binary splitting. Starting from the top, they split the data into two at each step, creating branches. It's called \"greedy\" because it picks the best split at each step without thinking ahead for the overall tree.\n\n\n```\nInformation gain of a node =  Entropy(parent) - Entropy(children)\n```\n\n```\nInformation gain of a node =  f(parent) - f(children) \n```\n\n`f` is a measure of impurity.\n\n`Entropy(node)` measures the impurity of a node. A node with only 1 class is pure and has entropy 0. A node with 50% of each class has entropy 1.\n\n`Entropy(children)` would be computed using weighted average of the entropies of each child node.","n":0.021}}},{"i":114,"$":{"0":{"v":"Curse of Large Dimensionality","n":0.5},"1":{"v":"Проклятието на големите размери...\n\nAn increase in the dimensions can in theory, add more information to the data thereby improving the quality of data but practically increases the noise and redundancy during its analysis.\n\n# Ballpark estimate for number of data points required\n\nSay for the number of points required by ML model to learn any value of a feature is 10.\n\n- 1 binary feature: $2^{1} * 10 = 10$ data points\n- 2 binary features: $2^{2} * 10 = 40$ data points\n\n...\n\n- n binary features: $2^{n} * 10 =$ many data points\n\n\n# Recognize COD\n- Overfitting\n- Sparse features\n- Comptational complexity\n\n# Techniques to avoid COD\n\n- Strict forward-feature selection (add feature only when you see marginal improvement in CV)\n- Feature selection using permutation importance\n- Feature extraction: PCA/t-SNE\n- Regularization\n- Model selection, choosing models that are less prone to overfitting\n- Sample more data - bootstrapping","n":0.085}}},{"i":115,"$":{"0":{"v":"Cross Validation","n":0.707},"1":{"v":"\"Trust your CV score in Kaggle competitions more than the public LB score.\"\n\n- Hold-out (standard one 80/20 split)\n- K-folds (split data into k folds and each fold would be a validation set)\n- Leave-one-out (extreme K-folds)\n- Leave-p-out\n- Stratified K-folds (useful for imbalanced datasets)\n- Repeated K-folds (pick 80/20 split data randomly k times, bad for imbalanced datasets)\n- Nested K-folds: need to implement mannually, good for hyperparameter tuning\n- Time series CV (deals with forwardlooking bias in TS data)\n\n[Description of CV techniques](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right)\n\n```python\nfrom sklearn.model_selection import KFold, GroupKFold\n```\n\n**Nested Cross Validation:**\n![alt text](./assets/images/nested_cv.png)","n":0.108}}},{"i":116,"$":{"0":{"v":"Comprehensive maths behind ML","n":0.5},"1":{"v":"[Pattern Recognition and Machine Learning](https://drive.google.com/file/d/13kYvOT9wtHvlm33tf77F_0qUiygXiItl/view?usp=share_link) explains the mathematics behind the more popular and basic ML models.","n":0.25}}},{"i":117,"$":{"0":{"v":"Code ML from scratch","n":0.5},"1":{"v":"[Data Science from scratch](https://drive.google.com/file/d/13uxFuAwgTGjAwPiKqyBjCz-CVoSXAx_i/view?usp=share_link) has Python implementation of all basic ML/DS models from scratch.","n":0.267}}},{"i":118,"$":{"0":{"v":"Casual Impact","n":0.707},"1":{"v":"\n\nBayesian Structural Time Series\n\nhttps://pypi.org/project/pycausalimpact/\n\nTime series clustering...","n":0.408}}},{"i":119,"$":{"0":{"v":"Journal","n":1}}},{"i":120,"$":{"0":{"v":"USACO","n":1},"1":{"v":"\nUSACO is the US highschool olympiad used for selection for IOI. \n\nI've been doing leetcode for the last 22 months and feel ready to do competitive programming. Thus in Sept 2022, I decided to start with USACO - 25 years old and back to high school...\n\nAs of 1st Oct 2022, I am in bronze division.\n\n8th Oct 2022 - just leaned I could get test casses for the problems... Click on the problem name when open USACO's IDE.\n\n14th Oct 2022 - Moved to Silver. Bronze went well overall. Just 3-4 Very Hard questions are remaining (haven't tried yet.) \n\n23th Oct 2022 - USACO is fucking hard to me. Some teenagers are able to solve these stuff. WTF. Problems are really coold though. Require lots of analysis before writing code.\nThey are very similar to problems at high school math competitions. Also in order to solve harder problems you really need to believe in yourself and keep pushing.\n\n\n\n#Zadachi\n- [cowvid19](http://www.usaco.org/index.php?page=viewproblem2&cpid=1037)\n- [sleepy cows](http://www.usaco.org/index.php?page=viewproblem2&cpid=892)\n- [photo shoot](http://usaco.org/index.php?page=viewproblem2&cpid=1227)\n- [meet in the middle](https://leetcode.com/problems/partition-array-into-two-arrays-to-minimize-sum-difference/)\n- [cow tipping](http://www.usaco.org/index.php?page=viewproblem2&cpid=689)\n- [cow features](http://www.usaco.org/index.php?page=viewproblem2&cpid=941)\n- [drought](http://usaco.org/index.php?page=viewproblem2&cpid=1181)\n- [out of place](http://www.usaco.org/index.php?page=viewproblem2&cpid=785)\n- [paint the barn](http://www.usaco.org/index.php?page=viewproblem2&cpid=919)\n- [gcd on board](https://atcoder.jp/contests/abc125/tasks/abc125_c)\n- [race](http://www.usaco.org/index.php?page=viewproblem2&cpid=989)\n- [rectangular pasture](http://www.usaco.org/index.php?page=viewproblem2&cpid=1063)\n- [lifeguards](http://www.usaco.org/index.php?page=viewproblem2&cpid=786)\n- [stuck in a rut](http://www.usaco.org/index.php?page=viewproblem2&cpid=1064)\n- [splitting the field](http://www.usaco.org/index.php?page=viewproblem2&cpid=645)\n- [diamond display](http://www.usaco.org/index.php?page=viewproblem2&cpid=643)\n- [sleepy cows herding](http://www.usaco.org/index.php?page=viewproblem2&cpid=918)\n- [movie festival ii](https://usaco.guide/problems/cses-1632-movie-festival-ii/solution)\n- [angry birds chain](http://www.usaco.org/index.php?page=viewproblem2&cpid=597)","n":0.071}}},{"i":121,"$":{"0":{"v":"Self Reflection","n":0.707},"1":{"v":"\nEvery week I self-reflect:\n- Things I did well and am happy about this week.\n- Imagine your ideal week ahead. What actions do I need to take to make it happen?\n- What is my biggest waste of time? \n- Write about a situation that didn’t go as planned. What could you do differently next time?\n- What relationships did I nurture or neglect?\n\n[Self Reflection Journal](https://docs.google.com/document/d/19HJA6dOp9uBXOcxRpgY2aTxEhY9qiGNt_JePRYIROK4/edit)","n":0.126}}},{"i":122,"$":{"0":{"v":"Razmisli","n":1},"1":{"v":"\nLife is not a problem to be solved, it is an adventure to be experienced.\n\n\n# Actions\nThese action items have arose after doing razmisli.\n- define clear dream\n- define passion which can bring value to other people\n- find a product which you would be passionate about\n- occasionally check yourself where you are compared to others ('сверявай си часовника'), e.g. blind, forums, articles, friends and family. Don't fall in the trap of just working by yourself. Competition is what drives people and opportunities.\n\n# General\n\nWhat do I want to do with my life?\n\n- In short term, I want to have few career accomplishments - competitive programmer, kaggle expert, successful quant,\nmake some good money.\n- Long term, accomplish dream to own business, entrepreneur, MORE money, takiva raboti.\n\n---\n\nWhat is my dream?\n- I am looking for concrete dream. I know I want to become entrepreneur (tech stuff/fintech probably), but am still searching\nfor the product that I would be passionate about.\n\n---\n\nWhat are my passions?\n- I like solving problems, seeing the green light in coding problems makes me happy. I like maths and solving puzzle/competitive\nquestions. ML is interesting too, though I have not been deep in that. Need to try out Kaggle seriously. \"Mnogo zuburski otgovor\".\n\n---\n\nJivota mi e bolka i zadachi\n\n---\n\nWhat is love?\n\n---\n\nHow to have meaningful and thoughtful discussions?\n\n---\n# Speeches\n\nSteve Jobs graduation speech - [youtube](https://www.youtube.com/watch?v=UF8uR6Z6KLc&ab_channel=Stanford).\nHis three stories:\n- Believe that the dots will connect\n\nHe was adopted when he was born. Six month in college he had no idea what he was doing and did not see value in it. He decided to attend classes which he was interested in! He stayed in college, just did not study what he had to. Studied calligraphy and was super interested in it. At the time of the study he had no idea how it would help him. 10 years later it came back when designing Mac. Cannot connect the dots looking forward but it all comes back! You need to believe that the dots will connect!\n\n- Love and loss - Keep looking, don't settle\n\nHe was fired when he was 30 from his own company. He decided to start over. \"Being fired from Apple was one of the best things happened to me.\" Started his most creative years - founded Next, Pixel, found his wife. The only thing helped him go through this period was that he knew was he loved doing. \"You have to believe in your work. In order to believe in your work, you need  to love it. Don't settle!\"\n\n- Death - thinking about death makes you think 'I have noting to loose, follow your heart'\n\n\"If you live each day as if it's your last, someday you will be right.\" Look in the mirror and ask yourself \"If today is my last day would I do this today.\" Many 'no-s' in a row means you need to change something.\nDeath is the destination we all share. Your time is limited, have the courage to follow your heart and intuition. They somehow know what you want to do - everything else is secondary.\n\n**Stay hungry, stay foolish!**\n\n---\n\nElon Musk: Argue from basic truths vs by analogy\n\nQuestions you should always ask/think about.\n\nTalking with my father is super productive, I learn so much stuff about life. Topics on which I should focus:\n- economics/trade he has great intuition about these stuff\n- general life advices\n- how to talk with people\n- he is always interested in other people and naturally curious\n\n---\n\nNai skupata taina na sveta\n\n---\n\n[The Mars generation is shooting for the stars](https://www.youtube.com/watch?v=I0booXqf5W4&ab_channel=TEDxTalks)\n\n\"Make my dreams turn into a goal\" - Tatyana\n\nPrioritazing my goals drastically changed the circle of people around me. By focusing on my work and studies, I attracted the right people around me.\n\nWhereever you go, you will follow your dream, you will speak the truth, you will be vocal, you will share your story.\n\n\"Imagine that this astronout is you. You are capable of doing everything as long as you believe it.\"\n\n---\n\n[Jeff Bezos in 1999](https://www.youtube.com/watch?v=GltlJO56S1g&t=29s&ab_channel=CNBC)\n\nLong term I believe it is very easy to predict lots of successful companies born of the internet - with very large market caps and so on. I also believe that today, it is very hard to predict which are these companies.\n\nIf you can focus **obsessively** enough on customer experience, they you would have a good chance.\n\nIf it one thing what Amazon is about it is excessive attention to customer experience end-to-end.\n\nIn the long term there is never misaalignment in shareholder experience and customer experience.\n\n\n# Oxford\n\nI made lots of mistakes during my 4 years at uni. When I started I thought too much of myself and that I could do whatever I want. Didn't realize that the true work actually started then... I thought I did so much work at highschool and that success was 'v kurpa vurzano'. Kind of wasted first year completely and half of second year. Had some 'love' distractions which affected my study focus and was pretty bad. Basically these 'love' distractions distracted me for nearly 4 years... Super stupid from my side but well, that's it. Now in 2022 I am fully back. I think I got over the love distractions in 100% around March 2020 (covid started and was last year of uni). At that point in time I have 0 in-person and remote contact with the distraction. Felt good. Felt great.\n\nThe fact that when talking about my Oxford experience I firstly start rambling about 'love' distractions proves I wasted a fair amount of time there. It is a pity to waste Oxford years like that but whatever.\n\nMain mistake I made during these 4 years is that I did not ave a clear and consistent study plan. I was mainly studying in Hilary and Trinity, skipped lots of classes and lectures and did not have a way to measure my progress. I did not track time/work done in almost any way. And then I got average grades and just complained..\n\nAnother mistake is I had some bad influence from toxic people. Should avoid this toxicity as much as possible!\n\nHad a lot of procrastination too\n\nMain mistake is I did not have a plan. Cultural plan, Studying plan, How to live independently plan, healthy lifestyle plan.\n\n---\n\nSIG internship\n\nMalko me pobivat trupki kato se setq za tozi experience\n\n---\n# Fitness and health\n\nIt is not expensive (time-wise and money-wise) to become nacepen. You basically pay fixed cost first 2 months to get nacepen,\nthen you need to pay small regular costs (2,3 times per week). Gym helps me to be more productive when working too. My most efficient work periods have always been when I go to gym regularly too. So it is worth it. Abs is all about diet.\n\n\n\n---\n\n#  I've lost motivation...\n\nHow to bring back your motivation and be productive?\n\n**Goal: Get your high energy state back!**\n\n- Eat healthy/loose weight\n- Go to gym (helps a lot)\n- Listen to motivational/party music\n- Work, work, work\n- Avoid being in states where you lie all day, watch movies/tv-series and play games\n\n**You cannot get depressed if you do not belive depression exists - Andrew Tate hahah**\n\nSongs \n- [Dara - Thunder](https://www.youtube.com/watch?v=ftgcdlazhk0)\n- [Dara - Cold as Ice](https://www.youtube.com/watch?v=D5YbBZpdjAE)\n- [100 Kila - Nqma takava jena](https://www.youtube.com/watch?v=keh-V_DP0y8)\n- [Upsurt - 3v1](https://www.youtube.com/watch?v=tQ5oMD5QDX4)\n- [Sabrina Carpenter - Thumbs](https://www.youtube.com/watch?v=uAVUl0cAKpo)\n---\n# Podcasts\n\n[Garry Kasparov](https://www.youtube.com/watch?v=8RVa0THWUWw&ab_channel=LexFridman)\n\nFear of mistakes guarantees making mistakes.\n\nKey in success is not fear of mistakes or love of winning. It is a passion to make a difference.\n\n\"I always think that the outcome of the game depends on my gameplay\"\n\nMachine will beat human in closed systems like chess.\n\n[Mark Zuckerberg](https://www.youtube.com/watch?v=5zOHSysMmH0&ab_channel=LexFridman)\n\nhealth, loving familiy and friends, **making time for friends**\n\n**be excited for something in the future**\n\nneed to have something you are looking forward to\n\nhiring heuristic: Hire somebody only if you would be happy to work on them.\n\n[Ray Dalio](https://www.youtube.com/watch?v=TISMidxdZoc)\n\nKnow yourself - https://principlesyou.com/\n---\n# Managing your work and setting goals\n\nYou have underestimated planning and project management. For example when you started using Jira tasks, roadmap, backlog, sprints, these have increased your productivity significantly. Being able to track what you do is super important. Writing things down is highly underappreciated. Writing when and what you achive and seeing it regularly adds lots of value and helps for conistency.\n\n---\n\nGetting in FAANG has lots of learning value. Will have the chance to learn super many new stuff. Meta publishes papers every week, including in big conferences like NeurIPS.\nI should aim for TC + going in such places with lots of smart people. Stefan seemed quite impressed by Meta tools and leaning stuff so it must be really good stuff.\n\n\n---\n\n# TC\n\nHigh TC factors/variables are:\n- interview skills: getting interviews + doing interviews (behaviour + technical)\n- your skillset and whether you can do the job (this is important but interviewing skills are more important to get high TC)\n- YOE\n- market\n\nTaka kakto za kasierka davat 2k ne zashtoto izkarva na firmata 2k+, taka v tech plashtat mn pari za gluposti.\n\nMain TC factor is interview skills. It is similar to rising in power in democratic countries. Main skill is marketing/populism and winning elections. Other things such as economics knowldge, finance, politics, leadership, giving speeches, diplomacy are secondary. It is all about winning elections. High TC is all about cracking the interviews.\n\n---\n\nThis year tech companies struggle. High interest rates means high cost to loan money.\n\nTech companies don't do lending. Lots of regulations, huigh buy-in.\n\n\n---\n\n# Blind\n\nWhich is more prestigious DS vs SDE? 13 vs 73\n\nGetting 200k in DS vs SDE is easier? 12 vs 57\n\nCan get from DS to SDE, yes vs no? 42 vs 4\n\nWhich career makes more more on average, DS vs SDE vs the same? 27 vs 135 vs 23\n\nDoes IDE matter Language specific vs general vs the same? 39 vs 14 vs 23\n\nWhich is better on CV, Yelp vs C1? 89 vs 45\n\nTop 5% TC-wise in London, 2YOE.\n300k+ 31 votes, 200k 15 votes, 150k 10 votes, 100 9votes, 120 8 votes, 250k 8 votes\n\nLondon TC-s for 2 YOE in hedge funds are around 200k (even sde-s can get that). Quant devs, Quant analyst/ research are coser to this. \n\nMeta TC Sanfrancisco is likley to be 300k for 2 YOE (E5)\n\n---\n**On job hopping**\n\nJob switching requires work which a fair number of people aren't willing to do.\nWe have all met people who have worked the same job for 10 years etc...\n\nCompanies know this and accounting departments have realized that people are willing to exchange the feeling of \"comfort\" (but not actual job security) of being in the same job for less than market compensation. Its optimizing for people's inertia.\n\nThis is why this community exists as it allows us in the industry the tools to collaborate to ensure we make the best TC we can.\n\nThis trend of needing to job hop extends far beyond tech and really is the economy as a whole. The boomer mythos of working the same job and showing loyalty to move up is well and thoroughly dead but it's remnants still exist and seep into people's perceptions.\n\nSo now that you realize this. Go out there and apply, it's the only way to truly determine your market TC. And take this lesson to heart, you owe companies nothing, it's simply an exchange of your time for more money.\n\n---\n\nМотивацията е силно надценена. Постоянството е ключа.  Изгради система от малки задачи/навици, които да те изведат до успеха.\n\n\n---\n\nFinancials\n\nP/E ratio\n\nEPS ratio\n\nQuarterly earnings\n\nhow to read balance sheet\n\n---\n\n# Communication\n\nConversations based on sarcasm, \"vsichko e na buzik\" mindset is a powerful framework. Your starting point in having conversations with people is not to think about yourself (or proving yourself), rather think that everything is a joke.\n\n# Покупка на жилище\n\nВ София апартаментите ще стават все по-недостъпни. Ще стане като Лондон, където масово хората живеят под наем, защото не могат да си позволят апартамент. След 20-30 години ще стане такаи в София.\n\n# Dealing with rejection\n\nJob hunt rejection. It is not personal, when they say it is not a good fit, it indeed is is not a good fit! And probs you did not do well enough on the interview. Just move on and apply at a bunch of places.\n\nAsking out girls and when they say no. She did nothing to you. You made an offer and she declined. As simple as that. Just move on.\n\n# Scaling tech business\n\nAvoid premature optimization. Check how levels fyi scaled to million of users using google sheets.\n\n[article](https://www.levels.fyi/blog/scaling-to-millions-with-google-sheets.html)\n\nsad\n# Birthday wishes\n\nYour birthday is the perfect excuse to get drunk on a weekday. Bottoms up!\n\n\n“Happy birthday! Your life is just about to pick up speed and blast off into the stratosphere. Wear a seat belt and be sure to enjoy the journey. Happy birthday!”\n\n\n\nCongratulations on being even more experienced.\n\n“Wishing you a beautiful day with good health and happiness forever. Happy birthday!”\n\nHappy Birthday Amy! Congratulations on being even more experienced. Wear a seat belt and be sure to enjoy the journey of life. I wish you a wonderful time ahead!\n\n\nBest wishes on your birthday! May you have maximum fun today, and minimum hangover tomorrow!\n\nHope your special day brings you all that your heart desires! Here’s wishing you a day full of pleasant surprises! Happy birthday!\n\n# Conversation Q&A (Chit Chat)\n\nHow are you doing?\n\nTell me about yourself\n\nWeather talk.. What’s your favorite season and why?\n\nDo you have any pets? What are their names? What’s your favorite animal?\n\nWhat were you obsessed with when you were a child? \n\nWhat’s your most controversial opinion? \n\nDo you have any brothers or sisters?\n\nWhat food would you eat every day if you could?\n\nWhat music do you listen to?\n\nWhat are you going to do this weekend?\n\nWhat do you do to relax?\n\nWhat’s your favorite thing about your hometown? \n\n\n\n\n# Story telling\n\n## In DS presentation\n\n**Template for graphs/figures:**\n\nOn this graph you can see.\n\nThe message you should take out of the graph....\n\nWhat this figure shows you is that...\n\nThe implication that you get from this graph is...\n\n\n\n**Structure**\n\nIntro: \n- set expectations\n- quick mention of an appropriate joke\n\nMid game:\n-  Between switching topics, ask if there are any questions!\n\nEnd:\n- Thank you for your attention. I am happy to answer any questions you might have.\n\n\n## Taking time to answer questions\n\nQuick stalling: \"This is a good question. I have not thought about it before. Let me think about it for a moment.\"\n\nUnderstand the question/take more time: Rephrasing the question: \n- \"From my understanding, you want to know...\"\n- \"What you are asking is whether...\"\n- \"In other words...\"\n\nTake things off-line:\n- I don't have the answer to that question right now. I will get back to you on that.\n- I don't want to give you the wrong information, so let me get back to you.\n\n\n\n# Random thoughts on engineering topics that I have seen or thought about.\n\n- [the pragmatic engineer blog](https://blog.pragmaticengineer.com/)\n\n# Big Tech Revenue Streams\n\nMicrosoft:\n- Azure (31%)\n- MS Office (23%)\n- Windows (13%)\n- Gaming (9%)\n- Linkdin (6%)\n- Search Ads, Bing (5%)\n- 15% Other\n\nMicrosoft portfolio is tech dependent - cloud service, software, OS, games and a bit of ads. Super **diversified**. Less recession dependent.\n\nAmazon:\n- Amazon Store (47%)\n- Third Party eller Service (22%) - shipping, commisions etc\n- AWS (13%)\n- Amazon Prime(7%)\n- Ads (6%)\n\nAmazon portfolio is recession dependent as it depends of everyday spends of people. Amazon is business which depend on everyday retail a lot ~ 80%\n\nApple:\n- Iphone (52%)\n- Services - Apple TV,Music,Pay (19%)\n- Wearables - headphones, hardware (10%)\n- Mac - (9.6%)\n- Ipad - (9%)\n\nApple portfolio is tech retail dependent. Apple has the advantage of being  \"cool\".\n\nGoogle \n- 81 % from ads\n\nMeta\n- 97 % from ads\n\n# Feeling a bit down\nIt is  17th of Sept 2024 - Sofia holiday, but also trung thu hah! I can't believe I'm writing these things down. Haven't done actual journaling for a while... I'm a bit lost on what I should be doing. there are a lot of tasks that need to be done, both in personal developement (professional and emotional) and at work.\n\nI've been eating unhealthy the past months. A lot of tupchene and  just can't stop. I am not fat (yet) just about 77-78 kilos. Bought a flat three months ago and was exciting. My problem is that career-wise I haven't improved much. TC is okish at 160K annually and that is good.  But my problem is that the last 2 years there have been little progress both in terms of TC but also in terms of skills/learnings. I found a girl that I like but it is an LDR - kind of sucks. All is good with her and I feel good about it. There are some yellow/red-ish flags but only time will show if my concerns are unnecessary. I think I will give it a chance.\n\nCareer-wise, my problem solving abilties are more or less the same as 2 years ago. OMG time flies so fast. I learned a bit of Kaggle, just slightly. Need to get back to speed with competitive programming. \n\nPersonal developement: I'm behind on reading lots of books, finance planning.\n\n## My principles to get back on track\n\n1. Do NOT over-eat, drink a lot of water strategy\n2. Regular sports\n3. Track your work/progress: Pomodoro and Jira\n4. Keep focused: turn off WiFi\n5. Personal development: property market, news, self-development books, financial planning\n6. Get enough sleep: sleep earl and get up early!\n\n\n\n\n# Getting in the zone with Pomodoro\n\nThe technique uses a timer to break down work into intervals traditionally 25 minutes in length, separated by short breaks.\n\n\nThere are five basic steps to implementing the technique:\n1. Decide on the task to be done\n2. Set the pomodoro timer to n minutes (traditionally 25)\n3. Work on the task until the timer rings; record with an x\n4. Take a short break (3–5 minutes)\n5. After four pomodoros, take a longer break (15–30 minutes)\n\n\n","n":0.019}}},{"i":123,"$":{"0":{"v":"Leetcode","n":1},"1":{"v":"# Progress\nMy first ever submission in leetcode was on 8th of January 2021. Did some Hackerrank for 2-3 months before that as I did\nnot know how to code (found it hard to press quickly stuff on the keyboard...). Started leetcoding more seriously mid Feb\n2021 and have been doing it for 1.5 years so far (24 Jul 2022). Current leetcode count is solved **1526** problems. Main progress\nstages were:\n- first 3 months good learning curve Feb-Apr, but copy-pasted lost of solutions and failed to solve most of the problems alone. Got good progress on easy questions, mediums were very hard for me, hard were impossible.\n- May rested a bit and slowed down the leetcoding\n- June - August great learning curve, started to solve confidently easy questions, working way through medium alone. Hard\nquestions are still impossible to solve alone, but can understand solutions\n- Sep - Dec stayed put and interviewed at companies so was chilling leetcode and tried to keep up the pace\n- (2022) Feb - July: great learning curve, confidently solved easy and mediums. Started making progress on hards and for\nsome I am able to do by myself. Had one hard asked during interview and managed to finish in 40 minutes with working solution.\n\nIn summary: for now I am on descent coding level for interviews. For competitive programming, its another story... In leetcode I manged to be in top 2% with 2000+ rating, but in codeforces I would be blown out by those guys.\n\n# Plans for the future\nIdeally I would become a descent competitive programmer in 1-2 years (written in August 2022). Need to:\n- do more leetcode competitions\n- learn more advanced topics in CLRS\n- take the MIT advance courses for algorithms and data structures\n- learn C++\n- start codeforces\n- join Google/Facebook coding jams\n\n\nTwo years later Oct 2024 I haven't become a competitive programmer.\n- I've bruteforced Leetcode and focused on quantity over quantity [MISTAKE]\n- Now I'm trying to change that and I really spend time on solving hard problems, thiinking on them and solving them more than one time\n\n\n**23.07.2022**\n\nWent to my 21st competition. Solved 3/4 questions. The last one was hard level but it was super easy if you could get the idea. SUPER easy. I kind of gave up when saw that is hard level and there was big mental barrier. I should work on removing this mental barrier from my head. This increases significantly my chances of solving the question.\nI lost 22 points in rating after the competition (2020 points now). Messed up one of the previous questions as well and got time penalties. Overall, I underperformed.\n\n\n**08.10.2022**\n\nHad 3 bad compatitions and lost a bit of rating. Last competiton went good - solved 4 problems under an hour and got rank around 600. Now rating is 2037. I started doing USACO and am currently in bronze division (easies one). Competitive programming problems seem much harder than leetcode. Some questions in bronze are diffficult,though most of them I am able to solve. But CP is whole another level compared with leetcoding.\nOverall last two months I have been doing standard and a bit more hards unseen problems on leetcode and started competitive programming. Leetcode $\\neq$ CP. In future I would probably need to learn C++, as Python code is sloooow. Also ordered a Leetcode pack (7800 leetcoins). It is flying from China to here!\nI am more confident in solving hard questions in Leetcode now - say 5.5/10. Easy I am 10/10, medium I am 8.5/10.\nCurrent leetcode count **1743** (08.10.2022)\n\n**30.10.2022**\n\nMy best competition performance was yesterday - rank 125 on Biweekly 90. Today I hit new milestone **1800** Leetcode problems solved.\n\n\n**20.01.2023**\n\nLast year in December I hit the **2000** problems solved mark. Now I solve Leetcode casually and focus more on Kaggle.\n\n\n**09.02.2023**\n\nHit another milestone of **2100** problems solved. **Two years of Leetcode.**\n\n**26.02.2023**\n\nHit another milestone of **2200** problems solved.\n\n**03.03.2023**\n\nSolved all easy questions in leetcode and hit another milestone of **2300** problems solved. BG NAD VSICHKO\n\n**06.06.2023**\nHit another milestone of **2405** problems solved. Only hard questions are left :(. \n\n**26.06.2023**\nHit another milestone of **2500** problems solved.\n\n**07.09.2023**\nHit another milestone of **2600** problems solved.\n\n\n# Tracking Leetcode Competitions\n\n- Goal is to become **Guardian rating**\n\n## Recap: Best Learnings\n- The power of solving with pen and paper. The power of the hand.\n\n## Biweekly Contest 141\n- 12/10/2024\n- Solved 3 problems\n\n**What do I learned**\n- Q4 was mathematical and I was rea;ly really close to solving it...\n- binom and comb functions from scipy are not exact for large numbers\n- `comb (exact = True)` !!\n- For the hard problems you will need to do bottom up dp\n- When I do competition I am much more focused and time flies quicker\n\n## Weekly Contest 412\n- 06/10/2024\n- Solved 3 problems\n\n**What do I learned?**\n- I should have pushed further on the idea for problem 3! Was on right track and I knew what was wrong. Instead of trying to fix it I thought that the whole approach is wrong. In hindsight I know that there should have been some repetitive (cycle) idea otherwise the time constraints would be violated. I should have asked myself: \"What are the things that I should definitely use here?\". And after that try ideas for the not definite parts.\n- \n\n\n## Biweekly Contest 138\n- 04/10/2024\n- Solved 3 problems\n\n**What do I learned?**\n\n- Last problem you should have followed your intuition. You started with a small example of two numbers and tried to generalize it. but you made a mistake in the generalizaton. When you played with the example you used the concrete numbers and based on the pattern you tried to generalize. Rather you should have tried to formalize with letters (ti*dj < tj*di) and this would have helped you to come up with a formula for two numbers then this would inspire you to generalize the correct way.\n\n## Weekly Contest 413\n- 03/10/2024\n- Solved 3 problems\n\n**What do I learned?**\n- The third problem was a DP\n- By looking at the problem constrained I should have thought sooner that I must have e problem subspace that is defined on the cell values\n- [Fourth problem](https://leetcode.com/problems/maximum-xor-score-subarray-queries/description/). I was not able to solve it. I was on the right track and I should have looked at the drawing more. Again this problem could be solved only by hand first. Otherwise you'd not be able to solve it.\n\n\n## BiWeekly Contest 140\n- 02/10/2024\n- Solved 2 problems (last two were very hard, 3rd has 16% AR, last one had 9% Acceptance rate)\n\n**What do I learned?**\n- [Third Problem](https://leetcode.com/problems/find-the-lexicographically-smallest-valid-sequence/description/)\n- I tried solving the 3rd problem recursively going through all leaves of the tree. There was no way to DP the problem but I had clear idea of how to traverse the recursion tree\n- Managed to write working solution but it TLE-ed.\n- The reason I had to go from top to bottom of the tree is beacuse I needed forward looking information\n- At this moment I should have started thinking of ways to store such information and fail fast\n- Once I read the solution with the `last` array that stored the last chance of matching a letter in the target string, I could use this array (info) to **greedily** traverse the recursion tree\n- This problem taught me to really understand the examples and whenever there is a choice to be made to be able to clearly formulate the choice (ideally in a programatic way)\n\n\n## Weekly Contest 417\n\n- 01/10/2024\n- Solved all 4 problems\n\n**What do I learned?**\n- Don't be afraid of the hard. You solved the last hard problem without having any idea how to do it at first. You even thought about looking at the solution but you resisted the temptation.\n- [Last problem] When you see thingsdoubleing in a problem, you should immediately think about bit manipulation, binary tree liftin etc\n- [Third problem] writing down examples by hand was very powerful + writing the code etc. You should have this habbit, it will save you a lot of time. Don't just try it out in the code shell. This problem was a sliding window and it really helped to go with by hand.\n- it also helped to come up with extra examples!\n\n## Weekly Contest 416\n\n- 30/09/2024\n- Solved all 4 problems (with help for the last 2)\n\n\n**What do I learned?**\n- Last two problems were sliding window dictionary\n- In order to solve this problem you really had to go through examples with hand\n- Before you write code, you better go thorugh examples with hand. When you rush into coding it actually becomes slowe\n- For sliding windows it is very important to:\n    - get the indexing right\n    - define the window structure, is it just 1 integer?, map + integer,set? etc. \n    - update the window structures correctly\n    - when/how to add to the result\n- come up with examples and WRITE DOWN WITH HAND\n\n## Weekly Contest 414\n\nSolved all 4 problems. \n\n**What do I learned?**\n- Problem 3 [3282](https://leetcode.com/contest/weekly-contest-414/problems/reach-end-of-array-with-max-score/)\n- Play around with example without looking at the expectedoutput. Just do it by yourself! Do no spoil the answer!\n- This way you will likely try out the wrong path and understand the logic for going through the correct path\n- In hindsight the problem looked like a DP, but in the end you were able to solve it using greedy. Often when problem constraints do not allow DP just think of Greedy approach\n\n- Problem 4 [3283](https://leetcode.com/contest/weekly-contest-414/problems/maximum-number-of-moves-to-kill-all-pawns/)\n- Two way DP function, where each DP depends on the other DP\n- Often can see this in game theory problems with two players\n- bitmasking, and just believe in yourself especially for problem 4\n- the proper mindset will help you push through\n\n\n## Weekly Contest 415\n\nSolved only 2 problems. \n\n**What do I learned?**\n- Problem 3 [3291](https://leetcode.com/problems/minimum-number-of-valid-strings-to-form-target-i/description/)\n- I learned how to implement a trie without adding a new class but just using a dictonary.\n- I learned how to adhoc traverse the trie while I was solving the dp\n- I learned in order to traverse the trie on demand it was important how to define the dp problem and its subspace/subproblems\n- I learned that dp problems with string can be solved forward and backwards and they will result in different order of travering the problems. It turned out that going from right to left is better ad I could traverse the trie on demand character by character\n\n\n- Problem 4 [3292](https://leetcode.com/problems/minimum-number-of-valid-strings-to-form-target-ii/description/)\n- I learned that KMP aaplication is ussually not obvious. You need to think a bit to reach to it\n- KMP returns an array that tells you 'what is the longest preffix that is also a suffix'\n","n":0.024}}},{"i":124,"$":{"0":{"v":"Interviews","n":1},"1":{"v":"A whole end-to-end interview cycle lasts about 5 months. Check [[interviews.All interview stages]]","n":0.277}}},{"i":125,"$":{"0":{"v":"The anatomy of the Perfect Technical Interview","n":0.378},"1":{"v":"Notes from: [The Anatomy of the Perfect Technical Interview](http://firstround.com/review/The-anatomy-of-the-perfect-technical-interview-from-a-former-Amazon-VP/) from a Former Amazon VP\n\nInterviewers perspective\n\nResume:\n- Probe: Look for things to push candidates to explain in depth. Ask for examples.\n- Dig: Find out what candidate really contributed toward stated outcomes.\n- Differentiate: we vs I, good vs great, exposure vs expertise, participant vs owner/leader\n\nPast Projects/Accomplishments with enough weight/depth for STAR questions:\n- Situation: Background of what you were working on?\n- Task: What tasks were you given?\n- Action: What actions did you take?\n- Results: What results did you measure?\n\nProjects/teams important to company?\n- Want employees that worked on highest profile products.\n\nInterview Process (in order):\n\nIntro\n- Introduce yourself and clearly state the goal for the interview.\n- Ask the person to introduce themself and give a few minutes about their interests and what excites them.\n\nHands-On Technical Questions\n\n- Examine candidate’s area of focus.\n- If coding, ask coding question based on their experience.\n- Dig into algs, data structures, code organization, simplicity.\n- Use some vague, open ended questions. See if they ask you questions to find out more.\n- Ask a design question, observe how candidate thinks about bigger picture problem.\n\nSoft Skills, Community Fit Questions\n- Question Roseman asks everyone: “Do you consider yourself lucky?”\n- If I asked people you’ve worked with, what three adjectives would they use to describe you?\n- Follow up, for each: What are some examples of when you were [adjective]?\n\n\n\nInterview Technical Questions Advice\n- Don’t ask a question you haven’t asked/tested before. \n- Entire team should sit down and discuss coding questions, answers, and which to use.\n- Multi-part questions, with extra requirements added after each successful answer, help identify candidate’s strength better than many unrelated questions.\n- Know (write down) in advance of asking question what a very good, good, poor, or very poor answer is and why.\n- For one-of-a-kind questions: ask about problems your company faces.\n- Want employees to ask questions, not sit in corner waiting for orders.\n- Create core competencies for company & make sure candidates measure up well.\n\nInterview Advice\n- If you know candidate is no-go 15 minutes into interview, it is still important to get through full interview, to spread a positive image of interviewing at the company.\n- Peak-End Rule? Maybe ending with the hard technical question part creates a worse memory if it doesn’t go well, than starting with it and ending with a less intense background/soft skills questioning.\n- If candidate is a clear hire, selling the candidate on the position is critical.\n- Answer their questions and communicate your enthusiasm about the place & opportunity.\n\nThe Hiring Team\n- Shadowing before giving interviews.\n- Take exhaustive notes on candidate to ensure everyone can articulate thoughts about candidate.\n- After everyone on hiring team meets candidate, deliberate in person.\n- “What we do is put our votes in and then list in detail the questions we asked and the candidate’s answers. If someone is not inclined to hire them because of a pure technical question, then they should list the exact question they asked and the candidates code.”\n- He tells his hiring teams two things: 1) if they can’t provide comprehensive feedback, then they’ve wasted their own time, the company’s time and the candidate’s time; and 2) “If you get to the end of an interview and all you can say is ‘Yeah, I kind of liked them, I think they’d be good,’” then you’ve also wasted everyone’s time.\n- Placing emphasis on the opinions of people who will work with the new employee day in and day out is critical. He says, “no matter what, you never want to let a hiring manager override the group’s decisions. They have to be able to convince everyone, and not by fear.”\n- “The expectation is that the people you hire are better than when you were hired. So that in fact if you left and came back, you might not be hired again in that position. You want to improve your overall bar up with each hire. Another way to put it is every new hire should be better than your average current team member.”\n\n\n","n":0.039}}},{"i":126,"$":{"0":{"v":"Talk with recruiters","n":0.577},"1":{"v":"\n[Tips when talking with recruiters (HR)](https://docs.google.com/document/d/1RcvhsTEIEt9lb8gmT-ozOkU-nnpOdB-n79PcIK0KMY0/edit?usp=share_link)","n":0.408}}},{"i":127,"$":{"0":{"v":"System Design","n":0.707},"1":{"v":"# Resources\n\n- [System Design Interview An Insider’s Guide by Alex Xu ](https://github.com/mukul96/System-Design-AlexXu/blob/master/System%20Design%20Interview%20An%20Insider%E2%80%99s%20Guide%20by%20Alex%20Xu%20(z-lib.org).pdf)\n- [Notes on Alex Xu Book](https://imaginary-basket-cbb.notion.site/System-Design-Interview-By-Alex-Xu-Book-Summary-17d97a868610804dbaf7e81fa7582caa)\n- [System Design Primer](https://github.com/donnemartin/system-design-primer)\n- [Grokking the System Design Interview github notes](https://github.com/Jeevan-kumar-Raj/Grokking-System-Design)\n- drawing tool [escalidraw](https://excalidraw.com/)\n- [TechLead overview](https://www.youtube.com/watch?v=WV2Ed1QTst8)\n- [Blog on System Design topics](https://www.systemdesignacademy.com/), one of its articles was liked by William (IMO best competitor)\n\n\n*Strategy prep*\n- read this notes\n- Anki notes and questions (you have an app)\n- System Design Primer github repo\n- System Design Interview insider book\n\n*Tackle the problem by doing it just like you would at work.*\n\nA 4-step process for effective system design interview:\n\n**1. Understand the problem and establish design scope**\n\nGoal: Gather requirements and clarify assumptions. WRITE THEM DOWN.\nNever start start designing before understanding **requirements**.\n\nExample questions: Product question -> technical question\n```\nDESIGN QUESTIONS\n```\n**Feature/Functional requirements**\n- What are the required features for the system? -> Main **functionalities**?\n- Who are my clients (external, internal, quants, mobile, web, international)?\n- How are they going to use it? -> What **services** clients want to run? (determine API-s functions)\n- Have we built this service before, or I am building it for the first time?\n- What do we want to optimize with our design -> Determine logging metrics\n\n**Non-functional requirements**\n- How many users are there? -> Horizontal vs Vertical Scalability\n- Whats the traffic volume, DAU? -> Single Server setup or MultiServer Setup?\n- Internal or External or Global system? -> Do we need high availability?\n- Available or Consistent data storage? -> Determine tradeoff in CAP theorem\n- How fast do we want to answer requests? -> Latency vs Consistency?\n- How many requests per second do we expect?\n- What is the scale of the system? Is it built for a startup or a big company with a large user base? -> Scalability?\n- Scale of the system such as requests per second (**RPS**), requests types, data written per second, data read per second)\n- Automatic scaling? The addition/deletion of servers should be automatic based on traffic?\n```\nDATA QUESTIONS\n```\n- What are the **I/O** of the system? **Data Flow**\n- **Data Volumes**(if need horizontal scaling use consistent hashing)\n- What **type of data** would I need to store? Just text, or images videos? Static Dynamic content?\n- What is the expected **read to write ratio**, read or write oriented.? (Master-Master, or Master-Slave database)\n- If we need to horizontally scale do we have **evenly distributed data**? Celebrity problem?\n- How long we need to keep the data?\n- If we do historical loads how much back in time should they be?\n- What's the distribution of accessing data? Is a long tail one (suitable for caching popular content).\n- Do we need to our data store to be agile? Change and process in real time?\n- On-prem or in the cloud?\n- What are my endpoints of different parts what APIs would I need?\n```\nImplementation questions\n```\n- Building from scratch? Can we use Third-Party API-s.\n- Can we leverage some of the existing cloud infrastructures provided by Amazon, Google, or Microsoft?\n\n**2. Propose high-level design and engage interviewer**\n\nGoal: Propose blueprint of initial design. Ask for feedback.\n\n\"Does that seem like a sensible strategy?\"\n\nTasks:\n- Do back-of-the-envelope calculations to evaluate if your blueprint fits the scale constraints\nCommonly asked back-of-the-envelope estimations: QPS, peak QPS, storage, cache, number of servers\nQPS = queries per second. 1 day ~ 80,000 seconds\n- Draw box diagrams with key components (Clients (user) mobile/web), APIs, web servers, data stores, cache, CDN, message queue.\n\n\n**3. Design deep dive**\nOnce you agree with your interviewer on the blueprint, go into details on each component. *Design the most critical components first.* This part is specific to the interview and interviewer.\n\n**4. Wrap up**\nInterviewer asks follow up questions. Or you can do some further discussion.\nTopics:\n- Monitoring your solution (analytics to decide whether the solution is good).\n- Performance optimization - e.g do we need multi data center setup using clouod provider\n- Identify system bottlenecks and discuss potential improvements\n- **Recap your design**\n- Error cases (server failure, network loss, etc.) are interesting to talk about.\n- Operation issues are worth mentioning. How do you monitor metrics and error logs?\n- Scale the design - load balancer, horizontal scaling, caching, database sharding\n- Discuss trade-offs:\n```\nPerformance vs scalability\nLatency vs throughput\nAvailability vs consistency\n```\n## General tips\n- gather requirement and write them down (don't be afraid to ask stupid questions)\n- start with simplest possible setup (single server, single database, single storage)\n- iterate, improve crucial parts (database (sharding,replication); storage(AWS S3, replicated buckets); load balancer)\n- make improvements on more important parts first (database usually goes down before server)\n\n\n## Horizontal scaling improvements\nCCARRF\n- loose consistency\n- loose complexity\n- win availability\n- win reliability\n- win redundancy\n- win failover (no SPOF)\n\n## Buzz words\nhorizontal, vertical scaling, availability, failover, reliability, redundancy, consistency, maintainability, load balancer, message queue, asynchronism, workers, data streaming services, database partitioning (split incoming data into different databases based on data rule e.g a-n, n-z), database replication (master-slaves, master-master), sharding, synchronization, parallel computing, SHA code, cache, CDN, Availability Zones (Data Centers), SQL = relational, NoSQL = non relational databases, celebrity problem (in data bases), reads >> writes usually (use cache), volatile memory = temporary memory, memcache (caching SQL) = @cache in python, MongoDB = document database, DynamoDB = key-value store, stateless vs stateful web tier, user session data, VPS(Virtual Private Server) = your own ubunto (EC2 instance) vs shared hosts, hypervisors, sharded database (shards), normalized and de-normalized databases, high cohesion low coupling (scale independently), single point of failure SPOF, Disk slower than In-memory database slower than In-memory cache, ACID properties of a relational database, Transaction, CAP theorem, PACELC theorem (high availability=data replication -> tradeoff between consistency and latency) BLOB (binary large object), BLOB cache, TTL, weak/eventual/strong consistency, cache-aside, cache write-through, cache-write-through, cache-write-back, cache-refresh-ahead, distributed system, multi-server setup, micro-services, database transaction, begin->select-> commit/rollback\n\n# Youtube\n\n[Harvard Scalability lecture](https://www.youtube.com/watch?v=-W9F__D3oY4)\n\nWhen looking for host providers:\n- FTP vs SFTP (secured file transfer protocol)\n- VPS (virtual private server) vs shared hosts(many users use same server)\n- VPS are your EC2 instances in AWS (your own ubunto) safe from other users but still susceptible to the provider\n- VPS uses hypervisors. A hypervisor, also known as a virtual machine monitor or VMM, is software that creates and runs virtual machines (VMs).\n\nLoad balancer strategies to balance:\n- based on work load, least busy servers take the work\n- dedicated servers for specific jobs (html server, Python server etc.)\n- job i is taken by server i % N (problem might not have uniformly distributed workload)\n\nLoad balancer problem? Single Point of Failure.\n- buy 2 load balancers (pretty expensive 100k)\n\nRAID technology is a virtualization of data memory - data many disks put into \"one\" and keep copy/backups to be safe.\n\nStateless architecture to have easier horizontal scaling on the web tier. Keep session data in NoSQL/SQL database shared by all servers (SPOF), better replicate it.\n\nMySQL caches queries, the same query running more than once is cached.\nmemcache is like @cache in Python\n\nDatabase replication - Master-Master, Master Slaves\n\nDatabase partitioning - partition incoming data using some rule (a-n, n-z) and split into different databases\n\n\n[Tech lead](https://www.youtube.com/watch?v=REB_eGHK_P4)\n\nLoad balancing help to distribute traffic to many different servers. Helps scalability, latency, throughput.\n\nNGINX is a load balancing software service.\nCan do DNS load balancing. Make the DNS returns you different ip addresses for same domain distributing to different servers.\n\nThroughput is a measure of how many units of information a system can process in a given amount of time.\n\nCaching - e.b often you'd hitting the database (reads) you need to setup cache.\nCache services Memcached, Redis, Cassandra\n\nCDN-s store static data, global network which caches your content.\n\nIn interview might be asked to design a database schema:\n- what are the primary keys\n- what are your indecies\n\nIndecies in a database make it fast - no need to do linear search in WHERE clause, indecing gives you random access.\nIndexing your table improves the joins - nee to index the column on which you join.\n\nDatabase optimizations:\n\n- replication (master slaves) \n- indexing\n\nScaling stuff:\n- Webservers - use load balancers to do horizontal scaling\n- Image server - CDN, cache\n- Database server - cache, replication (solves reads)\n- Database server writes - Sharding\n\nSharding your database (database partitioning) - horizontal (split rows) and vertical (split different columns) sharding\nEach shard goes to different server.\n\nYou don't want to scale too early and put optimization in the begining.\n\n[Clement System Design interview example](https://www.youtube.com/watch?v=q0KGYwNbf-0)\n\n\n# Databases\n\n## Differences between SQL and NoSQL\n### Storage\n- SQL: store data in tables.\n- NoSQL: have different data storage models.\n\n### Schema\n- SQL\n  - Each record conforms to a fixed schema.\n  - Schema can be altered, but it requires modifying the whole database.\n- NoSQL:\n  - Schemas are dynamic.\n\n### Querying\n- SQL\n  - Use SQL (structured query language) for defining and manipulating the data.\n- NoSQL\n  - Queries are focused on a collection of documents.\n  - UnQL (unstructured query language).\n  - Different databases have different syntax.\n\n### Scalability\n- SQL\n  - Vertically scalable (by increasing the horsepower: memory, CPU, etc) and expensive.\n  - Horizontally scalable (across multiple servers); but it can be challenging and time-consuming.\n- NoSQL\n  - Horizontablly scalable (by adding more servers) and cheap.\n\n### ACID\n- Atomicity, consistency, isolation, durability\n- SQL\n  - ACID compliant\n  - Data reliability\n  - Gurantee of transactions\n- NoSQL\n  - Most sacrifice ACID compliance for performance and scalability.\n\n## Which one to use?\n### SQL\n- Ensure ACID compliance.\n  - Reduce anomalies.\n  - Protect database integrity.\n- Data is structured and unchanging.\n\n### NoSQL\n- Data has little or no structure.\n- Make the most of cloud computing and storage.\n  - Cloud-based storage requires data to be easily spread across multiple servers to scale up.\n- Rapid development.\n  - Frequent updates to the data structure.\n\n**Reasons for SQL:**\n\n- Structured data\n- Strict schema\n- Relational data\n- Need for complex joins\n- Transactions\n- Clear patterns for scaling\n- More established: developers, community, code, tools, etc\n- Lookups by index are very fast\n\n**Reasons for NoSQL:**\n\n- Semi-structured data\n- Dynamic or flexible schema\n- Non-relational data\n- No need for complex joins\n- Store many TB (or PB) of data\n- Very data intensive workload\n- Very high throughput for IOPS\n\n## Common types of NoSQL\n### Key-value stores\n- Array of key-value pairs. The \"key\" is an attribute name.\n- Redis, Vodemort, Dynamo.\n\n### Document databases\n- KeyValue, but values are documents.\n- Documents are grouped in collections.\n- Each document can have an entirely different structure.\n- CouchDB, MongoDB , Dynamo\n\n### Wide-column / columnar databases\n- Column families - containers for rows.\n- No need to know all the columns up front.\n- Each row can have different number of columns.\n- Cassandra, HBase.\n\n### Graph database\n- Data is stored in graph structures\n  - Nodes: entities\n  - Properties: information about the entities\n  - Lines: connections between the entities\n- Neo4J, InfiniteGraph\n\n\n# System Design Interview - An Insider Guide\n\n## Chapter 1 - Scale from zero to million users\n\nThis chapter discusses the process of scaling from a single web server to an architecture with:\nUser, DNS, CDN, Load Balancer, Web tier (Stateless servers), Data Tier (Sharded Databases + Replication), Cache,\nNoSQL database to keep user session data (stateless servers), message queue, logging and metric tools, Data Centers for\ninternational coverage.\n\n• Keep web tier stateless \\\n• Build redundancy at every tier \\\n• Cache data as much as you can \\\n• Support multiple data centers \\\n• Host static assets in CDN \\\n• Scale your data tier by sharding \\\n• Split tiers into individual services \\\n• Monitor your system and use automation tools\n\n![web_traffic.png](assets/images/web_traffic.png)\n\n\n\nSystem Design : https://bit.ly/41PTaAp  \nWinning System Design Case Studies: https://tinyurl.com/yfvyrjn7  \nGithub : https://shorturl.at/hlow0  \nML System Design Series : https://rb.gy/dd8dfe  \nDesign Google Drive : https://bit.ly/3uXdQZ7  \nDesign Messenger App : https://bit.ly/3DoAAXi  \nDesign Instagram : https://bit.ly/3BFeHlh  \nDesign Twitter : https://bit.ly/3qIG9Ih  \nDesign Robinhood : https://bit.ly/3BFeHlh  \nDesign Swiggy: https://bit.ly/3BFeHlh  \nDesign CashApp : https://bit.ly/3BFeHlh  \nDesign Kayak: https://bit.ly/3DoAAXi  \nDesign Paytm : https://bit.ly/3qIG9Ih  \nDesign ESPN Streaming : https://bit.ly/3qIG9Ih  \nDesign Agoda : https://bit.ly/3xP078x  \nDesign Razorpay : https://bit.ly/3xP078x  \nDesign Apple Music : https://bit.ly/3xP078x  \nDesign CricHD: https://bit.ly/3xP078x  \nDesign Alibaba: https://bit.ly/3xP078x  \nDesign Substack: https://bit.ly/3xP078x  \nDesign TrueCaller: https://shorturl.at/pzABN  \nDesign Stock exchange Design System: https://shorturl.at/svCK1  \nDesign Distributed Cache : https://shorturl.at/fjBV6  \nDesign Twilio : https://shorturl.at/uDJP7  \nDesign Google Docs : https://tinyurl.com/mr4d9v83  \nDesign Doordash : https://tinyurl.com/27nh5s7s  \nDesign MS Docs : https://tinyurl.com/9fpa8jpc  \nDesign Zomato : https://tinyurl.com/566t34ph  \nDesign Linkedin : https://bit.ly/3OjXy7c  \nDesign Google Maps : https://bit.ly/3BDdTwn  \nDesign Telegram : https://bit.ly/42N5LW2  \nDesign Snapchat : https://bit.ly/3pRP3pW  \nDesign One Drive : https://bit.ly/438bAwZ  \nDesign Quora : https://bit.ly/3FeD9dL  \nDesign Tinder : https://bit.ly/3Mcyj3X  \nDesign TikTok : https://bit.ly/3UUlKxP  \nDesign Netflix : https://bit.ly/3GrAUG1  \nDesign Uber : https://bit.ly/3fyvnlT  \nDesign Youtube : https://bit.ly/3dFyvvy  \nDesign Reddit : https://bit.ly/3OgGJrL  \nDesign Facebook’s Newsfeed : https://bit.ly/3RldaW7  \nDesign Amazon Prime Video : https://bit.ly/3hVpWP4  \nDesign Dropbox : https://bit.ly/3SnhncU  \nDesign Yelp: https://bit.ly/3E7IgO5  \nDesign Whatspp : https://bit.ly/3M2GOhP  ","n":0.022}}},{"i":128,"$":{"0":{"v":"Stories","n":1},"1":{"v":"Master the art of story telling!\n\n[Stories](https://docs.google.com/document/d/1kgEkr_fAZ5_K35tRXasbgzOWZZ8_QzvwWpSVswbPBZ4/edit?usp=share_link)","n":0.408}}},{"i":129,"$":{"0":{"v":"Solving problems","n":0.707},"1":{"v":"\n# Algorithms\n\n**Steps**\n\n1.  Ask questions:\n    1.1 about input (size, values, all constraints, sorted, duplicates)\n    1.2 find solution -> chekcif there is input data for which the solution does not exist\n2. Play around with a examples (Visualize problem)\n    2.1 Think edge cases (especially linked lists)\nTTD Write tests before writing code!!!\n3. Get brute force solution\n4. Write all steps of the algorithm before actually writing it.\n    4.1 Write interface functions first each, then implement!\n    4.2 Power of abstraction, encapsulate your code by naming\n             different  logics - get your spell!\n5. Test code in you head out loud without running it\n\n\nWhen you are asked about another solution to the problem:\n- recursion vs iteration\n- mention call stack, python is 10000 \n- recursion smart way to procastinate work to be done later\n- recursion cannot control memory\n- can do tail recursion in compiled languages (C, C++, Lisp) reduce memory from O(n) -> O(1)\n- Online vs Offline (Given input upfront vs Data Stream) often for Online you'd use Balanced Binary Search Tree\n\n# Strategies:\n- directly writing code is BAD, think conceptually first- not like you do leetcodede!!!!\n- strategy of wishful thinking- when using Data Structures that are not implemented yet just image you have them, solve the problem and only after that design them\n- linked list questions modularize functions!! [see power](https://leetcode.com/problems/remove-duplicates-from-sorted-list-ii/submissions/)\n- linked list often use two pointers\n-  finding good examples and playing around them\n- for monotonic stack problems better go through example and write steps before that\n\n# Topics\n- BST delete procedure picture of cases from Intro to algo book page 297, dbabichev recursive!\n- power of 2 table 1KB,1MB..\n- latency table in System Design book An insider guide\n- BIT visualization\n- Manachers Algorithm + example to go through + exaplnation of O(n)\n- Morris Traversal of a tree O(1) memory! no stack or recursion\n- Popular questions like Median of two sorted lists, REgular expression matching \nsee in leetcode popular questions for the company you are interviewing\n- Palindrome question patterns: around center check, Manacher, DP (dp(i,j) usually), rolling hash\n-  Stream processing (one way to have conventional interface)\n[enumerate] ---- [map] ------ [filter] ------- [accumulate]\nrange(n)            \nstream creation------ map(fib,stream)------ filter(isodd,stream)-------sum(stream), tuple(stream)\n- Trie Implementation\n\n# Knowledge\n- OOP > Function programming,  Message passing and dispatch is much neater?\noriented programming whose idea of message passing scales better?\n- Functional programming > OOP as it is mathematical. No assingment in Functional\n    and this does not break mathematics. In OOP we use environmenta model of\n    computation to understand the process of program execution. Can be messy.\n- DP two key ingredients: Optimal substructure + Overlapping subproblems.\n 1. Determine subproblem space + choice -> See if there is Optimal substructure\n    ie if an optimal solution to the problem contains within it optimal solutions t subproblems.\n    - To characterize the space of subproblems, a good rule of thumb says to try to\n    keep the space as simple as possible and then expand it as necessary.\n2. Recursive equation\n3. Reconstruct solution by keeping extra decision table in point 2\n- Top- down(Memo) vs Bottom up:\n    if all subproblems must be solved at least once, a bottom-up\n    dynamic-programming algorithm usually outperforms the corresponding top-down\n    memoized algorithm by a constant factor, because the bottom-up algorithm has no\n    overhead for recursion and less overhead for maintaining the table. Moreover, for\n    some problems we can exploit the regular pattern of table accesses in the dynamic-\n    programming algorithm to reduce time or space requirements even further. Alter-\n    natively, if some subproblems in the subproblem space need not be solved at all,\n    the memoized solution has the advantage of solving only those subproblems that\n    are deﬁnitely required.\n- Greedy. How can we tell whether a greedy algorithm will solve a particular \n    optimization problem? No way works all the time, but the greedy-choice property \n    and optimal substructure are the two key ingredients.\n4. Dnamyc bindings in python environmental model\n    4.1 def modify(ls,m):\n                ls[0]= 1\n                m += 1\n                ls= [222,3]\n            m = 0 # m not changed\n            ls = [3,2,1] # changes ls[0] to 1 but does not assign ls, argument  passed by value\n            where the value is a copy of the reference. Similar thing in Java!\n            objects are passed by copy of reference, primitive types ae copy of themselves\n            modify(ls,m)\n5. Binary Search problems often can be solved with Two pointers! They have the same underlying idea of restring the search space in a consistent way\n6. Loop invariant proofs of correctness\n7. Before using Tries - consider using rolling hash of the string\n\n\n# Probability, Brain teasers\n\n- Start with extreme numbers (either very small or very large).\n- Symmetry\n- Bayesian theorem\n- ask as many questions as you need, especially during technical interviews\n- break down approach into small manageable steps\n- \n","n":0.036}}},{"i":130,"$":{"0":{"v":"Self evaluation","n":0.707},"1":{"v":"\n# Capitalab\n- Position: Quant analyst\n- Time: June 2022\n- Reached Stage: Final Round\n- Process: Screen chat, Python take home (poker hands), Technical Screening (1 math + 1 sde chats), Final rounds(met 4 people)\n- Mistakes: Last interview with the CEO (Gavin) could not solve the math question integrate logx... Forgot integration by parts...\n- Reject reason: stupid math question + not enough finance background. They asked lots of option pricing stuff. Volatility\ntrading. Black Scholes is wrong.. Implied volatility\n- Their business: portfolio optimization + compression, greek modelling. very niched business. Rates compression, derivatives,\nsmall company. Swaptionizer, swap on options. Capital Lab is under BGC. optimization problem, loss functions, constraint programming\nhigh paced, entrepreneurial environment,regulations → need for optimization, no regulation = no money for CapitaLab\n\n# World Quant\n- Position: Quant developer - write internal services and automating tools for quants and portfolio managers\n- Time: June 2022\n- Reached Stage: Screening\n- Process: Screening, Hackerrank, Technical screening, 3 x 45 min tech screenings, Final interview with general manager (James)\n- Structure: Quant Researchers + Portfolio managers. Quant Devs support both. Small 6month projects. Simulator written in C++.\n- Tech Stack: Python, C++, Linux, some AWS.\n- Reject reason: Not enough experience\n\n# Trailstone Group\n- Position: Quant developer / researcher. Role is to champion best practices, code a lot, scalable data tools. Good testing,\nmodularity, maintainability, flexible code, no over engineering. Understand whole business, interact with Front Office.\nMain role is in Berlin, there will be all people of the company mainly. Automate management based on scalable technology\n- Time: June 2022\n- Reached Stage: Final Round\n- Process: Screen chat, Hackerrank (algo + pandas), Technical (algorithms with Michael), Final rounds(met whole team )\nThey did not make me quant dev offer and wanted to interview me for quant research (said bullish on making an offer)\n- Mistakes: On final round they asked what problems would you expect to have when developing a trading strategy and\nimplementing it in practice. Quant devs take care of that. Data problem, services, notifications the whole pipeline.\nI did not know this stuff.\n- Reject reason: For QR they asked me about linear regression and how points affect the line. Larger Residuals have larger impact (squared error)... Also the x-axis position of he point matter as well. The farer you are from the mean the larger impact you have on the regression line.\n\nQuestion about optimization problems: Linear regression with beta<5 (yes) is that a convex problem. abs(beta) < 5 is that a convex problem (no - you might be on the wrong side of the parabola).\n\nDual problems, how to know you are on global minimum - maximize your dual problem = minimizing your loss\n\n- Questions: Agile, SQL, Python, Linear regression, Optimization problems\n- Their business: build renewable energy platform, commodity hedge fund\n\n# Hanson Applied Sciences - Smarkets\n- Position: Quant developer\n- Time: June 2022\n- Reached Stage: Offer\n- Process: Screen chat, Hackerrank (algo + math), Technical Screening (1 algo + 1 ML chats), Final round (went to office,\nHR chat + chats with Team + great lunch!)\n- Mistakes: In negotiation stage, when they made me an offer I should have asked **right away** if the offer is negotiable.\nAlways do that, so that you can set expectations for them that you want more. Got in small car/bike accident on the way\nto the interview. Don't put your headphones when driving a bike + be concentrated on the driving when doing it.\n- Their business: Market making on sports betting exchanges\nHanson Applied Sciences – is a from Liquidity provider turned to Market Maker\nPart of Smarkets 140 people/ 70 technical staff\nOnline betting\nQuant dev build production code (close to engineering role)\nteam of 3 quant devs graduate, hedge fund guy, sport betting guy\nTech stack Python(quants), Rust (engineers),\nSBK Sports book keeper is a mobile app goes through their other product which is the exchange\nQuant dev is not customer facing – work with engineers\n- Pricing engine, lots of microservices, service for post trading analysis, no ML yet, something to be created\n- main rival is Betfair.\n- Hanson has two jobs make money, make sure all products are prices (provide liquidity). Lowest margins in the business,\nrely on large volumes. Technical + fundamental analysis.\n- People met: Sean, Qiang, Anthony, Flavio, Sarah, Simon, Ajay\n\n# Google\n- Position: Product Analyst Merchant\n- Time: June 2022\n- Reached Stage: First screening\n- Reason for reject: Talked with recruiter, Had great interview with product analyst, they have not answered me yet,\nI think there is hiring freeze (job ad is gone)\n- Role: product analyst gives direction of general business. No need to be super statistically rigorous. Write lots of SQL.\nPassionate about data. Talked with Haifend Wu in the interview. Shape product in strategic level. Do analysis/metrics, create\ndashboards/segmentation fof google's users.\n\n# Yelp\n- Position: Data Scientist\n- Time: June 2022\n- Reached Stage: Final Round\n- Process: Screen chat, Hackerrank (SQL + Python), Technical Screening (SQL + DataScience (A/B testing)), final interviews\n(Product interview, 1 presentation, 2 Data Science interviews, 1 HR/Leadership chat)\n- Mistakes: Look for keywords, retention, 2 things to measure in these places user experience and money. Key metrics (HEART)\nPresentation talked about ONE project, but could not say whats the final business impact. Overall the 2 Data Science interviews\nwent 50/50 the rest were excellent. Don't interrupt your interviewer! Felt a bit off after the negotiation process. They made\nme good deal and I was hoping to bump a bit more from it, but god bump just 5k. Final offer 120 gbp ~ 160 usd which is good.\nHowever, I felt I could do better and get more money.\n- Role/Company: Tech Stack: Python, some R, Tableau, AWS for compute and storage, SQL. Role is to work with Multi-local\nbusiness (they would be your clients). Unique dataset with lots of interactions\n\n\n# Marshall Wace\n- Position: Data Engineer\n- Time: June 2022\n- Reached Stage: Pre-Final Round\n- Process: Screen chat, Hackerrank (1 easy + 2 mediums), Technical Screening (1 algo (leetcode hard with 20% acceptance)\n+ 1 data chats), Final rounds\n- Mistakes: I could not quickly understand the hints during the data interview. It was simple SQL stuff with data table schema.\nAlgo was leetcode hard and I did it. Communication barrier mistake as I could not get what they meant quickly. Those guys were\ndata engineers but the feel was like they were traders/pretty sharp, young and quick.\n- Role/Company: small/productive teams structure, no BA-s, no PM-s, entrepreneurial, OWN-ership, make your own decisions,\nfast-paced environment, flat structure, serve quants mostly\n\n\n# Atomico\n- Position: Data Scientist\n- Time: June 2022\n- Reached Stage: Second technical round\n- Process: Screen chat, First technical (easy algos), Second technical, Take home, Panel/presentation, CEO chat\n- Mistakes: Weird interview in Jupyter notebooks, working with datasets. Could not click with interviewer and could not\ngrasp quickly his ideas. Gave me data which was generated from sin((x-const)/const) + N(0,1) and I had to determine that.\nDid some visualization stuff, computed correlations, but did not have a structured way to get the final result so I was slow.\nI did not enjoy the experience either. Made mistake not to listen Arlo as I should have. Do not interrupt!\n\n# Shopify\n- Position: Senior SDE\n- Time: June 2022\n- Reached Stage: Schedule for Final Round\n- Process: Technical (algo) super simple hashmap, Life Story (1h chat, went good), Final Stage\n- Reject reason: Hiring freeze\n\n# Point72\n- Position: Data Scientist\n- Time: June 2022\n- Reached Stage: Take home task\n- Process: Hackerrank, Take home DS task\n- Mistakes: Did not heard from them after sending the take home task. It was not really good.\n\n# Akuna\n- Position: Quant Developer\n- Time: June 2022\n- Reached Stage: Final Stage\n- Process: Hackerrank, Technical(algo), Final rounds (presentation algo + math with QRs)\n- Mistakes: On the Hackerrank I was not concentrated and missed some parts even due to time pressure but still passed wtf\n\n\n# HRT\n- Position: Algo engineer\n- Time: June 2022\n- Reached Stage: Second stage\n- Process: Hackerrank, Technical (asked about a sde project, multiprocessing vs multithreading, + hard dp)\n- Mistakes: Could not solve DP\n\nGiven an array of size n and and integer k. We partition the array in k subarrays (some can be empty). Compute each sum\nof the elements in each subarray s1, s2, ... sk. Take the max. Among all partitionings minimize the max value.\n\nMultiway partitioning [problem](https://en.wikipedia.org/wiki/Multiway_number_partitioning#Dynamic_programming_solution)\n\n[Leetcode](https://leetcode.com/problems/find-minimum-time-to-finish-all-jobs/)\n\nNP-hard\n\n# DE Shaw\n- Position software engineer ( the guy said could be 50/50 computer science vs maths)\n- Time June 2022\n- Reached Stage: Second\n- Process: Hackerrank, Technical in coder pad (gave calculator question + implement a queue with an array, table resizing...)\n- Mistakes: so far none?\n\n\n# Jane Street Internship\n- Position: Quant Trader\n- Time: Jan 2019\n#TODO Add document in Staj folder","n":0.027}}},{"i":131,"$":{"0":{"v":"Probability and Statistics","n":0.577},"1":{"v":"variance, covariance formula, correlation, distributions, Linear regression in details, CI, Hypotheis testing, t tests, z tests, pvalue, bayesian vs frequentist statistics, time series stuff, law of total probability/expectation, main theorems - CLT, Markov inequality, LLN, WLLN\n\nSplit in Data Science / Quant / SDE stuff/ Product interview\nQuestions to ask in System Design in Product interviews\n","n":0.136}}},{"i":132,"$":{"0":{"v":"Practice and guide","n":0.577},"1":{"v":"\nCollection of problem solving and system design questions. General guidlines from interviewers perspective.\n\n# Machine Learning Roles\nAlthough not a comprehensive list, ML roles can be split into three roles that complement each other in a typical project lifecycle. These roles are:\n\n**Applied Scientists:** Applied Scientists leverage data to identify and tackle opportunity areas, employing statistical inference and machine learning skills in exploratory as well as production settings to develop new products, optimize business metrics, and perform causal inference.\n\n**Machine Learning Engineers:** Machine Learning Engineers work on engineering ML systems such as Yelp’s real-time ad targeting and search ranking services or batch-based content mining applications.  This involves an understanding of software engineering and system design, data ETL and data stores, and machine learning.\n\n**ML Platform Engineers:** ML Platform Engineers build the infrastructure and tooling to support our Machine Learning Engineers and Applied Scientists.\n\n\n[Role responsibilities](https://drive.google.com/file/d/1DeoYjPHcetPNN4fuyA6_4ih4b2N7QSO9/view?usp=share_link)\n\n\n# Resume screening\n\n[What recruiters/hiring managers look in your CV?](https://drive.google.com/file/d/1djEJvVQOKgPRsp4vJDH6vfAWLXQvvzcB/view?usp=share_link)\n\n**Google** [Resume Tips and Advice](https://www.youtube.com/watch?v=BYUy1yvjHxE)\n\nTop: Programming languages, github profile\n\nFor experienced industry professsional list:\n- First section: Experience should be the bulk of the CV.\n- Second Section or within First Section: Projects\n- Leadership and awards section\n- Final Section: Education\n\nList everything in reverse CChronological order with your most recent experience first. Use action words: created, dsigned, debugged, negotiated,developed, managed.\n\n**Framework**:\nAccomplished [X] as by measured by [Y] by doing [Z]\n\nIncreased server query response by 15\\% by restructuring API.\n\nGrow revenue from small and medium business clients by 1- \\% by mapping new software features as solutions to their business goals.\n\n\n# Your CV\n\nExample LinkdIn profiles of AS/MLE people with no Phd:\n- https://www.linkedin.com/in/shanchaoli/\nA lot of projects. His DS work is closely related to productionizing ML pipelines. Also takes part in business definition of the problems using ML.\n\n- https://www.linkedin.com/in/tianwng/ - Publications...\n- https://www.linkedin.com/in/makeshsreedhar/ - no phd but has showed doing some research internships\n\nMLE:\n- https://www.linkedin.com/in/alex-lo-19528414a/ (DS/DA)\n- https://www.linkedin.com/in/aman3014/ (CS, Computer Vision)\n- https://www.linkedin.com/in/rayan-hatout/ (AI/ML)\n- https://www.linkedin.com/in/thakarar/ (DS, SE)\n- https://www.linkedin.com/in/dtomek/ (Applied Maths + SE)\n- https://www.linkedin.com/in/yunru-l-49157784/(Maths + Engineering)\n- https://www.linkedin.com/in/simone-merello-bb0aa6107/ (CS)\n\n\nLooks like for Meta to become a Research/Applied Scientist you need a phd. Out of 30 profiles I found only 3 without a Phd.\n\nFor MLE, looks like you do not need a Phd. Most of the MLE people have Bachelor or MS. However, they all show proof that they know how to code.\n\n\n**No fluffy things!**\n\nCarefully select which projects to share from your Github (remove snake and ping pong games).\n\n\n# General guidelines\n[pdf](https://drive.google.com/file/d/1kJNLV6ZHrvh6Yu4t20acEAZ7KyYFDm-T/view?usp=share_link)\n\n# How to grade Problem Solving Interviews\n[doc](https://docs.google.com/document/d/1Tj2cpfUaPS514njS1g546Wb6zr4IjMqa/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n# Problem Solving Interview Format\n[doc](https://docs.google.com/document/d/16sBzvNa_pXyO63fmAl2LHtPnwcL7vehY/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n\n# Applied Scientist\n\n- [Resume screening](https://docs.google.com/document/d/1nSMJpsikFKxt-cyk3hjCNW6yavxyMVKL/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n- [MLE/AS leveling](https://docs.google.com/document/d/1CJsiCuI9XYuDbKRc-N11MQ-3Bj49731y/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n# Problems\n\n## ML/AS\n\n[Bigram Tokenizer](https://docs.google.com/document/d/1QWoi4QNgx6ddTNnXOAqryoRcZRdc3csR/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Yelp waitlist](https://docs.google.com/document/d/1ITHwV974iNIB1uMbrXriTJ-aRQLmjljc/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Yelp Beans](https://docs.google.com/document/d/18Cu5Lf0xAPs7_qXTztnrPP-6htKrUaId/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Business Ranking](https://docs.google.com/document/d/1_4kydPYei9HW89d9Tx--TqJn2PCgOL71/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Weighted Random Sampling](https://docs.google.com/document/d/1-h_wBxtu7fsn3fI27bqv5NwYdhRt3rN_/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Most popular business](https://docs.google.com/document/d/1qNrkAMG5NeGn4rYYhC25FK0qHqzrsB0F/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n## Full stack, App Backed, Data Backend\n \n[Questions](https://drive.google.com/drive/folders/1qluCihEEtKKZB0lAMdn3JG35ICmi2NjG?usp=share_link)\n\n## Distributed Systems\n\n[Questions](https://drive.google.com/drive/folders/1XWIl4XLh-pM5TOoXo2dahTgC8ms4HZQo?usp=share_link)\n\n## SRE\n[Questions](https://drive.google.com/drive/folders/1Gg_JIwQYHAAWWcQSJay44DB6F5Hbctzu?usp=share_link)\n\n# ML System Design\n\nAS often don't have engineering backgrounds, so we don't expect them to have depth in infrastructure or engineering oriented areas. Eg, MLEs should know about caching and risks of different online vs offline feature code paths; but that's not required for AS.\n\n\n[Store Visits](https://drive.google.com/drive/folders/1N2Ji--OH5ioazX5hUxBKOWhN231L5eBS?usp=share_link)\n\n[Category ranking](https://docs.google.com/document/d/1X87vVRbs2ZvXn1flFWDmqzAvy-nbyvfx/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Query To Category](https://docs.google.com/document/d/1tYWvARgQz_NswsKSW2n70H5-FmwbQyOl/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[AS Metrics System design](https://docs.google.com/document/d/1BCsm0B3ZLyXvXoYcbMU2DcSn3Cm07MUE/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[AS Ads Experiements Metrics System design](https://docs.google.com/document/d/1JERLeNoqVgKwCYAHhkx_spODITHA6ITU/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n# Programming principles\n\n[doc](https://drive.google.com/file/d/1S2BRa8DYgIUHzMJq85LgaN07_NpXEHlg/view?usp=share_link)\n\n\n# System Design\n\n[Point System](https://docs.google.com/document/d/1qlPM07PqkiO3GkI9PzDERA5lIj0JLUXU/edit?usp=share_link&ouid=108938246171883183662&rtpof=true&sd=true)\n\n[Grading System](https://drive.google.com/file/d/1UpgErSWshnSN2zBxyb0C5pzB-U-g12Oq/view?usp=share_link)\n\nThe focus of the System Design interview is to evaluate whether the candidate can take a somewhat high-level problem, understand the technical requirements and propose a concrete system that solves the problem. While it’s nice if the candidate can solve all parts of the problem, it’s more important for the interviewer to focus on asking good questions and guiding the candidate in order to evaluate the candidate wrt all the criteria that would make them a good “system designer”.\n\nAll interviews follow the same format: a short introduction, followed by a significant amount discussion, brainstorming, and when relevant some \"white-boarding\".\n\n[1-2m] Introduction\nIntroduce yourself and your team.\n\nHow does the work you do impact Yelp?\n\nMention a few things that make you excited about working at Yelp.\n\nGo over the format of the interview:\n\nFew minutes discussing the candidate’s interests\nBulk of the time on a technical problem involving coding and design\nFive minutes at the end for questions about Yelp\n\n[3-5m] Interests\nLearn the communication style and build a rapport with the candidate, so that you can better understand them later on. This also helps make them more at ease with the interview format. Note that information from this section isn’t incorporated into your rating, but you can call out any concerns you observe here in your written feedback.\n\nExplain what this interview is about and that it will include little to no coding but will be more of a discussion and possibly some whiteboarding. \n\nLearn a little of their history from them, past experience, and projects. What types of systems have they worked with?\n\n[35m] System Design\nUse one of the 2 questions from the pool of questions in your track that you have familiarized yourself with. This is rare but if it is not the candidate's first SD interview, make sure that the question you chose hasn’t already been used in the previous interview. \n\nEach system design question has all the details about how to pace the interview, how to nudge and what to focus on. Below are the system design tracks linking to their corresponding SD questions:\n\n[5m] Questions for the interviewer\nAllow a few minutes at the end of the interview to allow candidates to ask you questions.\n\n[Questions Backend](https://drive.google.com/drive/folders/1Owdf6iTqx00FWfN9sYDLBRvFPgJaV0DG?usp=share_link) \n\nFullstack, Data backed questions for SD are the same as Backend questions.\n\n\n[Questions Distributed Systems](https://drive.google.com/drive/folders/1GvXfwIrVJzAWZ5EIAQtrbg2Dd_1XBHRk?usp=share_link)\n\n\n[Questions Security](https://drive.google.com/drive/folders/1SQYLv2yi-vXr0RsEULwtbzvE32VtdU5E?usp=share_link)\n\n[Questions SRE](https://drive.google.com/drive/folders/1NTpW6SfiTOZa3SjozGhVO4nLUgE-D2OF?usp=share_link)\n\n\n# OTC (Ownership, Tenacity and Curosity)\n\n[Questions and evaluation](https://drive.google.com/file/d/1uXcHczNy10snTiuNM9ZwP1UBAgVi0ZoP/view?usp=share_link)\n\n\n# Behaviour (play well with others)\n[Questions](https://drive.google.com/file/d/18brJkuwLdmsy5PtapS39fMSh7ib1GSaF/view?usp=share_link)\n\n\n# Additional Resources\n[Hacckerrank questions](https://drive.google.com/file/d/1T2ptr1gQEJ3AdSb2va0yH4bQIZt_D9rO/view?usp=share_link)\nSee folders:\n- questions\n-  structured questions (System Design)\n\n[Crack Meta MLE interview](https://towardsdatascience.com/how-i-cracked-the-meta-machine-learning-engineering-interview-aa32f64b8e4b)\n\n\n\n# Misc\n\nI made the quiz based on FAANG interview exp here: https://github.com/khangich/machine-learning-interview\n\nhttps://www.linkedin.com/posts/yann-lecun_github-facebookresearchllama-inference-activity-7034956639526952960-B1-d?utm_source=share&utm_medium=member_android\n\n motivational videos\n\nhttps://youtu.be/Lp7E973zozc\n\nActivation energy - try  set alarm early\n\nits your job to do the crap you need to do to become what you want \n\nyou will never feel it... you have to FORCE yourself\n\nYour brain has two states autopilot and emergency break. Whenever you do something different from your routine your brain would go EMERGENCY BREAK. You need to force yourself to overcome it.\n\nIts the routine is what is killing you. If you feel sad or dissatisfied it is a SIGNAL, that some of your basic need is not met. You need to FORCE yourself to do something different. Get out. Get ouside of your comfort zone.\n\nWhen you have an impulse you need to ACT. 5 secods rule, if you wait more than 5 sec you are unlikely to do anything.\n\n\n\nI know there are a lot of people out there affected by layoffs and I hope this post gives you some inspiration that the job market isnt as scary as people make it seem. I ended up with 2 offers:\n\nNetflix Senior SWE: 485k base salary (a bit low compared to what I’m seeing online but I’m happy)\n\nTiktok: 250k base 25% avg bonus 100k sign on bonus split over 2 years and 400k in RSU (paper money)\n\nI still have an on-site left with Jump Trading which if it goes well I can assume tc above Netflix.\n\nEdit:\nResources for those who need them:\n\nReferral site:\nRefer.me\n\nResume ATS checker:\nSkillsyncer.com\n\nResume generator:\nHttps://resume.lol\n\nMock interviews:\nInterview.io\n\nI have 6 YOE and my previous TC was 280k so I  very excited for what the future holds 😊 lmk if there are any questions I can answer as well\n","n":0.029}}},{"i":133,"$":{"0":{"v":"Negotiation","n":1},"1":{"v":"[Negotiation tips](https://docs.google.com/document/d/1JIUyI-aGgK6x5arfvcOLgddqenLGEyvAd4OcJvc-B2Q/edit?usp=share_link)","n":0.707}}},{"i":134,"$":{"0":{"v":"Machine Learning","n":0.707},"1":{"v":"\n# ML System Design\n\nStefan's general ML design on [escalidraw](https://drive.google.com/drive/folders/1CT4cBULH30uYSQt5MA0Xc0UsMD89ZyCZ?usp=sharing)\nStefan's recommendation system [notes](https://drive.google.com/drive/folders/1CT4cBULH30uYSQt5MA0Xc0UsMD89ZyCZ?usp=sharing)\n\n# Coding\nwrite from scratch basic ml models, linear regression etc\n\nnumpy tricks, vectorization\n\n- [image overlap](https://leetcode.com/problems/image-overlap/)\n\n# Math\n\n- PCA, SVM, Matrix diagonalization\n- Loss functions\n- Linear regression","n":0.171}}},{"i":135,"$":{"0":{"v":"ML System Design","n":0.577},"1":{"v":"\nA confounding effect occurs when the effect of one factor is confused with the effect of another factor. This can happen when trying to determine the cause of a disease or other outcome\n\n\nSimpson paradox","n":0.171}}},{"i":136,"$":{"0":{"v":"Leave Job","n":0.707},"1":{"v":"Just be honest about why you are leaving. Things to mention:\n\n- select reasons which only the new company has and your old one cannot possibly offer. E.g. relocation in city only the\nnew company has office in, compensation package (e.g 2x)\n- say things you like about the team, be specific\n- say work-wise what you would do for the transition period. Should be 50/50 split why you leave and work you would do\n- ask whether you need to submit resignation letter or other documents\n\nIt's all professional, no hard feelings\n\n---\n\nLeaving speech, make it as if it is a farewell speech not a leaver\n\nMention the people, events and tasks that were most memorable and thank everyone who contributed to making the overall experience enjoyable. If your experiences there led you to a better opportunity, this is also something worth mentioning, as it ensures that you appear grateful for the opportunities you've had. \n\n- what you enjoyed\n\n- feelings about leaving\n\n- gratitude for support\n\n- specific examples\n\n- fun stuff\n\n\n**Capital One**\n\nThanks a lot to everybody for coming. I am delighted to see all of you here. Didn't expect so many people to show up, so I am a bit suprised in a nice way of course.\n\nI've really enjoyed my time here, it's been great to meet all of you. I am glad I was part of Analytics engineering. It's been a fun and interesting experience.\n\nI want to also thank Gareth and Rob for giving this opporunity. Without you guys, I wouldn't be here so I really appreciate it. I have a lot from both of you and during my time in the team. \n\nIt is still hard for me to believe how you believed in me. The first time I met Rob was on an interview last year and he gave me a very simple math question about calculating some averages. In essense, he asked me something like \" hey Ngo, how much is 1 + 2\" and because I was so nervous during the interview I completely messed up and said 4. The worst part was that 10 second before he gave me the question I told him I got a Masters degree in Maths. At that moment I am not sure if he actually believed me.\n\nIn the end Rob saw something which I am not sure what and gave me the opportunity, so I am very greatful for that. It was definitely a valuable experience which helped me to grow more in my career and an important step for my future endeavours. Without this opporunity I wouldn't be having the other opportunities that I have now.\n\nIt's been a pleasure to work with Johnny, Abhishek and Guoda. Although with Guoda we overlapped for only 1 week because she just joined. But I am sure that you will have some great time here, as I did.\n\n","n":0.046}}},{"i":137,"$":{"0":{"v":"Engineer levelling","n":0.707},"1":{"v":"\nhttps://drive.google.com/file/d/19baH27xoZGwVY-ssO4TuVoOev0VOck4G/view?usp=drive_link","n":1}}},{"i":138,"$":{"0":{"v":"Behaviour","n":1},"1":{"v":"\nTop questions:\n\n- Tell me about yourself\n- Why did you apply for this position\n- Why our company/ company info/ what do you know about us\n- Weakness/Strength\n- Why do you move companies so often (give specific reason for each), or better focus on why you want to join the new company rather than why you leave the old one. I want to work for you beacuse you are tech and currently I am in a bank. Find something that the new company has and the old one does not and just say it.\n\n**NB** prepare questions to ask them. Some questions might sound offensive careful. E.g Asking Atomico (venture capital) how they deal with crisis, recession, war, lower investements is not a very good question. It might imply k they are doing bad.\n\nLP principles from Stefan are pretty good stuff. You should do your own like these.\n\nListen, listen, listen! Do not interrupt even if you know something - no points for saying things quickly if you interrupt.\n\n\nCreate **Bullet points to some of your major projects (stories)** and referece those during interview.\n\nneed to prepare stories per each company\n\nInterviewers search for signal in these domains:\n- for low level just don't be asshole\n- team work\n- CV key words. You need to be able to talk abount everything you have in your CV\n\n- For higher levels:\n- team management\n- teaching other people, motivate, mentee others\n- can give examples of you helping people to onboard\n- contribution in refinement sessions, jira stories, defining objectives and goals\n- helping people with getting started\n- think of what your managers have done to help you and these are stuff that show signal that you are higher level\n\n\n# Tactics\n\n[DS Interviews](https://www.youtube.com/watch?v=PNExsBwdDlo&t=365s)\n\nSubmit research report for your top companies (check minute 7).\n\nEven better is to reach to gatekeepers (hiring managers directly and sed the research report).\n\nHiring manager is the tech manager.\n\n[DS hiring 01.03.2023](https://s3.amazonaws.com/kajabi-storefronts-production/file-uploads/sites/2147512189/themes/2150624317/downloads/cf16572-07c-ab0b-681-5dd4f8df277_U.S._Companies_Hiring_Data_Scientists.pdf)\n","n":0.057}}},{"i":139,"$":{"0":{"v":"All interview stages","n":0.577},"1":{"v":"\nInterviewing has different stages. I have divided them in 3 main ones - core preparation, warm-up mindset + applications, interviewing + negotiation.  For each of the steps  below you need to prepare Jira tickets and complete them in order. **Expected time needed to find new job: 5 months.**\n\n\n1. Preparation for interviews (**2 months**)\n    - algo: neetcode.io + leetcode targeted for companies, cracking the coding interview book\n    - reherse talking out load when solving problems, check steps in CTCI book when explaining your solution to a problem\n    - system design: [[interviews.System Design]], practice in excalidraw\n    - ml design: stefan's escalidraw example is pretty good\n    - probability: puzzle's app, 3 quant books\n    - stats: [[interviews.Probability and Statistics]]\n    - skim job descriptions to check tech stacks and requirements\n    - Think about what you want to have in your CV (avoind flufing, [[interviews.Practice and guide]] has CV screening from hiring manager's perspective)\n\n2. Prepare for behavior questions + leadership principles (**2 weeks**)\n    - stories for each of your jobs - starting point is your Job search folder\n    - [lp principles file](https://drive.google.com/file/d/15UJ5RJvlf-jNZFzDw8Y5m6KoEBUgo1Il/view?usp=drive_link) from stefan is good\n    - cracking the coding interview book\n    - why you james reed book\n    - [[interviews.Behaviour]]\n\n3. Prepare good CV (**3 days**)\n    - one for engineering (for MLE)\n    - one for data science (math + stats stuff)\n    - **before you apply for each position check in LinkdIN what people working at this position have in their CV**\n    - this step often will improve through iteration, as you apply to companies and talk stuff you will add/change your CV\n\n4. Apply at bunch of companies (**2 weeks**)\n    - timing is key here to get competing offers\n    - colllect a list of companies you want to apply for before hand\n        - LinkdIn, your previous applications, Referalls (from friends and random Blind users + write a Blind Post), Linkdin recruiters, email recruiters\n    - roles to aim for: quant, quant devs, data science, algo engineers, machine learning engineer\n    - prepare applications for all of them, don't apply yet! (some may need motivation letters or random stupid questions)\n    - prepare messages/talk with recruiters (see [[interviews.Talk with recruiters]], define what you want in terms of role, company, TC\n    - apply at all companies all at once\n\n5. Mock interviews for System Design and ML Design.\n\n6. Arrange interviews\n    - pretty dynamic step, some companies will push for earlier interviews, often with recruiters is like that.\n\n7. Prepare company specific stuff\n    - after you get interviews for each company you will need to prepare company specific stuff\n\n8. Do the interviews\n    - see [[interviews.Self evaluation]] for past interview experience\n    - take notes during each interview to remember company specific stuff\n\n9. Offer Negotiation\n    - [[interviews.Negotiation]]\n    - look in levels.fyi\n    - ask Blind\n\n10. Post offer\n    - do self evaluation of the whole interview cycle and update [[interviews.Self evaluation]]\n    - gather all your notes and put in dendron\n\nSteps 1-2 (**core preparation**) take most of the time (2-months) and depends on your current interview prep state. Although this takes most of your time in the interviewing cycle it is actually the part you would enjoy the most as it is about solving problems. Make sure you spend proper time on behaviour practice too.\n\nSteps 3-5 (**warm-up stage + applications**) take about 2-4 weeks. This part is the shortest one. Key here is to adjust your mindset to interview mode. Mocks with friends or paid one help. Also this part includes lots of admin work to make applications. Although it could be tedious it is super important as it gets you the interviews. Make sure you spend enough time on the applications. Need to pull out all your connections for referals, ask Blind for referrals, write to recruiters.\n\nSteps 6-10 (**interviewing + negotiation**) form the stage where you apply what you have been practicing for. These four steps are dynamic and less structured. Your goal is to finish them in the span of **1 to 2 months** (at most). Main goal is to have competing offers with similar deadlines. Key in this stage is that after doing lots of interviews you might get tired and will be tempted to accept the first descent offer you get. Be willing to put the extra effort to get competing offers as it could pay out pretty well for just a few weeks more of interviewing. Also accepting a job offer means you will be doing that at least a year, and cannot go back quickly to interview mode and increase TC.\n\n# Principles\n\n- Interviews are ultimately NOT about you. They'd ask lots of questions about you, but they are not interested in you. \n- Job is about solving the problems of the employer. It is not for you to make money. It is for you to solve problems.\n- Interview is a chat between two human beings figuring out if they have common interests.\n- Show genuine interest in the company and the role. Ask Questions!\n- Talk with excitetment! Chris Deloit from Kaggle.\n\n**CV updates**\n- change Applied GBM to another verb\n- need to convey message that you are DS/SDE\n- Two CV-s DS and SDE (HIT THE KEYWORDS!), make a list of key words you need to hit\n- add that you are currently in the DS team\n- Built internal app\n- add ML as keywords in more places\n- data pipeline at Transmetrics (ML pipeline)\n\n\n\n\n# Google Prep\n\n\nOne of my favorite (and slightly underrated) interview prep tricks? \n\nGoogle.\n\nI’ll search for anything tied to the company—those that overlap with my area of work.\n\nIf I’m prepping for an interview at Amazon, my search history will look something like: \n-> “Amazon A/B testing”\n-> “Amazon experiments”\n-> “Amazon metrics”\n-> “Amazon data science”\n-> \"Amazon business model\"","n":0.033}}},{"i":140,"$":{"0":{"v":"Algorithms","n":1},"1":{"v":"Strategy of wishful thinking, write interface/signature of functions before implementation. Brute force, play with toy\nexamples. For more comprehensive guide see [[engineering.algorithms]]\n\n#Todo add \"Cracking the coding interviews book.\"\n\n\n\n# Questions \n\nOne of the important things is to ask clarifying questions during the interview:\n- problem constraints (input size, memory, required time complexity, input type)\n- what is the expected output\n\n\n","n":0.134}}},{"i":141,"$":{"0":{"v":"How to Articles","n":0.577}}},{"i":142,"$":{"0":{"v":"Scope new project","n":0.577},"1":{"v":"# Questions you need to figure out\n\n- What are the revenue goals of this project?\n- Who are the key people in that project: PM, Sales, DS, ML, Engineers\n- Product road map for next quarter, year\n- Is that produt client facing or internal\n- Review current ideas that are being considered.\n- What additional ideas can you bring to the table?\n- Macroeconomical statistics of the product\n- Understand the whole user journey of the product - from pitching/getting the user attention to actual buying and then usage.\n\n\n\n1. Background and Analysis\n- Executive summary\n- Key insights\n2. Project Overview\n3. Involved Parties\n- Editors  \n- Implementers\n- Reviewers or Stakeholders\n4. Project Governance\n- Project Governance plan\n- Project Tracker\n- Project Calendar\n5. Action Plan Deep Dive\n6. Deliverables\n7. Implementation\n8.Vision\n8. Decision Log\n","n":0.092}}},{"i":143,"$":{"0":{"v":"Engineering","n":1}}},{"i":144,"$":{"0":{"v":"Algorithms","n":1},"1":{"v":"\n\nResources:\n- [cp algos](https://cp-algorithms.com/)\n- [e-maxx](https://github.com/e-maxx-eng/e-maxx-eng)\n- [dbabichev](https://flykiller.github.io/)\n- [pyrival](https://github.com/ngocuong0105/PyRival)\n- [usaco](https://usaco.guide/CPH.pdf)\n\n#TODOs \n- add from dbabichev link for [patterns](https://flykiller.github.io/coding%20ideas/).\n- add from pyrival\n- add from cp algos\n# Buzz words\nBIT, AVL trees, B-trees (used in databses?), Red Black Trees, Segment Trees, A* Search, Dijkstra, Kruskal, Prim algo, Trie. String algorithms \n[TSP](https://leetcode.com/problems/find-the-shortest-superstring/), Sprague-Grundy theorem, game theory, normal incomplete games\n\n#TODO\nthese buzz words should be sorted as a content page - maybe use backlinks\n\n# Algos\n\n\n- guess the algo, [p1](https://leetcode.com/problems/shortest-path-with-alternating-colors/)\n\n- BFS with smart states [p1](https://leetcode.com/problems/shortest-path-visiting-all-nodes/)\n\n- sliding window with augmented/additional data structure heap + queue, [p1](https://leetcode.com/problems/longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit/)\n\n\n- kth element in sorted matrix [p](https://leetcode.com/problems/kth-smallest-element-in-a-sorted-matrix/), #TODO check\n[paper](https://github.com/ngocuong0105/dendron-wiki/blob/main/vault/assets/files/Engineering/X%2BY.pdf)\n\n- order statistic tree, Red Black tree implementation I think\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nfrom sortedcontainers import SortedList\nsl.add(num)\nsl.bisect_left(num) # get order statistic\nsl.remove(num)\n```\n</details>\n","n":0.095}}},{"i":145,"$":{"0":{"v":"String Processing","n":0.707},"1":{"v":"# Fundamentals\n## String Hashing\n## Rabin-Karp for String Matching\n- [longest duplicate string](https://leetcode.com/problems/longest-duplicate-substring/)\n\n\n## Prefix function - Knuth-Morris-Pratt\n\n**Definition** You are given a string  $s$  of length  $n$ . The prefix function for this string is defined as an array  $\\pi$  of length  $n$ , where  $\\pi[i]$  is the length of the longest proper prefix of the substring  $s[0 \\dots i]$  which is also a suffix of this substring.\n\n$\\pi[i] = \\max_ {k = 0 \\dots i} \\{k : s[0:k] = s[i-k+1:i+1] \\}$\n\nFor example, prefix function of string \"abcabcd\" is  $[0, 0, 0, 1, 2, 3, 0]$ , and prefix function of string \"aabaaab\" is  $[0, 1, 0, 1, 2, 2, 3]$ \nBy definition $\\pi[0] = 0$\n\n\nBrute force algo to compute $\\pi$ is $O(n^3)$\n```python\n# O(n^3)\ndef kmp(s):\n    pi = [0]*len(s)\n    for i in range(len(s)):\n        for k in range(i):\n            if s[:k] == s[i-k+1:i+1]:\n                pi[i] = k\n    return pi\n```\n\n**First optimization**\n\nObserve $\\pi(i+1)$ is at most 1 larger than $\\pi(i)$. Thus when moving to the next position, the value of the prefix function can either **increase by one, stay the same, or decrease by some amount**. \n\nAlgo: In total the function can grow at most  $n$  steps (after all interations), and therefore also only can decrease a total of  $n$  steps. This means we only have to perform  $O(n)$  string comparisons, and reach the complexity  $O(n^2)$ .\n\n\n```python\n# O(n^2)\ndef kmp(s):\n    pi = [0]*len(s)\n    for i in range(1,len(s)): # on each iteration, pi[i] increases at most with 1\n        for j in range(pi[i-1]+1,0,-1):\n            if s[:j] == s[i-j+1:i+1]: # hit this line at most n times during all iterations from the two loops above\n                pi[i] = j # here pi decreases or increases at most by 1\n                break\n    return pi\n```\n\n\n**Second optimization**\n\n*no string comparisons*\n\nLet us compute $\\pi[i]$. Notice that $\\pi[i] = \\pi[i-1] + 1$ iff $s[i] == s[\\pi[i-1]]$\n\n$\\underbrace{\\overbrace{s_0 ~ s_1 ~ s_2}^{\\pi[i]} ~ \\overbrace{s_3}^{s_3 = s_{i+1}}}_{\\pi[i+1] = \\pi[i] + 1} ~ \\dots ~ \\underbrace{\\overbrace{s_{i-2} ~ s_{i-1} ~ s_{i}}^{\\pi[i]} ~ \\overbrace{s_{i+1}}^{s_3 = s_{i + 1}}}_{\\pi[i+1] = \\pi[i] + 1}$\n\ngoal is to find largest $j \\leq \\pi[i-1]$ such that $s[0 \\dots j-1] = s[i-j+1 \\dots i]$\n\n$\\overbrace{\\underbrace{s_0 ~ s_1}_j ~ s_2 ~ s_3}^{\\pi[i]} ~ \\dots ~ \\overbrace{s_{i-3} ~ s_{i-2} ~ \\underbrace{s_{i-1} ~ s_{i}}_j}^{\\pi[i]} ~ s_{i+1}$\n\n```python\ndef KMP(s):\n    pi = [0]*len(s)\n    for i in range(1,len(s)):\n        j = pi[i-1]\n        while j > 0 and s[i] != s[j]:\n            j = pi[j-1]\n        if j == 0: \n            pi[i] += s[i] == s[0]\n        else: \n            pi[i] = j+1\n        # if s[i] == s[j]:\n        #     j += 1\n        # pi[i] = j\n    return pi\n```\n$O(n)$ complexity as we increase $pi[i]$ at most $n$ times and decrease it at most $n$ times. No string comparisons.\n\n$pi$ is called prefix function = longest preffix suffix (LPS)\n\n**Problems**\n\n[Need creativity to come up with why you need KMP](https://leetcode.com/problems/minimum-number-of-valid-strings-to-form-target-ii/description/)\n\n[Search for a substring in a string](https://leetcode.com/problems/find-the-index-of-the-first-occurrence-in-a-string/)\n\n<details>\n<summary> <b>Idea</b> </summary>\nSearch substring t in string s:\n\nApply KMP(t+'#'+s) # use the hashtag for cases such as s = 'aaa', t = 'aa'\n</details>\n\n**Counting the number of occurrences of each prefix**\n\nGiven a string  $s$  of length  $n$ . In the first variation of the problem we want to count the number of appearances of each prefix  $s[0 \\dots i]$  in the same string. \n\n```python\ndef solve(s):\n    pi = KMP(s)\n    res = [0]*(len(s)+1)\n    for i in range(len(s)):\n        res[pi[i]] += 1\n    for i in range(len(s)-1,0,-1):\n        res[pi[i-1]] += res[i]\n    for i in range(len(s)+1):\n        res[i] += 1\n    return res[1:]\n```\n\n[The number of different substring in a string](https://cp-algorithms.com/string/prefix-function.html#counting-the-number-of-occurrences-of-each-prefix)\n\n\n- [remove substring while it exits](https://leetcode.com/problems/remove-all-occurrences-of-a-substring/)\n\n<details>\n<summary> <b>Idea</b> </summary>\nWe will solve this problem iteratively. Namely we will learn, knowing the current number of different substrings, how to recompute this count by adding a character to the end.\n\nWe take the string  $t = s + c$  and reverse it. Now the task is transformed into computing how many prefixes there are that don't appear anywhere else.\n\nTherefore the number of new substrings appearing when we add a new character  $c$  is  $|s| + 1 - \\pi_{\\text{max}}$ .\n</details>\n\n\n\n\n## Z-function\n## Suffix Array\n## Aho-Corasick algorithm\n## Suffix Tree\n## Suffix Automaton\n## Lyndon factorization\n# Tasks\n## Expression parsing\n## Manacher's Algorithm - Finding all sub-palindromes in O(N)\n\n- Manacher, [p1](https://leetcode.com/problems/longest-palindromic-substring/), [p2](https://leetcode.com/problems/shortest-palindrome/)\n- [maximum product of two palins](https://leetcode.com/problems/maximum-product-of-the-length-of-two-palindromic-substrings/)\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# O(n**2)\nclass Solution:\n    def longestPalindrome(self, s: str) -> str:\n        def palin(i,j):\n            while i >= 0 and j < len(s) and s[i] == s[j]:\n                i -= 1\n                j += 1\n            return s[i+1:j]\n        res = ''\n        for i in range(len(s)):\n            odd = palin(i,i)\n            even = palin(i,i+1)\n            if len(res)<len(odd): res = odd\n            if len(res)<len(even): res = even\n        return res\n\n# Manacher Algorithm. Find Longest Palindrome in O(n) time.\nclass Solution:\n    def longestPalindrome(self, s: str) -> str:  \n        T = '$#'+'#'.join(s)+'#&'\n        P = [0]*len(T)\n        C,R = 0,0\n        for i in range(len(T)-1):\n            if R > i: # P[i] = (R>i) and min(R-i,P[2*C-i]) slows down\n                P[i] = min(R-i,P[2*C-i])\n            while T[i+P[i]+1] == T[i-P[i]-1]:\n                P[i] += 1\n            if R < i+P[i]:\n                C,R = i,i+P[i]\n        l = max(P)\n        i = P.index(l)\n        # P[2:-2:2] returns the sizes of largest palindrom for each i being the center (only odd length palindromes!)\n        #P[2:-2] returns sized of largest palindromes (odd and even)\n        return s[(i-l)//2:(i+l)//2]\n\n```\n</details>\n\n\n- using Manacher to query if substring is palindome in `O(1)` per query\n\n```Python\ndef Manacher(s):\n    T = '$#'+'#'.join(s)+'#&'\n    P = [0]*len(T)\n    C,R = 0,0\n    for i in range(len(T)-1):\n        if R > i: \n            P[i] = min(R-i,P[2*C-i])\n        while T[i+P[i]+1] == T[i-P[i]-1]:\n            P[i] += 1\n        if R < i+P[i]:\n            C,R = i,i+P[i]\n    return P[2:-2]\n\ndef palin(i,j):\n    if j > len(s): return False\n    l = j-i\n    c = 2*((i+j)//2)-(l%2==0)\n    return P[c] >= l\nP = Manacher(s)\npalin(i,j) # returns if s[i:j] is a palindrome\n```\n\n## Finding repetitions","n":0.034}}},{"i":146,"$":{"0":{"v":"Sorting","n":1},"1":{"v":"\n- [leetcode](https://leetcode.com/problems/sort-an-array/)\n\n\nComparison sort algorithms determine the order of elements based on pairwise comparison between elements. Insertion sort, bubble sort, quick sort, and merge sort are all examples of comparison sort algorithms.\n\n\nCount, bucket, radix sort are examples of non-comparison sort algorithms. They do not compare elements directly but instead use the properties of the elements (like their range or digits) to sort them.\n\n# Quick Sort\n- Expected time complexity $O(nlogn)$, worse case $O(n^2)$.\n\n# Merge Sort\n- Time complexity $O(nlogn)$ - always\n\n\n# Examples\n\n```python\n'''\nInsertion Sort, inplace O(n^2)\nget the min, swap place\n'''\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        for i in range(len(nums)):\n            min_idx = i\n            for j in range(i+1,len(nums)):\n                if nums[j] < nums[min_idx]:\n                    min_idx = j\n            nums[i],nums[min_idx] = nums[min_idx],nums[i]\n        return nums\n\n'''\nBubble Sort - Inplace O(n^2)\nkeep nums[:j] sorted\n'''\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        for i in range(len(nums)):\n            while i and nums[i] < nums[i-1]:\n                nums[i],nums[i-1] = nums[i-1],nums[i]\n                i -= 1\n        return nums\n\n'''\nMerge Sort O(nlogn)\nsort(nums[:mid]) sort(nums[mid:])\nmerge()\n'''\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        def merge_sort(nums):\n            if len(nums) == 1: return nums\n            mid = len(nums)//2\n            left, right = merge_sort(nums[:mid]), merge_sort(nums[mid:])\n            merged = []\n            i,j = 0,0\n            while i<len(left) or j<len(right):\n                l = left[i] if i < len(left) else float('inf')\n                r = right[j] if j < len(right) else float('inf')\n                if l < r:\n                    merged.append(left[i])\n                    i += 1\n                else:\n                    merged.append(right[j])\n                    j += 1\n            return merged\n        \n        return merge_sort(nums)\n\n\n'''\nQuick Sort\npivot + merge\nworse case is O(n^2), every time we hit a bad split (imbalnces, 1 vs n-1)\naverage case is O(nlogn). Intuiton bad splits and good splits cancel each other. Draw a tree with depth 2 one bad and one good split. the sized of your remaining arrays is like you had 1 good split.\n\ngood split is when it is balanced\n\n\nO(n^2) worst case reached when array is decreasing or all elements are eqaul\n'''\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        def partition(l,r):\n            pivot = nums[r]\n            i = l\n            for j in range(l,r):\n                if nums[j] < pivot:\n                    nums[j],nums[i] = nums[i],nums[j]\n                    i += 1\n            nums[i],nums[r] = nums[r],nums[i]\n            return i\n\n        def quick_sort(l,r):\n            if l >= r: return\n            q = partition(l,r)\n            quick_sort(l,q-1)\n            quick_sort(q+1,r)\n\n        quick_sort(0,len(nums)-1)\n        return nums\n\n'''\nRandomised Quick Sort\n'''\n\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        def partition(l,r):\n            idx = random.randint(l,r)\n            # randomized\n            nums[idx],nums[r] = nums[r],nums[idx]\n            pivot = nums[idx]\n            i = l-1 # weird implementation see above. This one comes from the Intro to Algo book, but mine is a bit clearer\n            for j in range(l,r):\n                #fix the case when there are many equal numbers, want to have more balanced split\n                if j%2:\n                    if nums[j] < pivot:\n                        i += 1\n                        nums[i],nums[j] = nums[j],nums[i]\n                else:\n                    if nums[j] <= pivot:\n                        i += 1\n                        nums[i],nums[j] = nums[j],nums[i]\n                \n            nums[i+1],nums[r] = nums[r],nums[i+1]\n            return i+1\n\n        def quick_sort(l,r):\n            if l >= r: return None\n            q = partition(l,r)\n            quick_sort(l,q-1)\n            quick_sort(q+1,r)\n        \n        quick_sort(0,len(nums)-1)\n        return nums\n\n\"\"\"\nQuick Sort - not implace but easy to remember\n\"\"\"\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        def quick_sort(nums):\n            if len(nums) <= 1: return nums\n            pivot = nums[random.randint(0,len(nums)-1)]\n            smaller, equal, greater = [],[],[]\n            for num in nums:\n                if num < pivot: smaller.append(num)\n                elif num > pivot: greater.append(num)\n                else: equal.append(num)\n\n            return quick_sort(smaller) + equal + quick_sort(greater)\n\n        return quick_sort(nums)\n```\n\n# Heap Sort\n```\n\n- Heap is a nearly complete binary tree represented using an array\n- Root = i, parent = i//2, left = 2*i, right = 2*i+1\n- Root has value greater than its children\n- max_heapify(arr,i) fixes i, pushes it down to its place, maintains the heap property\n- build_max_heapify(arr)\n- heapsort(arr)\n- Heap size variable! The heap part is not sorted, the rest is.\n\n\"\"\"\nheap is a 1-indexed array that represents a nearly complete binary tree\npar(i) = i // 2\ni -> 2*i, 2*i+1\n max_heapify(i)\n checks if i is less than any of the childs and swaps with it, pushes down the tree \n\n\"\"\"\nclass Solution:\n    def sortArray(self, nums: List[int]) -> List[int]:\n        def max_heapify(i,size):\n            l,r = 2*i,2*i+1\n            i,l,r = i-1,l-1,r-1\n            largest = i\n            if l < size and nums[largest] < nums[l]:\n                largest = l\n            if r < size and nums[largest] < nums[r]:\n                largest = r\n            if largest != i:\n                nums[largest],nums[i] = nums[i],nums[largest]\n                max_heapify(largest+1,size)\n                \n\n        def heapsort():\n            size = len(nums)\n            # note heapsort is after max_heapify.\n            # first element has the largest value!\n            # we put it at th end and fix the root.\n            for i in range(len(nums)-1,-1,-1):\n                nums[i],nums[0] = nums[0],nums[i]\n                size -= 1\n                max_heapify(1,size)\n\n        def heapify():\n            n = len(nums)\n            for i in range(n,0,-1):\n                max_heapify(i, n)\n\n        \n        heapify()\n        heapsort()\n        return nums\n```\n\n\n\n\n# Count Sort\n- $O(n+k)$ where all numbers in nums are in the range (0,k)\n- count array such that arr[i] is the count of element i\n\n# Bucket Sort\n- put numbers in buckets, sort each bucket using comparison sort (usually insertion), as buckets are relatively small\n\n\n\n\n# K-th order statistic\n\n## Quick Select\n\n- [leetcode](https://leetcode.com/problems/kth-largest-element-in-an-array/)\n- Expected run time $O(n)$. Note it is as if we search for min and max. The reason for this is because unlike in quick_sort where we solve both branches of subproblems here we solve only the branch that contains the k-th order statistic element.\n\n\n```python\n\n# Quick Select (Not sure if it is called like this). Could be called Merge Sort Select?\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        def quick_select(nums,k):\n            pivot = nums[random.randint(0,len(nums)-1)]\n            smaller,equal,greater = [],[],[]\n            for i in range(len(nums)):\n                if nums[i] < pivot:\n                    smaller.append(nums[i])\n                elif nums[i] == pivot:\n                    equal.append(nums[i])\n                else:\n                    greater.append(nums[i])\n            if len(smaller)+len(equal) < k:\n                return quick_select(greater,k-len(smaller)-len(equal))\n            elif len(smaller) >= k:\n                return quick_select(smaller,k)\n            return pivot\n\n        nums = [-num for num in nums]\n        return -quick_select(nums,k)\n\n# Quick Select, but no xtra memory\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        def partition(l,r):\n            idx = random.randint(l,r)\n            pivot = nums[r]\n            nums[idx],nums[r] = nums[idx],nums[r]\n            i = l-1\n            for j in range(l,r):\n                if j %2:\n                    if nums[j] < pivot:\n                        i += 1\n                        nums[i],nums[j] = nums[j],nums[i]\n                else:\n                    if nums[j] <= pivot:\n                        i += 1\n                        nums[i],nums[j] = nums[j],nums[i]\n\n            nums[i+1],nums[r] = nums[r],nums[i+1]\n            return i+1\n\n        def quick_select(l,r,k):\n            if l == r: return nums[l]\n            q = partition(l,r)\n            # q is index between l,r\n            # k is count!, it is the first element in array[l:r+1]\n            # q-l+1 becomes a count\n            if q-l+1 == k: return nums[q]\n            if k < q-l+1:\n                return quick_select(l,q-1,k)\n            else:\n                return quick_select(q+1,r,k-(q-l+1)) # WTF\n\n        nums = [-num for num in nums]\n        return -quick_select(0,len(nums)-1,k)\n\n```\n","n":0.032}}},{"i":147,"$":{"0":{"v":"Python tricks","n":0.707},"1":{"v":"\n- transpose of a matrix\n\n```Python\n# transpose with unzipping\nmat = [(1,2,3),(4,5,6)]\ntranspose_mat = list(zip((*mat)))\n```\n\n- one star unpacks arguments\n```Python\nfunction(1,2,3) = function(*[1,2,3])\n\nargs = [1,2,3]\nfunc(*args) #much nicer than\nfunc(args[0],args[1],args[2],...)\n```\n\n- two stars is for key worded arguments and is for maps\n```Python\nfunction(**dict) # keys are the arguments and values are the argument values\n```\n\n\n- cumulative sum,min,max,xor,gcd. Note sum and xor are semi-rings on $\\Z$ (that is they have minus) - you can query subarrays in $O(1)$.\n```Python\n# cumulative sum array, cdf\nfrom itertools import accumulate\nnums = [1,2,3,4]\ncum = list(accumulate(nums,func = lambda x,y:x+y)) # [1,3,6,10]\n```\n\n- xor cumulative\n```python\nxor_cum = [0] + list(accumulate(arr,lambda x,y:x^y))\ndef f(l,r):\n    '''returns xor subarray arr[l]^arr[l+1]^...^arr[r]'''\n    xor_cum[r+1]^xor_cum[l]\n```\n\n- flatten list with chain\n```Python\n# flatten list\nfrom itertools import chain\nequations = [[1,2,3],[4,5]]\nlist(chain(*equations))\n\n```\n\n- adjacency list\n```Python\nfrom collections import defaultdict\nadj = defaultdict(list) # default dict takes a function\nfor u,v in edges:\n    adf[u].append(v)\n\n```\n\n- Counter\n```Python\n# Counter dict\nfrom collections import Counter\narr = [1,2,3,1,2,4,4,2]\nc = Counter(arr)\nc.most_common() # returns tuple of all elements and counts in sorted order\n```\n\n```Python\n# count occurrences of element in list\nls = [1,2,3,1,1]\nls.count(1)\n```\n\n- [Toeplitz Matrix](https://leetcode.com/problems/toeplitz-matrix/), diagonals have constant values\n```Python\nclass Solution:\n    def isToeplitzMatrix(self, matrix: List[List[int]]) -> bool:\n        for i in range(len(matrix)-1):\n            for j in range(len(matrix[0])-1):\n                if matrix[i][j] != matrix[i+1][j+1]: return False\n        return True\n    \nclass Solution:\n    def isToeplitzMatrix(self, m: List[List[int]]) -> bool:\n        return all(m[i][j] == m[i+1][j+1] for i,j in product(range(len(m)-1),range(len(m[0])-1)))\n\nclass Solution:\n    def isToeplitzMatrix(self, m: List[List[int]]) -> bool:\n        return all(r1[:-1] == r2[1:] for r1,r2 in zip(m,m[1:]))\n\n```\n\n- check if two intervals overlap\n\n```Python\n# [a,b], [x,y]\ndef overlap(a,b,x,y):\n    return a < y and x < b\n```\n\n- max with key, array of arrays, return array of max length\n```Python\nmax(arrs, key = len) # gives array of maximul length\n```\n\n- custom comparator in sorting, sorted(), sort, key, compare\n```Python\nfrom functools import cmp_to_key\ndef compare(x, y):\n     return x[0] - y[0]\n \ndata = [(4, None), (3, None), (2, None), (1, None)]\nsorted(data, key=cmp_to_key(compare))\n```\n\n- python does not have tail [recursion](https://stackoverflow.com/questions/13591970/does-python-optimize-tail-recursion) (unlike Lisp)\n\n- get groups, groupby\n\n```Python\nfrom itertools import groupby\ns = 'aaabbddca'\ngroups = [(ch,len(list(g))) for ch,g in groupby(s)] # [(a,3),(b,2),(d,2),(c,1),(a,1)]\n```\n\n- concurrent, multithreaded programming, [web crawler](https://leetcode.com/problems/web-crawler-multithreaded/)\n```Python\n# simple DFS\nclass Solution:\n    def crawl(self, start: str, parser: 'HtmlParser') -> List[str]:\n        hostname = lambda x: x.split('/')[2]\n        visited,stack= set([start]),[start]\n        while stack:\n            s = stack.pop()\n            for u in parser.getUrls(s):\n                if u not in visited and hostname(start) == hostname(u):\n                    visited.add(u)\n                    stack.append(u)\n        return visited\n\n# concurrent DFS\nfrom concurrent import futures\nclass Solution:\n    def crawl(self, s: str, parser: 'HtmlParser') -> List[str]:\n        hostname = lambda x: x.split('/')[2]\n        visited = set([s])\n        with futures.ThreadPoolExecutor(max_workers=16) as executor:\n            tasks = [executor.submit(parser.getUrls, s)]\n            while tasks:\n                neigh = tasks.pop().result()\n                for u in neigh:\n                    if u not in visited and hostname(s) == hostname(u):\n                        visited.add(u)\n                        tasks.append(executor.submit(parser.getUrls, u))\n        return visited\n```\n\n- string operations/methods\n```Python\nchar.islower()\nchar.isupper()\nchar.lower()\nchar.upper()\nch.isnumeric() # is an integer\nch.isalpha() # is a character\n```\n\n- permutations\n```python\nfrom itertools import permutations\nnums = [2,1,3,5]\nfor p in permutations(nums):\n    print(p)\n```\n\n- is a string a rotated version of another string, is rotate\n```python\n# Consider a string S=\"helloworld\". Now, given another string T=\"lloworldhe\", can we figure out if T is a rotated version of S? Yes, we can! We check if S is a substring of T+T.\ndef is_rotate(s,t):\n    return s in t+t\n```\n\n- traverse squares quickly. If $x = a*a$ then $x = 1+3+5...$\n```python\nx,n = 0,1\nwhile True:\n    print(x)\n    x += n\n    n += 2\n```\n- [Binary operations laws](https://stackoverflow.com/questions/12764670/are-there-any-bitwise-operator-laws)\n\n- Distributive of AND over XOR\n```\na & (b1 ^ b2) = (a & b1) ^ (a & b2)\n```\n\n- bitwise tricks, binary operations\n```Python\nclass Binary:\n\n    \"\"\"\n    This is a class for basic manipulation methods for bits.\n    Arguments: Integer in base 10\n    Returns:\n    binary representation;\n    get, set, clear, update specific bits;\n    count number of bits;\n    least significant bit;\n    \"\"\"\n\n    def __init__(self, num: int):\n        self.decimal = num\n\n    # binary representation\n    def __str__(self) -> str:\n        res = []\n        num = self.decimal\n        while num > 0:\n            res.append(str(num % 2))\n            num //= 2\n        return f\"{self.decimal} has binary representation {''.join(res[::-1])}\"\n\n    def __repr__(self) -> str:\n        res = []\n        num = self.decimal\n        while num > 0:\n            res.append(str(num % 2))\n            num //= 2\n        return \"\".join(res[::-1])\n\n    # get bit\n    def get(self, i: int) -> int:\n        msk = 1 << i\n        if self.decimal & msk != 0:\n            return 1\n        return 0\n\n    # set bit\n    def set(self, i: int) -> int:\n        msk = 1 << i\n        self.num = self.decimal | msk\n        return self.decimal\n\n    # clear specific bit i\n    def clear(self, i: int) -> int:\n        msk = ~(1 << i)  # ~ reverses bits, 0 -> 1 and 1 -> 0\n        self.decimal = self.decimal & msk\n        return self.decimal\n\n    # clear all bits from beginning to bit i\n    def clearFirstBits(self, i: int) -> int:\n        msk = (1 << i) - 1\n        self.decimal = self.decimal & msk\n        return self.decimal\n\n    # clear all bits from end to bit i\n    def clearEndBits(self, i: int) -> int:\n        msk = -1 << (i + 1)  # note -1 is 11111..1\n        self.decimal = self.decimal & msk\n        return self.decimal\n\n    # update bit i to value val\n    def update(self, i: int, val: bool) -> int:\n        msk = ~(1 << i) | (val << i)\n        self.decimal = self.decimal & msk\n        return self.decimal\n\n    # returns number of bits\n    def countBits(self) -> int:\n        res = 0\n        while self.decimal > 0:\n            # res+=self.decimal%2\n            res += self.decimal & 1\n            self.decimal >>= 1\n        return res\n\n    # least significant bit\n    def lsb(self) -> int:\n        # negative numbers are represented as two's complement\n        # two's complement = one's complement + 1\n        return self.decimal & -self.decimal\n```\n\n- print + assignment\n```python\nprint(x:=10)\n```\n\n- window multiply\n```python\ndef window_multiply(filter_window: np.ndarray, target: np.ndarray):\n    \"\"\"\n    Example:filter_window = np.array([1,2,3]),  target = np.array([4,5,6,7,8,9,10]), \n    out = [\n        [ 4, 10, 18],\n        [ 5, 12, 21],\n        [ 6, 14, 24],\n        [ 7, 16, 27],\n        [ 8, 18, 30]\n        ]\n    out[0] = [1*4, 2*5, 3*6] = [4, 10, 18]\n    out[1] = [1*5, 2*6, 3*7] = [5, 12, 21]\n    etc.\n    \"\"\"\n    w = len(filter_window)\n    indices = np.arange(len(target) - w + 1)[:, None] + np.arange(w)\n    out = filter_window * target[indices]\n    return out\n\n```\n\n- itertools.groupby()\n```python\nfor k,v in groupby('aaabbcddd'):\n    print(k,list(v)) # a, [a,a,a]\n\ngroups = []\nfor k,v in groupby(s):\n    groups.append((k,len(list(v))))\n```\n\n- [negative power modulus](https://math.stackexchange.com/questions/2592324/how-to-do-a-modular-arithmetic-with-negative-exponents). You can use it as of Python 3.8. Useful for solving linear Diophantine equality. First and third argument need to be coprime.\n```python\npow(23,-1,2)\n```\n\n- binary to decimal\n```python\nint('101',2)\n```\n\n- careful with instantiation and constructor\n```python\n\nclass Node:\n    def __init__(self,val,children=[]):\n        self.val = val\n        self.children = children\n\na = Node('a')\nb = Node('b')\na.children.append(12)\nprint(b.children) # prints [12]\n# modification on object a modifies b too, children is a class variable... if not set\n```\n\n- swapcase(), string, uppercase, lowercase swap\n```python\n'aA'.swapcase() # gives 'Aa'\n```\n\n- (dangerous default values)[https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments]\n\n```python\ndef append_to(element, to=[]): # mutable default arguments\n    to.append(element)\n    return to\n\nmy_list = append_to(12)\nprint(my_list)\n\nmy_other_list = append_to(42)\nprint(my_other_list)\n\n# What you want:\n# [12]\n# [42]\n\n# What actually happens:\n# [12]\n# [12, 42]\n```\n\n- key, custom bisect bisect_left\n```\ndef poss:\n    \"\"\" returns True/False\"\"\nbisect.bisect_left(range(n), x=True, key=poss)\n\n# equivalent to\ndef bs(nums=range(n)):\n    l,r = 0,n\n    while l<r:\n        m =l+r>>1\n        if poss(m):\n            r = m\n        else:\n            l = m+1\n    return l\n```\n\n- [generators are slower than list comprehension](https://stackoverflow.com/questions/62975325/why-is-summing-list-comprehension-faster-than-generator-expression)\n\n-check common characters in two strings A and B\n\n```python\nsum([ch in A for ch in B]) # fast\nsum(ch in A for ch in B) # slow\n```\n\n\n- check if t is a subsequence of s using iterator trick\n\n```python\ndef is_subsequence(t,s):\n    it = iter(s)\n    return all(ch in it for ch in t)\n# return all(ch in iter(t) for ch in s) fails, creates a new instance of iter(t) every time \n```","n":0.03}}},{"i":148,"$":{"0":{"v":"Numerical Methods","n":0.707},"1":{"v":"\n    Search\n        Ternary Search\n        Newton's method for finding roots\n    Integration\n        Integration by Simpson's formula\n","n":0.267}}},{"i":149,"$":{"0":{"v":"Misc","n":1},"1":{"v":"# Sequences\n\n## Binary Search\n- template for finding smallest element $i$ in array such that `func(i)` is `True`\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef bs(i,j,func):\n    while i < j:\n        mid = i+j >> 1\n        if func(mid):\n            j = mid\n        else:\n            i = mid + 1\n    return i\n```\n\n</details>\n\n#Zadachi\n- 2D problems often treat columns/rows as elements. E.g each column is element and we do binary search on columns $O(m*logn)$\n    - [black pixels](https://leetcode.com/problems/smallest-rectangle-enclosing-black-pixels/)\n    - [peak element](https://leetcode.com/problems/find-a-peak-element-ii/)\n\n**binary search in disguise**\n- [maximum average subarray](https://leetcode.com/problems/maximum-average-subarray-ii/)\n- [sum of floored pairs](https://leetcode.com/problems/sum-of-floored-pairs/)\n- [max submatrix sum less than k](https://leetcode.com/problems/max-sum-of-rectangle-no-larger-than-k/)\n- [mysterious function](https://leetcode.com/problems/find-a-value-of-a-mysterious-function-closest-to-target/)\n- [minimum wasted space](https://leetcode.com/problems/minimum-space-wasted-from-packaging/)\n- [angry birds](http://www.usaco.org/index.php?page=viewproblem2&cpid=597)\n- [minimum cost to make equal array](https://leetcode.com/problems/minimum-cost-to-make-array-equal/)\n**built in python library**\n\n**Added after Python 3.10!!!**\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nimport bisect\nbisect.bisect_left(arr,num,key) # uses >=\nbisect.bisect_right(arr,num,key) # uses >\n\n# can binary search tuples too\nbisect.bisect_left(arr, (x,y), key = lambda x: (x[0],x[1]))\n```\n- binary search + insert\n```python\nimport bisect\nbisect.insort_left(arr,num,key) # runs binary search and inserts O(n)\n```\n\n\n</details>\n\n\n## RMQ task (Range Minimum Query - the smallest element in an interval)\n## Longest increasing subsequence\n## Maximum/minimum subarray sum\n- [leetcode](https://leetcode.com/problems/maximum-subarray/)\n\n**Solution 1.**\n- goal find `nums[l:r+1]` with max sum\n- use cumulative sums `nums[l:r+1] = cum[r]-cum[l-1]`\n- redefined goal for each $r$ find $l$ which maximizes `sum(nums[l:r+1])`\n- compute $cum[r]$ as we go, and maintain minimum possible value for $cum[l-1]$\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef maxSubArray(nums: List[int]) -> int:\n    res,min_sum,sm = -float('inf'),0,0\n    for r in range(len(nums)):\n        sm += nums[r]\n        res = max(res,sm-min_sum)\n        min_sum = min(min_sum,sm)\n    return res\n```\n</details>\n\n**Solution 2.**\n- Kadane algo\n- compute partial/cumulative sum `sm` as we go. If negative restart to 0\n- our maximum subarray must start at a critical point when $sm < 0$ proof bby minimality + contradiction.\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef maxSubArray(nums: List[int]) -> int:\n    res,curr = -float('inf'),0\n    for num in nums:\n        curr += num\n        res = max(res,curr)\n        curr = max(curr,0)\n    return res\n            curr,res = -float('inf'),-float('inf')\n        for num in nums:\n            curr = max(curr+num,num)\n            res = max(res,curr)\n        return res\n```\n</details>\n\n\n**Solution 3.**\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef maxSubArray(nums: List[int]) -> int:\n    curr,res = -float('inf'),-float('inf')\n    for num in nums:\n        curr = max(curr+num,num)\n        res = max(res,curr)\n    return res\n```\n</details>\n\n- [maximum alternating subarray](https://leetcode.com/problems/maximum-alternating-subarray-sum/)\n\n#QED\n\n\n## K-th order statistic in O(N)\n# Game Theory\n## Games on arbitrary graphs\n## Sprague-Grundy theorem. Nim\n\n**Concept 1 (Impartial Game)**: Given a particular arrangement of the game\nboard, if either player have exactly the same set of moves should he\nmove first, and both players have exactly the same winning condition,\nthen this game is called impartial game. For example, chess is not\nimpartial because the players can control only their own pieces, and\nthe [flip game II](https://leetcode.com/problems/flip-game-ii/) , on the other hand, is impartial.\n\n\n**Concept 2 (Normal Play vs Misere Play)**: If the winning condition of\nthe game is that the opponent has no valid moves, then this game is\nsaid to follow the normal play convention; if, alternatively, the\nwinning condition is that the player himself has no valid moves, then\nthe game is a Misere game. Our +- [flip game II](https://leetcode.com/problems/flip-game-ii/) has apprently normal play.\n\nWe consider impartial normal games.\n\nSuch games can be completely described by a directed acyclic graph: the vertices are game states and the edges are transitions (moves). \nA vertex without outgoing edges is a losing vertex (a player who must make a move from this vertex loses).\n\n\n**Goal:** A state is winning if there is at least one transition to a losing state and is losing if there isn't at least one transition to a losing state. Our task is to classify the states of a given game.\n\n**Nim**\nThere are several piles, each with several stones. In a move a player can take any positive number of stones from any one pile and throw them away. A player loses if they can't make a move, which happens when all the piles are empty.\n\n**Charles L. Bouton Theorem.** The current player has a winning strategy if and only if the xor-sum of the pile sizes is non-zero.\n\nProof by induction. Induction step proves that if the current state is 0 then all other neighbour states are non zero. If the current state is non-zero you can always reach a zero state, consider the pile with the largest number of stones and consider is largest bit. (do xor tricks).\n\n**Colloraly.** Nim games are equivalent as long as the xor value is the same.\n\nGame of \ndsacan be reduced to game of one pile.\n\n**Sprague-Grundy theorem** \nThis theorem proves the equivalence of impartial games and Nim. It reduces every impartial normal game to Nim.\n\nLet's consider a state $v$ of a two-player impartial game and let $\\{v_{i},i = 1 \\dots n\\}$ be the states reachable from it. To this state $v$, we can assign a fully equivalent game of Nim with one pile of size $x$. The number is called the Grundy value or **nim-value** of state $v$. (this is the Game to Nim mapping)\n\nFind this number $x$ recursively:\n\n$x = mex(x_1, x_2, .... x_n)$\n\nwhere $mex$ is the minimum excluding function, e.g $mex([0,1,2,4,5]) = 3$\n\nRecursion base case is the end states where there are no possible moves and whoever turn it is they loose (nim-value equal to 0).\n\n\n**Recipe**\nTo calculate the Grundy value of a given state you need to:\n1. Get all possible transitions from this state\n2. Each transition can lead to a sum of independent games (one game in the degenerate case). Calculate the Grundy value for each independent game and xor-sum them. Of course xor does nothing if there is just one game.\n3. After we calculated Grundy values for each transition we find the state's value as the $mex$ of these numbers.\n4. If the value is zero, then the current state is losing, otherwise it is winning.\n\n\n**Problems**\n\n- [flip game II](https://leetcode.com/problems/flip-game-ii/)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# O(C_n) where C_n is n-th Catalan number\nclass Solution:\n    def canWin(self, s: str) -> bool:\n        def adj(s):\n            neigh = []\n            for i in range(len(s)-1):\n                if s[i:i+2] == '++': neigh.append(s[:i]+'--'+s[i+2:])\n            return neigh\n        @cache\n        def dfs(s):\n            for u in adj(s):\n                if not dfs(u): return True\n            return False\n        return dfs(s)\n\n# O(n^2) Sprague-Grundy theorem\nclass Solution:\n    def canWin(self, s: str) -> bool:\n        @cache\n        def dp(s):\n            if len(s) < 2: return 0\n            st = set()\n            for i in range(len(s)-1):\n                if s[i:i+2] == '++':\n                    st.add(dp(s[:i]) ^ dp(s[i+2:]))\n            # mex\n            i = 0 \n            while i in st:\n                i += 1\n            return i\n        return dp(s) != 0\n\n```\n\n</details>\n\n\n**Note** the second solution is not $O(N^2)$ truly as it has lots of subproblems. Need to preprocess the input so that the subproblems depend on single numbers. e.g. get lenghts of plus groups: '++++--++-' preprocess to `[4,2]`. See your leetcode solution.\n\n\n- [game of nim](https://leetcode.com/problems/game-of-nim/)\n- [chalkboard](https://leetcode.com/problems/chalkboard-xor-game/)\n\n## Hackenbush game\n\n[The game](https://en.wikipedia.org/wiki/Hackenbush)\n\nUses Sprague-Grundy numbers\n\n-[Remove from fibonaci tree][https://leetcode.com/problems/subtree-removal-game-with-fibonacci-tree/description/)\n\n#QED\n\n# Schedules\n## Scheduling jobs on one machine\n## Scheduling jobs on two machines\n## Optimal schedule of jobs given their deadlines and durations\n# Miscellaneous\n## Josephus problem\n## 15 Puzzle Game: Existence Of The Solution\n## The Stern-Brocot Tree and Farey Sequences\n\n## Rotate image\n\n- rotate image, [p1](https://leetcode.com/problems/rotate-image/)\n- 90 degree rotation = flip + transpose\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef rotate(matrix):\n    matrix.reverse()\n    return list(zip(*matrix))\n\ndef rotate_inplace(matrix):\n    matrix.reverse()\n    for i in range(len(matrix)):\n        for j in range(i):\n            matrix[i][j],matrix[j][i] = matrix[j][i],matrix[i][j]\n```\n</details>\n\n## Longest valid parenthesis\n- [leetcode](https://leetcode.com/problems/longest-valid-parentheses/)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Solution:\n    def longestValidParentheses(self, s: str) -> int:\n        def compute(ch,s):\n            bal,res,curr = 0,0,0\n            for p in s:\n                bal += (p == ch)\n                bal -= (p != ch)\n                curr += 2*(p == ch)\n                if bal == 0: \n                    res = max(res,curr)\n                elif bal<0:\n                    curr,bal = 0,0\n            return res\n        return max(compute('(',s),compute(')',s[::-1]))\n```\n</details>\n\n\n## Bit manipulation\n#Zadachi\n- [convert to hexadecimal](https://leetcode.com/problems/convert-a-number-to-hexadecimal/)\n\n## Random index pick with weights in O(1)\n\n[problem](https://leetcode.com/problems/random-pick-with-weight/)\n\nCan do Preprocessing in $O(n)$ and then pick in $O(1)$ (better than standard $O(log(n))$ using cdf and binary search)\n\nReference [Alias method](https://en.wikipedia.org/wiki/Alias_method), [dbabichev](https://leetcode.com/problems/random-pick-with-weight/discuss/671439/Python-Smart-O(1)-solution-with-detailed-explanation)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Solution:\n\n    def __init__(self, w: List[int]):\n        ep = 10e-10\n        n,sm = len(w),sum(w)\n        w = [ww/sm for ww in w]\n        self.boxes = []\n        s = [[i,ww] for i,ww in enumerate(w) if ww<1/n]\n        g = [[i,ww] for i,ww in enumerate(w) if ww>=1/n]\n        while s and g:\n            i,ws = s.pop()\n            j,wg = g[-1]\n            self.boxes.append((i,j,ws)) # index,index,weight\n            g[-1][1] -= (1/n-ws)\n            if g[-1][1]<1/n-ep:\n                s.append(g.pop())\n        for i,ww in g:\n            self.boxes.append((i,i,1))\n            \n    def pickIndex(self) -> int:\n        n = len(self.boxes)\n        box_num = random.randint(0, len(self.boxes) - 1)\n        return self.boxes[box_num][random.uniform(0, 1 / n) >= self.boxes[box_num][2]]\n```\n</details>\n","n":0.028}}},{"i":150,"$":{"0":{"v":"Linear Algebra","n":0.707},"1":{"v":"\n    Matrices\n        Gauss & System of Linear Equations\n        Gauss & Determinant\n        Kraut & Determinant\n        Rank of a matrix\n","n":0.236}}},{"i":151,"$":{"0":{"v":"Graphs","n":1},"1":{"v":"\n# Graph traversal\n## Breadth First Search\n\n- [visit all](https://leetcode.com/problems/shortest-path-visiting-all-nodes/?envType=study-plan&id=dynamic-programming-iv)\n## Depth First Search\n\n----\n\n# Connected components, bridges, articulations points\n\n## Finding Connected Components\n\n## Finding Bridges in O(N+M)\n\n- find bridges in graph in linear time\n- [leetcode](https://leetcode.com/problems/critical-connections-in-a-network/)\n- [explanation](https://cp-algorithms.com/graph/bridge-searching.html#implementation)\n- key: track entry times in array `tin`, and compute array `low`\n`low[s]` is minimum of:\n    - entry time in s\n    - entry time in all descendants u of s which are back edges\n    - `low[u]` for all u where $(s,u)$ is tree edge\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef bridges(n, adj_mat) -> List[List[int]]:\n    def dfs(s,p):\n        nonlocal t\n        visited.add(s)\n        t += 1\n        low[s] = tin[s] = t\n        for u in adj_mat[s]:\n            if u not in visited:\n                dfs(u,s)\n                low[s] = min(low[s],low[u])\n                if low[u] > tin[s]:\n                    res.append([s,u])\n            elif u != p:\n                low[s] = min(low[s],tin[u])\n    res,visited = [],set()\n    t,low,tin = 0,[float('inf')]*n,[float('inf')]*n\n    for s in range(n):\n        if s not in visited:\n            dfs(s,-1)\n    return res\n```\n</details>\n\n## Finding Bridges Online\n\n## Finding Articulation Points in $O(N+M)$\n- [leetcode](https://leetcode.com/problems/minimum-number-of-days-to-disconnect-island/)\n- [explanation](https://cp-algorithms.com/graph/cutpoints.html)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python \ndef articulation_point(adj):\n    def dfs(s,p):\n        nonlocal t\n        visited.add(s)\n        t += 1\n        tin[s] = low[s] = t\n        children = 0\n        for u in adj[s]:\n            if u not in visited:\n                dfs(u,s)\n                low[s] = min(low[s],low[u]) \n                if low[u] >= tin[s] and p!=-1: # \\geq !\n                    art.append(s)\n                children += 1\n            elif u != p:\n                low[s] = min(low[s],tin[u])  \n        if children != 1 and p==-1:\n            art.append(s)\n    t,visited = 0, set()\n    tin,low,art = {},{},[]\n    for s in adj:\n        if s not in visited: dfs(s-1)\n    return art != []\n            \n```\n</details>\n\n- [malware spread](https://leetcode.com/problems/minimize-malware-spread-ii/); can be solved using simple dfs/bfs for each node in $O(N^3)$, use articulation points $O(N^2)$, DSU $O(N^2)$\n\n## Strongly Connected Components and Condensation Graph\n\n## Strong Orientation\n\n\n----\n\n# Single-source shortest paths\n\n## Dijkstra - finding shortest paths from given vertex\n- [Cheapest flights within k stops](https://leetcode.com/problems/cheapest-flights-within-k-stops/?envType=study-plan&id=level-3)\n\n- [Modify edges, SMART Dijkstra application](https://leetcode.com/problems/modify-graph-edge-weights/)\n\n```python\ndef dijkstra(s):\n    h = [(0,s)]\n    dist,parent = {s:0},{}\n    while h:\n        d,s = heappop(h)\n        for u,du in adj[s]:\n            if dist.get(u,inf) > du+d:\n                dist[u] = du+d\n                parent[u] = s\n                heappush(h,(dist[u],u))\n    return dist,parent\n```\n\n**First loop on k, otherwise it will error. k is the phase count.**\n\n## Dijkstra on sparse graphs\n## Bellman-Ford - finding shortest paths with negative weights\n## 0-1 BFS\n## D´Esopo-Pape algorithm\n\n\n# All-pairs shortest paths\n## Floyd-Warshall - finding all shortest paths\n\n**NB: Order of the loops is important. MUST START WITH K that is the phase.**\n\nGiven a directed or an undirected weighted graph $G$ with $n$ vertices.\nThe task is to find the length of the shortest path $d_{ij}$ between each pair of vertices $i$ and $j$.\n\nThe graph may have negative weight edges, but no negative weight cycles.\n\n\nThe key idea of the algorithm is to partition the process of finding the shortest path between any two vertices to **several incremental phases**.\n\nLet us number the vertices starting from 1 to $n$.\nThe matrix of distances is $d[ ][ ]$.\n\nBefore $k$-th phase ($k = 1 \\dots n$), $d[i][j]$ for any vertices $i$ and $j$ stores the length of the shortest path between the vertex $i$ and vertex $j$, which contains only the vertices $\\{1, 2, ..., k-1\\}$ as internal vertices in the path.\n\nIn other words, before $k$-th phase the value of $d[i][j]$ is equal to the length of the shortest path from vertex $i$ to the vertex $j$, if this path is allowed to enter **only the vertex with numbers smaller than $k$** (the beginning and end of the path are not restricted by this property).\n\nIt is easy to make sure that this property holds for the first phase. For $k = 0$, we can fill matrix with $d[i][j] = w_{i j}$ if there exists an edge between $i$ and $j$ with weight $w_{i j}$ and $d[i][j] = \\infty$ if there doesn't exist an edge.\n\n\nSuppose now that we are in the $k$-th phase, and we want to compute the matrix $d[ ][ ]$ so that it meets the requirements for the $(k + 1)$-th phase.\nWe have to fix the distances for some vertices pairs $(i, j)$.\nThere are two fundamentally different cases:\n\n*   The shortest way from the vertex $i$ to the vertex $j$ with internal vertices from the set $\\{1, 2, \\dots, k\\}$ coincides with the shortest path with internal vertices from the set $\\{1, 2, \\dots, k-1\\}$.\n\n    In this case, $d[i][j]$ will not change during the transition.\n\n*   The shortest path with internal vertices from $\\{1, 2, \\dots, k\\}$ is shorter.\n\n    This means that the new, shorter path passes through the vertex $k$.\n    This means that we can split the shortest path between $i$ and $j$ into two paths:\n    the path between $i$ and $k$, and the path between $k$ and $j$.\n    It is clear that both this paths only use internal vertices of $\\{1, 2, \\dots, k-1\\}$ and are the shortest such paths in that respect.\n    Therefore we already have computed the lengths of those paths before, and we can compute the length of the shortest path between $i$ and $j$ as $d[i][k] + d[k][j]$.\n\n\n```cpp\nfor (int k = 0; k < n; ++k) {\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < n; ++j) {\n            d[i][j] = min(d[i][j], d[i][k] + d[k][j]); \n        }\n    }\n}\n```\n\n## Number of paths of fixed length / Shortest paths of fixed length\n\n\n# Spanning trees\n    Minimum Spanning Tree - Prim's Algorithm\n    Minimum Spanning Tree - Kruskal\n    Minimum Spanning Tree - Kruskal with Disjoint Set Union\n    Second best Minimum Spanning Tree - Using Kruskal and Lowest Common Ancestor\n    Kirchhoff Theorem\n    Prüfer code\n# Cycles\n## Checking a graph for acyclicity and finding a cycle in O(M)\n\n- nonlocal variables cycle and visited\n- keep path variable tracing the dfs\n- CLRS and CP algo use coloring, white, gray, black and nonlocal cycle variable (i think we cannot avoid nonlocal stuff)\n- **NB!!!** below is for cycle in directed graph, for **undirected** need to keep parent pointers and make sure you when you go back to parent you don't consider it as cycle\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef dfs(s,path):\n    nonlocal cycle\n    visited.add(s)\n    for u in adj[s]:\n        if u not in visited:\n            dfs(u,path|{u})\n        elif u in path:\n            cycle = True\n            return\ncycle,visited = False,set()\nfor s in range(n):\n    if s not in visited:\n        dfs(s,set([s]))\nprint(cycle)\n```\n</details>\n\n- To get the nodes in the cycle you might keep a `parent` dictionary and do a while loop instead of marking cycle = True\n- Alternatively there is this trick to get the cycle nodes:\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```python\n# step 1: backtracking DFS to find the cycle\ncircle = []\nvis = set()\n\ndef find_circle(node, par):\n    if node in vis:\n        return (True, node)\n    for nei in g[node]:\n        if nei == par: continue\n        vis.add(node)\n        circle.append(node)\n        status, res = find_circle(nei, node)\n        if status: return status, res\n        circle.pop()\n        vis.remove(node)\n\n    return False, None\n\n\n_, node = find_circle(0, None)\n# get the circle from start \"node\"\ncircle = circle[circle.index(node):] \n```\n</details>\n\n\n\n#QED\n\n## Finding a Negative Cycle in the Graph\n\n\n## Eulerian Path\n\nEuler path\n\nEulerian Path is a path in a graph that visits every edge exactly once. \n\n- [leetcode](https://leetcode.com/problems/valid-arrangement-of-pairs/)\n```Python\ndef hierholzer_recursive(graph):\n    def visit(vertex, circuit):\n        while graph[vertex]:\n            next_vertex = graph[vertex].pop()\n            visit(next_vertex, circuit)\n        circuit.append(vertex)\n\n    circuit = []\n    start_vertex = list(graph.keys())[0]\n    visit(start_vertex, circuit)\n    circuit.reverse()\n    return circuit\n\n```\nTo properly initialize the start vertex in the recursive version of Hierholzer's algorithm, you can modify the code to select a vertex with an odd degree (if one exists) as the starting point. This ensures that the algorithm will find an Eulerian circuit if one exists in the graph.\n\n\n\n# Lowest common ancestor\n## Lowest Common Ancestor\nGiven a tree $G$ find the lowest common ancestor of two nodes $(u,v)$. If you have to do just once it is easy, see [lca](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef lca(root, p, q):\n    if not root or root == p or root == q:\n        return root\n    l,r = lca(root.left),lca(root.right)\n    if l and r: return root\n    return l if l else r\n```\n</details>\n\nMore interesting is when you have **queries** $(u_i,v_i)$. Note lca lies on shortest path.\n\n\n## Hamiltonian path\n\nA Hamiltonian path or traceable path is a path that visits each vertex of the graph exactly once. A graph that contains a Hamiltonian path is called a traceable graph.\n\nWhether Hamiltonian path exists in a graph or not is NP-complete problem.\n\n## Binary Lifting\n\n![](assets/images/o(logn).png)\n\nWith Binary tree lifting you can answer questions such as given a tree and a node inn the tree, what is its k-th ancestor.\n\nBrute force is to repeat $k$ times `v = parent[v]` -> complexity $O(Q * N)$ if we have $Q$ queries and the tree is very deep.\n\nBinary tree lifting is also known as jump pointers. [Errihto](https://www.youtube.com/watch?v=oib-XsjFa-M&t=579s&ab_channel=Errichto).\n\n**There are $log(n)$ powers of 2.**\n\n\n**Preprocessing**\n\n**Define** $u[v][j]$ is the $2^j$-th ancestor of v.\n\n$u[v][j]$ is the $2^j$-th ancestor of v.\n$u[v][0] = parent[v]$ - that is my first parent\n$u[v][1] = u[u[v][0]][0]$ - what is my second parent? first parent of my first parent\n$u[v][2] = u[u[v][1]][1]$ - what is my fourth parent? second parent of my second parent\n\n`u` is a map that tells me for each node what is the 1st, 2nd, 4th, 8th ... ancestor.\n\n\n```Python\n# parent[i] if the parent of node i\nlog = len(bin(n))\nparent[0] = 0 # root\nfor i in range(len(parent)):\n    up[i][0] = parent[i]\nfor j in range(1,log):\n    for i in range(n):\n        up[i][j] = up[up[i][j-1]][j-1]\n        \ndef getKthAncestor(node: int, k: int) -> int:\n    # if depth[node] < k: return -1\n    for j in range(log):\n        if k & (1<<j):\n            node = up[node][j]\n    return node\n```\n\ncareful with for loops, you might need $parent[i] < i$ to do preprocessing (computing) matrix $u$ (depending on your loop order).  \n\n- [Kth ancestor](https://leetcode.com/problems/kth-ancestor-of-a-tree-node/) \n\n**Complexity:** $O(nlog(n))$ on preprocessing and $O(log(n))$ per query.\n\n## Lowest Common Ancestor - Binary Lifting\n\n\n\n\n**Lowest common ancestor**\nFor queries with nodes $(u,v)$ we want to get the lowest common ancestor of $u$ and $v$.\n\nIdea: Run dfs, record for each nodes `timein` and `timeout`. This helps to answer if $u$ is ancestor ov $v$ or vice versa.\n\nStart from the top $up[u][L]$ = the highest ancestor of $u$. Decrement $L$ checking if $up[u][i]$ is ancestor of $v$. Goal is to find highest ancestor of u which is not ancestor of $v$, return $up[u][0]$.\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef is_ancestor(p,q):\n    return timein[p] <= timein[q] and timeout[p] >= timeout[q]\n\ndef lca(p,q):\n    if is_ancestor(p,q): return p\n    if is_ancestor(q,p): return q\n    for i in range(l-1,-1,-1):\n        if not is_ancestor(up[p][i],q):\n            p = up[p][i]\n    return up[p][0]\n\ndef dfs(root,p):\n    nonlocal time\n    if not root: return\n    timein[root] = time\n    time += 1\n    up[root] = [None]*l\n    up[root][0] = p\n    for i in range(1,l):\n        up[root][i] = up[up[root][i-1]][i-1]\n    dfs(root.left, root)\n    dfs(root.right, root)\n    timeout[root] = time\n    \ntime,l = 0,20 # supports 2**19 nodes\nup,timein,timeout = {},{},{}\ndfs(root,root)\n    \nprint(lca(p,q)) # lowest common ancestor in O(log(n))\n```\n</details>\n\nProblem where you need to use optimised LCA:\n- [queries in tree](https://leetcode.com/problems/minimum-edge-weight-equilibrium-queries-in-a-tree/)\n\n\n- [longest good segement](https://www.codechef.com/problems/LGSEG)\n\n\n- semi brute force, for each index i (starting index), find largest good segment\n- $O(n^2)$\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef solve(nums,N,K,S):\n    def compute(i):\n        seg,curr,j = 1,nums[i],i+1\n        while seg <= K and j<N:\n            if curr + nums[j] > S:\n                curr = 0\n                seg += 1\n            curr += nums[j]\n            j += 1\n        return j-i-(seg>K)\n    return max(compute(i) for i in range(N))\n```\n</details>\n\n- binary lifting = jumps\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n\ndef build(N,start_index):\n    up = [[None]*N for _ in range(20)]\n    up[0] = start_index\n    for i in range(1,20):\n        for j in range(N):\n            p = up[i-1][j]\n            if p == -1:\n                up[i][j] = -1\n            else:\n                up[i][j] = up[i-1][p]\n    return up\n\ndef call(up, node, K):\n    last,jump = node,1\n    for i in range(19):\n        if node == -1: break\n        if K & jump:\n            node = up[i][node]\n        jump <<= 1\n    return last-node\n\ndef solve2(nums,N,K,S):\n    start_index,j,curr = [],0,0\n    for i in range(N):\n        curr += nums[i]\n        while curr > S:\n            curr -= nums[j]\n            j += 1\n        start_index.append(j-1)\n    \n    up = build(N,start_index)\n    res = 0\n    for i in range(N-1,-1,-1):\n        res = max(res, call(up,i,K))\n    return res\n\n```\n</details>\n\n#QED\n\n## Lowest Common Ancestor - Farach-Colton and Bender algorithm\n## Solve RMQ by finding LCA\n\n## Lowest Common Ancestor - Tarjan's off-line algorithm\n\n-----------------------------------------\n# Flows and related problems\n    Maximum flow - Ford-Fulkerson and Edmonds-Karp\n    Maximum flow - Push-relabel algorithm\n    Maximum flow - Push-relabel algorithm improved\n    Maximum flow - Dinic's algorithm\n    Maximum flow - MPM algorithm\n    Flows with demands\n    Minimum-cost flow\n    Assignment problem\n    - [min XOR sum](https://leetcode.com/problems/minimum-xor-sum-of-two-arrays/)\n    - hungarian algo?\nMatchings and related problems\n    Bipartite Graph Check\n    Kuhn's Algorithm - Maximum Bipartite Matching\n# Miscellaneous\n## Topological Sorting\nFor DAGs only. Topo sort exists only if there are no cycles in the DAG.\n- use dfs\n- after you have exosted vertex `s`, append it as all its descendants have been visited.\n- think of exit/finish times\n- need to reverse answer in the end\n- below version we also track if there is a cycle\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef dfs(s,path):\n    nonlocal cycle\n    visited.add(s)\n    for u in adj[s]:\n        if u not in visited:\n            dfs(u,path|{s})\n        elif u in path:\n            cycle = True\n            return\n    res.append(s) # all topo sort needs, the rest are for cycle tracking\ncycle,res,visited = False,[],set()\nfor s in range(n):\n    if s not in visited:\n        dfs(s,set([s]))\nif cycle: print([])\nprint(res[::-1])\n```\n</details>\n\n#QED\n\n## Edge connectivity / Vertex connectivity\n## Tree painting\n## 2-SAT\n## Heavy-light decomposition","n":0.022}}},{"i":152,"$":{"0":{"v":"Geometry","n":1},"1":{"v":"\n\n# Elementary operations\n## Basic Geometry\n- cross product checks for collinearity (line on one line)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef collinear(u,v):\n    sm = 0\n    for i in range(len(u)):\n        sm += u[i]*v[i]\n    return sm == 0\n```\n</details>\n\n# Finding the equation of a line for a segment\n- [leetcode](https://leetcode.com/problems/check-if-it-is-a-straight-line/)\nvertical lines have infinite slope, to avoid edge cases use above representaion of a line $Ax+By+C = 0$\n\n# Minimum enclosing circle\n\n$O(N)$\n[Erect the fence](https://leetcode.com/problems/erect-the-fence-ii/)\n\n```python\ndef circle_three(arr):\n    \"\"\"arr is a list of 3 points\"\"\"\n    (x0, y0), (x1, y1), (x2, y2) = arr\n    A = x0*(y1-y2) - y0*(x1-x2) + x1*y2 - x2*y1\n    B = (x0*x0 + y0*y0)*(y2-y1) + (x1*x1 + y1*y1)*(y0-y2) + (x2*x2+y2*y2)*(y1-y0)\n    C = (x0*x0 + y0*y0)*(x1-x2) + (x1*x1 + y1*y1)*(x2-x0) + (x2*x2+y2*y2)*(x0-x1)\n    D = (x0*x0 + y0*y0)*(x2*y1-x1*y2) + (x1*x1+y1*y1)*(x0*y2-x2*y0) + (x2*x2+y2*y2)*(x1*y0-x0*y1)\n    return (-B/(2*A), -C/(2*A), sqrt((B*B+C*C-4*A*D)/(4*A*A)))\n\ndef inside(p,x,y,r):\n    \"\"\"return: is point p inside circle (x,y,r)\"\"\"\n    return (p[0]-x)**2+(p[1]-y)**2 < r**2\n\ndef dist(a,b):\n    return ((a[0]-b[0])**2+(a[1]-b[1])**2)**0.5\n\ndef mec(i,R):\n    \"\"\"\n    Minimum enclosing circle using Welzl’s algorithm\n    R = points which need to be on the boundary\n    return: MEC of all points[:i+1] where points in R are on the boundary\n    \"\"\"\n    if len(R) == 3:\n        return circle_three(R)\n    if i == len(points):\n        if len(R) == 0: return (0,0,0)\n        elif len(R) == 1: return (R[0][0],R[0][1],0)\n        elif len(R) == 2: \n            return (R[0][0]+R[1][0])/2,(R[0][1]+R[1][1])/2,dist(R[0],R[1])/2\n\n    x,y,r = mec(i+1,R)\n    if inside(points[i],x,y,r): return (x,y,r)\n    return mec(i+1,R+[points[i]])\n\npoints = list(set(tuple(p) for p in points))\nprint(\"Result:\", mec(0,[]))\n```\n        Intersection Point of Lines\n        Check if two segments intersect\n        Intersection of Segments\n        Circle-Line Intersection\n        Circle-Circle Intersection\n        Common tangents to two circles\n        Length of the union of segments\n# Polygons\n        Oriented area of a triangle\n        Area of simple polygon\n        Check if points belong to the convex polygon in O(log N)\n## Check if convex polygon\n[problem](https://leetcode.com/problems/convex-polygon/)\n- cross product of three points (a,b,c) sign shows the direction of the turn\n\n        Minkowski sum of convex polygons\n        Pick's Theorem - area of lattice polygons\n        Lattice points of non-lattice polygon\n    Convex hull\n        Convex hull construction\n        Convex hull trick and Li Chao tree\n# Sweep-line\n## Search for a pair of intersecting segments\n- [perfect rectangle](https://leetcode.com/problems/perfect-rectangle/)\nGiven  a list of rectangles we want to find if there is any overlap of two rectangles.\n\n- Line sweep x-axis and add y1,y2 intervals into a SortedList data structure\n\n\n\n## Point location in O(log N)\n    Miscellaneous\n        Finding the nearest pair of points\n        Delaunay triangulation and Voronoi diagram\n        Vertical decomposition\n        Half-plane intersection - S&I Algorithm in O(N log N)\n","n":0.052}}},{"i":153,"$":{"0":{"v":"Dynamic Programming","n":0.707},"1":{"v":"\n#Zadachi\n- [combination sum](https://leetcode.com/problems/combination-sum-iv)\n- [remove boxes](https://leetcode.com/problems/remove-boxes/)\n- [dungeon game](https://leetcode.com/problems/dungeon-game/)\n- [domino tromino](https://leetcode.com/problems/domino-and-tromino-tiling/)\n- [merge stones](https://leetcode.com/problems/minimum-cost-to-merge-stones/)\n- [smallest sufficient team](https://leetcode.com/problems/smallest-sufficient-team/)\n- [student exams](https://leetcode.com/problems/maximum-students-taking-exam/)\n- [stone game 2](https://leetcode.com/problems/stone-game-ii/)\n- [hats to different people](https://leetcode.com/problems/number-of-ways-to-wear-different-hats-to-each-other/)\n- [binary tree cameras](https://leetcode.com/problems/binary-tree-cameras/?envType=study-plan&id=dynamic-programming-iv)\n- [constraint sum](https://leetcode.com/problems/constrained-subsequence-sum/)\n- [arithmetic subsequences](https://leetcode.com/problems/arithmetic-slices-ii-subsequence/)\n- [numbers with repeated digits](https://leetcode.com/problems/numbers-with-repeated-digits/)\n- [encode string](https://leetcode.com/problems/encode-string-with-shortest-length/)\n- [maximum profit in job scheduling](https://leetcode.com/problems/maximum-profit-in-job-scheduling/)\n- [encode string with shortest length](https://leetcode.com/problems/encode-string-with-shortest-length/)\n- [grid with 3 different colors](https://leetcode.com/problems/painting-a-grid-with-three-different-colors/)\n- [sum of distances in tree](https://leetcode.com/problems/sum-of-distances-in-tree/)\n- [count palindromes](https://leetcode.com/problems/count-palindromic-subsequences/)\n- [substrings diff 1](https://leetcode.com/problems/count-substrings-that-differ-by-one-character/submissions/)\nFibonacci numbers in $O(log(n))$ - matrix multiplication, golden ratio ideas, see solution tab in [Climbing stairs](https://leetcode.com/problems/climbing-stairs/)\n- [good binary strings](https://leetcode.com/problems/number-of-good-binary-strings/submissions/)\n- [least number of lines cover all points](https://leetcode.com/problems/minimum-number-of-lines-to-cover-points/)\n- [max nonegative path](https://leetcode.com/problems/maximum-non-negative-product-in-a-matrix/)\n- [Minimum flips in evaluation tree](https://leetcode.com/problems/minimum-flips-in-binary-tree-to-get-result/)\n- [Apples in pizza](https://leetcode.com/problems/number-of-ways-of-cutting-a-pizza/)\n- [Pyramids count](https://leetcode.com/problems/count-fertile-pyramids-in-a-land/)\n- [Music playlist](https://leetcode.com/problems/number-of-music-playlists/)\n- [Minimum cost to connect groups](https://leetcode.com/problems/minimum-cost-to-connect-two-groups-of-points/)\n- [Min Cost to make all chars equals](https://leetcode.com/problems/minimum-cost-to-make-all-characters-equal/)\n- [Count k-free subsets](https://leetcode.com/problems/count-the-number-of-k-free-subsets/)\n- [Type on keyboard using 2 fingers](https://leetcode.com/problems/minimum-distance-to-type-a-word-using-two-fingers/)\n- [make sorted columns](https://leetcode.com/problems/delete-columns-to-make-sorted-iii/)\n- [Dice roll simulation](https://leetcode.com/problems/dice-roll-simulation/)\n- [Longest arithmetic sequence](https://leetcode.com/problems/longest-arithmetic-subsequence/)\n- [First day](https://leetcode.com/problems/first-day-where-you-have-been-in-all-the-rooms/)\n- [Minuimum time to finish race](https://leetcode.com/problems/minimum-time-to-finish-the-race/)\n- [Painting the Walls](https://leetcode.com/problems/painting-the-walls/)\n- [Count Stepping numbers](https://leetcode.com/problems/count-stepping-numbers-in-range/)\n- [Max profit as a salesman](https://leetcode.com/problems/maximize-the-profit-as-the-salesman/)\n- [longest sequence](https://leetcode.com/problems/longest-unequal-adjacent-groups-subsequence-ii/description/)\n- [max profitable triplets](https://leetcode.com/problems/maximum-profitable-triplets-with-increasing-prices-i/description/)\n- [beautiful numbers, digit dp](https://leetcode.com/problems/count-beautiful-numbers/description/)\n- [beautiful numbers, digit dp](https://leetcode.com/problems/number-of-beautiful-integers-in-the-range/)","n":0.079}}},{"i":154,"$":{"0":{"v":"Data Structures","n":0.707},"1":{"v":"\n# Fundamentals\n\n\n## Hash Tables\nTags: Hashing, Open Addressing, Open-Addressing, Chaining\n\n[Design HashSet](https://leetcode.com/problems/design-hashset/)\n\n```Python\n# Chaining\nclass MyHashSet:\n\n    def __init__(self):\n        self.buckets = [[] for _ in range(1999)]\n        \n    def hash(self, key):\n        return key%len(self.buckets)\n        \n    def add(self, key: int) -> None:\n        i = self.hash(key)\n        self.remove(key)\n        self.buckets[i].append(key)\n        \n    def remove(self, key: int) -> None:\n        if self.contains(key):\n            i = self.hash(key)\n            self.buckets[i].remove(key)\n            \n    def contains(self, key: int) -> bool:\n        i = self.hash(key)\n        return key in self.buckets[i]\n\n# Open addressing\nclass MyHashSet:\n\n    def __init__(self):\n        self.size = 10069\n        self.hash_table = [None for _ in range(self.size)]\n\n    def hash(self, key, probe):\n        return (key%1999 + probe + probe**2) % self.size\n    \n    def add(self, key: int) -> None:\n        for probe in range(self.size):\n            i = self.hash(key,probe)\n            if self.hash_table[i] in [key,-1,None]:\n                break\n        self.hash_table[i] = key\n        \n    def remove(self, key: int) -> None:\n        for probe in range(self.size):\n            i = self.hash(key,probe)\n            if self.hash_table[i] in [key,-1,None]:\n                self.hash_table[i] = -1\n                break    \n                \n    def contains(self, key: int) -> bool:\n        for probe in range(self.size):\n            i  = self.hash(key, probe)\n            if self.hash_table[i] == None:\n                return False\n            elif self.hash_table[i] == key:\n                return True\n        return False\n        \n        \n```\n\n\n## Minimum Stack / Minimum Queue\n- monotonic queue, [p1](https://leetcode.com/problems/constrained-subsequence-sum/), the cnt variable below defines the enqueue priority, can have different priority implementations, e.g in max sliding window [problem](https://leetcode.com/problems/sliding-window-maximum/) it would be the index of the element\n\nQueue solves sliding window minimum problem, which means that we should report the smallest value inside each window.\n\nStack solves next nearest element problem\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Monoqueue(collections.deque):\n    def enqueue(self, val):\n        count = 1 # counts the number of elements which value is greater or equal than\n        while self and self[-1][0] < val:\n            count += self.pop()[1]\n        self.append([val, count])\n\n    def dequeue(self):\n        ans = self.max()\n        self[0][1] == 1\n        if self[0][1] =S= 0:\n            self.popleft()\n        return ans\n\n    def max(self):\n        return self[0][0] if self else 0\n\nclass MonoQueue(collections.deque):\n    def enqueue(self,i,num): # enqueue dequeu depending on index value, useful when you need monotonic queue used as sliding window\n        while self and self[-1][1] <= num:\n            self.pop()\n        self.append((i,num))\n    def dequeue(self,i):\n        if self and self[0][0] <= i:\n            self.popleft()\n    def max(self):\n        if not self: return 0\n        return self[0][1]\n```\n</details>\n\n- [max stack](https://leetcode.com/problems/max-stack/)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass MaxStack:\n\n    def __init__(self):\n        self.heap = []\n        self.stack = []\n        self.del_stack = set()\n        self.del_heap = set()\n        self.id = 0\n        \n    def push(self, x: int) -> None:\n        heappush(self.heap,(-x,-self.id))\n        self.stack.append((x,self.id))\n        self.id += 1\n        \n    def pop(self) -> int:\n        self._update_stack()\n        self.del_heap.add(self.stack[-1][1])\n        return self.stack.pop()[0]\n\n    def top(self) -> int:\n        self._update_stack()\n        return self.stack[-1][0]\n\n    def peekMax(self) -> int:\n        self._update_heap()\n        return -self.heap[0][0]\n\n    def popMax(self) -> int:\n        self._update_heap()\n        self.del_stack.add(-self.heap[0][1])\n        return -heappop(self.heap)[0]\n\n    def _update_heap(self):\n        while self.heap and -self.heap[0][1] in self.del_heap:\n            heappop(self.heap)\n            \n    def _update_stack(self):\n        while self.stack and self.stack[-1][1] in self.del_stack:\n            self.stack.pop()\n\n```\n</details>\n\n\n- Most Recently Used Queue [p](https://leetcode.com/problems/design-most-recently-used-queue/)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# O(nlogn) initialization, O(logn) fetch\nfrom sortedcontainers import SortedList\n\nclass MRUQueue:\n\n    def __init__(self, n: int):\n        self.ls = SortedList([(i-1,i) for i in range(1,n+1)])\n        self.rank = n\n        \n    def fetch(self, k: int) -> int:\n        res = self.ls.pop(k-1)\n        res = res[1]\n        self.ls.add((self.rank,res))\n        self.rank += 1\n        return res\n\n\n# BIT solutions are hard to come up with?\nclass BIT:\n    \n    def __init__(self, n) -> None:\n        self.bit = [0]*(n+1)\n\n    def add(self, index, delta) -> None:\n        index += 1\n        while index < len(self.bit):\n            self.bit[index] += delta\n            index += index & -index\n        \n    def query(self, index) -> int:\n        res = 0\n        while index:\n            res += self.bit[index]\n            index -= index & -index\n        return res\n\n# O(NlogN) initialization, O(log^2n)fetch\nclass MRUQueue:\n\n    def __init__(self, n: int):\n        self.bit = BIT(n+2000)\n        self.vals = [0]*(n+2000)\n        for i in range(n):BIT\n            self.vals[i] = i+1\n            self.bit.add(i,1)\n        self.size = n\n    \n    # O(log^2n)\n    def fetch(self, k: int) -> int:\n        l,r = 1, self.size\n        while l<r:\n            m = l+r >> 1 \n            if self.bit.query(m) >= k:\n                r = m\n            else:\n                l = m+1\n        self.bit.add(l-1, -1)\n        self.bit.add(self.size, 1)\n        self.vals[self.size] = self.vals[l-1]\n        self.size += 1\n        return self.vals[l-1]\n    \n# Square root decomposition technique - O(n) init, O(sqrt(n)) fetch\nclass MRUQueue:\n\n    def __init__(self, n: int):\n        self.buckets = []\n        self.indecies = []\n        self.n = n\n        self.nn = int(n**0.5)\n        for i in range(1,n+1):\n            ii = (i-1)//self.nn\n            if ii == len(self.buckets):\n                self.indecies.append(i)\n                self.buckets.append([])\n            self.buckets[-1].append(i)\n            \n    def fetch(self, k: int) -> int:\n        i = self._bs(self.indecies, k)-1\n        res = self.buckets[i].pop(k-self.indecies[i])\n        for ii in range(i+1,len(self.indecies)):\n            self.indecies[ii] -= 1\n            \n        if len(self.buckets[-1]) >= self.nn:\n            self.buckets.append([])\n            self.indecies.append(self.n)\n        self.buckets[-1].append(res)\n        \n        if not self.buckets[i]:\n            self.buckets.pop(i)\n            self.indecies.pop(i)\n            \n        return res\n        \n    def _bs(self, nums, num):\n        l,r = 1,len(nums)\n        while l<r:\n            m = l+r>>1\n            if nums[m] > num:\n                r = m\n            else:\n                l = m+1\n        return l\n```\n</details>\n\n\n- [heights queue](https://leetcode.com/problems/number-of-visible-people-in-a-queue/)\n\n## Sparse Table\n\n# Trees\n\n## Trie - Prefix tree\n\n\nin a list of $n$ words $w_{1},w_{2}, ... w_{n}$ we check if the word t is a prefix of any of these words\n- $O(len(t))$ look up\n- $O(len(w))$ for insert\n\n```Python\nclass Trie:\n\n    def __init__(self):\n        self.children = defaultdict(Trie)\n        self.is_end = False\n\n    def insert(self, word: str) -> None:\n        node = self\n        for ch in word:\n            node = node.children[ch]\n        node.is_end = True\n\n    def search(self, word: str) -> bool:\n        node = self\n        for ch in word:\n            if ch in node.children:\n                node = node.children[ch]\n            else:\n                 return False\n        return node.is_end\n\n    def startsWith(self, prefix: str) -> bool:\n        node = self\n        for ch in prefix:\n            if ch in node.children:\n                node = node.children[ch]\n            else:\n                 return False\n        return True\n        \n\n\n# Your Trie object will be instantiated and called as such:\n# obj = Trie()\n# obj.insert(word)\n# param_2 = obj.search(word)\n# param_3 = obj.startsWith(prefix)\n```\n\n## Disjoint Set Union = DSU = Union Find\n- Complexity:\n- If we make $N$ requestis to the union method it would take\n- $O(alpha(N))$ amortised time per ops and alpha is the Inverse-Ackermann function. This is approximately constant\n- To perform a sequence of m addition, union, or find operations on a disjoint-set forest with n nodes requires total time\n- $O(mα(n))$, where $α(n)$ is the extremely slow-growing inverse Ackermann function.\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass DSU:\n    def __init__(self, n):\n        self.parent = [i for i in range(n)]\n        self.rank = [0 for _ in range(n)]\n\n    # path compression\n    def find(self, x: int) -> int:\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n\n    # keep tree's rank small\n    def union(self, x: int, y: int) -> None:\n        u, v = self.find(x), self.find(y)\n        if self.rank[u] < self.rank[v]:\n            self.parent[u] = v\n        elif self.rank[u] > self.rank[v]:\n            self.parent[v] = u\n        else:\n            self.parent[v] = u\n            self.rank[u] += 1\n\n```\n</details>\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass DSU:\n    \n    def __init__(self):\n        self.parent = {}\n        self.rank = {}\n    \n    def add(self,x):\n        if x not in self.parent:\n            self.parent[x] = x\n            self.rank[x] = 0\n    \n    # path compression\n    def find(self, x):\n        if self.parent[x] != x:\n            self.parent[x] = self.find(self.parent[x])\n        return self.parent[x]\n\n    # keep tree's rank small\n    def union(self, x, y) -> None:\n        u, v = self.find(x), self.find(y)\n        if self.rank[u] < self.rank[v]:\n            self.parent[u] = v\n        elif self.rank[u] > self.rank[v]:\n            self.parent[v] = u\n        else:\n            self.parent[v] = u\n            self.rank[u] += 1\n```\n</details>\n\n- [Google onsite question](https://leetcode.com/problems/number-of-good-paths/)\n\n## Balanced binary search tree\n\n```Python\nfrom sortedcontainers import SortedList\n```\n#Zadachi\n- [Data Stream as Disjoint Intervals](https://leetcode.com/problems/data-stream-as-disjoint-intervals/)\n- [Closest Room](https://leetcode.com/problems/closest-room/)\n- [Traffic lights](https://cses.fi/problemset/task/1163/)\n- [Movie Festiva](https://usaco.guide/problems/cses-1632-movie-festival-ii/solution)\n## Fenwick Tree = BIT = Binary index tree\n\n- light weight BIT\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass BIT:\n    def __init__(self,n):\n        self.bit = [0]*(n+1)\n    def update(self,i,val):\n        '''adds val to nums[i]'''\n        i += 1\n        while i<len(self.bit):\n            self.bit[i] += val\n            i += (i & -i)\n    def query(self,i):\n        '''sum(nums[:i+1])'''\n        i += 1\n        res = 0\n        while i:\n            res += self.bit[i]\n            i -= (i & -i)\n        return res\n        \n    def sum_range(self, l, r):\n        '''sum(nums[l:r+1])'''\n        return self.query(r) - self.query(l-1)\n```\n</details>\n\n- BIT, 1D, [problem](https://leetcode.com/problems/range-sum-query-mutable/), [problem](https://leetcode.com/problems/count-of-smaller-numbers-after-self/?envType=study-plan&id=algorithm-iii)\n- supports cumulutaive computations only on functions which have inverse like sum\n- min function has limited support. cannot do min_range(i,j) and also whenever you do an update the new value should be smaller than the old one\n- BIT needs functions which form a **group**, such as $\\Z$ with operator + \n- $\\Z$ and min form a semi-ring and that is not enough. \n\n- A Fenwick tree can support the following range operations:\n    - Point Update and Range Query (classical one with implementation below)\n    - Range Update and Point Query (initialize to 0-s, range update = update(l,x), update(r+1,-x), query(l) becomes a point query. Cumulative sum trick)\n    - Range Update and Range Query [math trick using two BIT-s](https://cp-algorithms.com/data_structures/fenwick.html#2-range-update-and-point-query)\n\n\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass BIT:\n    \n    def __init__(self, nums):\n        self.nums = nums\n        self.bit = [0]*(len(nums)+1)\n        for i in range(1,len(self.bit)):\n            self.bit[i] += nums[i-1]\n            if i + (i & -i) < len(self.bit):\n                self.bit[i + (i & -i)] += self.bit[i]\n\n    def update(self, i, val):\n        diff = val-self.nums[i]\n        self.nums[i] += diff\n        i += 1\n        while i < len(self.bit):\n            self.bit[i] += diff\n            i += (i & -i)\n\n    def query(self, i):\n        '''sum nums[:i+1] '''\n        i += 1\n        res = 0\n        while i:\n            res += self.bit[i]\n            i -= (i & -i)\n        return res\n\n    def sum_range(self, l, r):\n        return self.query(r) - self.query(l-1)\n    \nclass NumArray:\n\n    def __init__(self, nums: List[int]):\n        self.bit = BIT(nums)\n\n    def update(self, i: int, val: int) -> None:\n        self.bit.update(i,val)\n\n    def sumRange(self, l: int, r: int) -> int:\n        return self.bit.sum_range(l,r)\n\n```\n</details>\n\n- BIT, Fenwick Tree, Binary Index Tree, 2D, [problem](https://leetcode.com/problems/range-sum-query-2d-mutable/)\n- think BIT on x axis, then recursively create another BIT on Y axis.\n- $O(log(n) log(n))$ for updates and queries. Linear initialization is a bit tricky.\n- nesting loops in update and query methods\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass BIT:\n    \n    def __init__(self, mat):\n        self.nums = mat\n        self.bit = [[0]*(len(mat[0])+1) for _ in range(len(mat)+1)]\n        \n        # build O(n*m*logn*logm)\n        # self.mat = [[0]*len(mat[0]) for _ in range(len(mat))]\n        # self.bit = [[0]*(len(mat[0])+1) for _ in range(len(mat)+1)]\n        # for i in range(len(mat)):\n        #     for j in range(len(mat[0])):\n        #         self.update(i,j,mat[i][j])   \n        \n        # build O(m*n), order of loops matter\n        for i in range(1,len(self.bit)):\n            for j in range(1,len(self.bit[0])):\n                self.bit[i][j] += mat[i-1][j-1]\n                if self.next(i) < len(self.bit):\n                    self.bit[self.next(i)][j] += self.bit[i][j]\n        \n        for i in range(1,len(self.bit)):\n            for j in range(1,len(self.bit[0])):\n                if self.next(j) < len(self.bit[0]):\n                    self.bit[i][self.next(j)] += self.bit[i][j]\n\n    def next(self, i):\n        return i + (i&-i)\n    \n    def update(self, i, j, val):\n        diff = val - self.nums[i][j]\n        self.nums[i][j] += diff\n        i,j = i+1, j+1\n        while i < len(self.bit):\n            jj = j\n            while jj < len(self.bit[0]):\n                self.bit[i][jj] += diff\n                jj += (jj & -jj)\n            i += (i & -i)\n\n    def query(self, i, j):\n        res,i,j = 0,i+1,j+1\n        while i:\n            jj = j\n            while jj:\n                res += self.bit[i][jj]\n                jj -= (jj & -jj)\n            i -= (i & -i)\n        return res\n\n    def sum_range(self,i,j,x,y):\n        return self.query(x,y) - self.query(x,j-1) - self.query(i-1,y) + self.query(i-1,j-1) \n\nclass NumMatrix:\n\n    def __init__(self, mat: List[List[int]]):\n        self.bit = BIT(mat)\n\n    def update(self, i: int, j: int, val: int) -> None:\n        self.bit.update(i,j,val)\n\n    def sumRegion(self, i: int, j: int, x: int, y: int) -> int:\n        return self.bit.sum_range(i,j,x,y)\n\n```\n</details>\n\n\n## Sqrt Decomposition\n## Segment Tree\n\n- [Increments on subarrays](https://leetcode.com/problems/minimum-number-of-increments-on-subarrays-to-form-a-target-array/)\n\n- Questions: [Falling Squares](https://leetcode.com/problems/falling-squares/), [Skyline](https://leetcode.com/problems/the-skyline-problem/)\n\n- Segment Tree recursive, slower than iterative 2,3 times in practice\n- below is **point update, range query** - both $O(log(n))$\n- query can be sum, max, gcd. lcd etc (as long as it is a semi-ring)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n\nclass SegmentTree:\n    def __init__(self, update_fn, query_fn):\n        '''\n        for summation tree: query_fn = update_fn = lambda x,y: x+y \n        works if these two with the space of the values form a semi-ring\n        '''\n        self.UF, self.QF = update_fn, query_fn\n        self.T = defaultdict(int) # [0]*4*n\n\n    def update(self, v, tl, tr, pos, delta):\n        '''\n        v is the index of the node (we use 1-indexing, as v has children 2v, 2(v+1))\n        (v,tl,tr) is root node, e.g. (1,0,n-1)\n        The tree nodes are represented using the index v and INCLUSIVE intervals [tl,tr]\n        Updates SINGLE value at position pos by coposing delta with UF (e.g. adding delta)\n        '''\n        if tl == tr: \n            self.T[v] = self.UF(self.T[v], delta)\n        else:\n            tm = (tl + tr)//2\n            if pos <= tm:\n                self.update(v*2, tl, tm, pos, delta)\n            else:\n                self.update(v*2+1, tm+1, tr, pos, delta)\n            self.T[v] = self.QF(self.T[v*2], self.T[v*2+1])\n\n    def query(self, v, tl, tr, l, r):\n        '''\n        (v,tl,tr) is root node, e.g. (1,0,n-1)\n        returns QF[l:r+1], e.g. sum(nums[l:l+r]) if QF = lambda x,y: x+y \n        '''        \n        if l > r: return 0\n        if l == tl and r == tr: return self.T[v]\n        tm = (tl + tr)//2\n        return self.QF(self.query(v*2, tl, tm, l, min(r, tm)), self.query(v*2+1, tm+1, tr, max(l, tm+1), r))\n\nst = SegmentTree(lambda x,y:x+y, lambda x,y: x+y)\n```\n</details>\n\n- Segment tree with **range update, range query** - both $O(log(n))$\n- lazy update to have query in $O(logn)$\n- in both segment trees if you have been given array `nums` in advance you can do build in `__init__` in $O(n)$ time (recursively)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass SegmentTree:\n    def __init__(self, update_fn, query_fn):\n        self.UF, self.QF = update_fn, query_fn\n        self.T = defaultdict(int)   # [0] * (4*N)\n        self.L = defaultdict(int)   # [0] * (4*N), keep info for whole segment when making range updates\n \n    # lazy propagation\n    def push(self, v):\n        for u in [2*v, 2*v+1]:\n            self.T[u] = self.UF(self.T[u], self.L[v])\n            self.L[u] = self.UF(self.L[u], self.L[v])\n        self.L[v] = 0\n\n    def update(self, v, tl, tr, l, r, h):\n        '''changes nums[l,r+1]'''\n        if l > r: return\n        if l == tl and r == tr:\n            self.T[v] = self.UF(self.T[v], h)\n            self.L[v] = self.UF(self.L[v], h)\n        else:\n            self.push(v)\n            tm = (tl + tr)//2\n            self.update(v*2, tl, tm, l, min(r, tm), h)\n            self.update(v*2+1, tm+1, tr, max(l, tm+1), r, h)\n            self.T[v] = self.QF(self.T[v*2], self.T[v*2+1])\n\n    def query(self, v, tl, tr, l, r):\n        '''max(nums[l:r+1])'''\n        if l > r: return -float(\"inf\")\n        if l == tl and tr == r: return self.T[v]\n        self.push(v)\n        tm = (tl + tr)//2\n        return self.QF(self.query(v*2, tl, tm, l, min(r, tm)), self.query(v*2+1, tm+1, tr, max(l, tm+1), r))\n\n```\n</details>\n\n\n**Assignment on segments**\n\nSuppose now that the modification query asks to assign each element of a certain segment `a[l...r]` to some value $x$.\n\n- store at each vertex of the Segment Tree whether the corresponding segment is covered entirely with the same value or not. Augment the segment tree with `self.marked = defaultdict(bool)`\n- \"lazy\" update: instead of changing all segments in the tree that cover the query segment, we only change some, and leave others unchanged. \n\n- A marked vertex will mean, that every element of the corresponding segment is assigned to that value, and actually also the complete subtree should only contain this value.\n\nSmall problem: assume you do `update(0,n-1)` and you keep info only in the root. Then you do a second `update(0,n//2)`. the info in the root is irrelevant as half of the values are with one value and the other half with another.\n\nThe way to solve this is to push the information of the root to its children and then do the second update.\n\n- Question: [Range module](https://leetcode.com/problems/range-module/)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass SegmentTree:\n    def __init__(self):\n        self.T = defaultdict(bool)   # [0] * (4*N) takes values 0 or 1 whether segment is covered or not\n        self.marked = defaultdict(bool)\n        \n    # lazy propagation\n    def push(self, v):\n        if self.marked[v]:\n            for u in [2*v, 2*v+1]:\n                self.T[u] = self.T[v]\n                self.marked[u] = True\n            self.marked[v] = False\n\n    def update(self, v, tl, tr, l, r, h):\n        '''changes nums[l,r+1]'''\n        if l > r: return\n        if l == tl and r == tr:\n            self.T[v] = h\n            self.marked[v] = True\n        else:\n            self.push(v)\n            tm = (tl + tr)//2\n            self.update(v*2, tl, tm, l, min(r, tm), h)\n            self.update(v*2+1, tm+1, tr, max(l, tm+1), r, h)\n            self.T[v] = self.T[v*2] and self.T[v*2+1]\n\n    def query(self, v, tl, tr, l, r):\n        if l > r: return 1\n        if l == tl and tr == r: return self.T[v]\n        self.push(v)\n        tm = (tl + tr)//2\n        return self.query(v*2, tl, tm, l, min(r, tm)) and self.query(v*2+1, tm+1, tr, max(l, tm+1), r)\n\nclass RangeModule:\n\n    def __init__(self):\n        self.sl = SegmentTree()\n        self.n = 10**9+1\n        \n    def addRange(self, l: int, r: int) -> None:\n        self.sl.update(1,0,self.n-1,l,r-1,True)\n\n    def queryRange(self, l: int, r: int) -> bool:\n        return self.sl.query(1,0,self.n-1,l,r-1) == 1\n\n    def removeRange(self, l: int, r: int) -> None:\n        self.sl.update(1,0,self.n-1,l,r-1,False)\n```\n\n</details>\n\n## Treap\n## Sqrt Tree\n## Randomized Heap\n\nAdvanced\n    Deleting from a data structure in O(T(n) log n)\n\n\n\n## LRU cache\n\n`LRUCache(int capacity)` Initialize the LRU cache with positive size `capacity`.\n\n`int get(int key)` Return the value of the key if the key exists, otherwise return -1.\n\n`void put(int key, int value)` Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If reach capacity **evict** the least recently used key.\n\n**Algorithm:**\n\n- [LRU](https://leetcode.com/problems/lru-cache)\n- $O(1)$ amortised for put and get\n- use hashmap to map keys to nodes (nodes has key, val, prev and next)\n- use Dlink (represented below as `self.head` and `self.tail`) to track least recently used element (it would be at the tail)\n\n`__init__`\n```Python\nself.cache = {}\nself.cap = cap\nself.head = Node()\nself.tail = Node()\n```\n\n`get(key)`\n1. if key not in cache return -1\n2. else: update(key) \n3. return `self.cache[key].val`\n\n`put(key,val)`\n1. if key in cache: update(key) and change the val\n2. else: \n    - if cache is full: evict()\n    - add(key,val)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Node:\n    def __init__(self, key = None, val = None, next = None, prev = None):\n        self.key = key\n        self.val = val\n        self.next = next\n        self.prev = prev\n\nclass LRUCache:\n\n    def __init__(self, cap: int):\n        self.cache = {}\n        self.cap = cap\n        self.head = Node()\n        self.tail = Node()\n        self.link(self.head,self.tail)\n        \n    def get(self, key: int) -> int:\n        if key not in self.cache: return -1\n        self.update(key)\n        return self.cache[key].val\n\n    def put(self, key: int, val: int) -> None:\n        if key not in self.cache:\n            if len(self.cache) == self.cap:\n                self.evict()\n            self.add(key,val)\n        else:\n            self.remove(key)\n            self.add(key,val)\n    \n    def add(self,key,val):\n        node = Node(key,val)\n        self.link(node,self.head.next)\n        self.link(self.head,node)\n        self.cache[key] = node\n        \n    def remove(self,key):\n        node = self.cache[key]\n        self.link(node.prev,node.next)\n        del self.cache[key]\n    \n    def evict(self):\n        self.remove(self.tail.prev.key)\n        \n    def link(self,a,b):\n        a.next,b.prev = b,a\n    \n    def update(self,key):\n        node = self.cache[key]\n        self.link(node.prev,node.next)\n        self.link(node,self.head.next)\n        self.link(self.head,node)\n```\n\n</details>\n\n\n## LFU cache\n\n`LFUCache(int capacity)` Initializes the object with the `capacity` of the data structure.\n\n`int get(int key)` Gets the value of the key if the key exists in the cache. Otherwise, returns -1.\n\n`void put(int key, int value)` Update the value of the key if present, or inserts the key if not already present. If reach capacity **evict** least frequently used. Ties are resolved using least recently used. (LFU,LRU)\n\n- [LFU](https://leetcode.com/problems/lfu-cache/)\n- $O(1)$ amortised for put and get\n- idea: for every frequency create a doubly linked list (LRU idea)\n\n**Algorithm:**\n\n`get(key)`\n1. query the node by calling self._node[key]\n2. find the frequency by checking node.freq, assigned as f, and query the DLinkedList that this node is in, through calling self._freq[f]\n3. pop this node\n4. update node's frequence, append the node to the new DLinkedList with frequency f+1\n5. if the DLinkedList is empty and self._minfreq == f, update self._minfreq to f+1.\n6. return node.val\n\n`put(key, value)`\n1. If key is already in cache, do the same thing as get(key), and update node.val as value\n2. Otherwise:\n    - if the cache is full, pop the least frequenly used element (*)\n    - add new node to self._node\n    - add new node to self._freq[1]\n    - reset self._minfreq to 1\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Node:\n    def __init__(self, key=None, val=None, freq=1, prev=None, next=None):\n        self.key = key\n        self.val = val  \n        self.freq = freq\n        self.prev = prev    \n        self.next = next\n        \nclass DLink:\n    def __init__(self):\n        self.head = Node()\n        self.tail = Node()\n        self.link(self.head,self.tail)\n        \n    def pop(self,node):\n        self.link(node.prev,node.next)\n        \n    def append(self,node):\n        self.link(node,self.head.next)\n        self.link(self.head,node)\n    \n    def link(self,a,b):\n        a.next,b.prev = b,a\n    \n    def is_empty(self):\n        return self.head.next == self.tail\n\nclass LFUCache:\n\n    def __init__(self, cap: int):\n        self.cap = cap\n        self.cache = {}\n        self.freq = defaultdict(DLink)\n        self.min_freq = 0\n        \n    def get(self, key: int) -> int:\n        if key not in self.cache: return -1\n        self.update(key)\n        return self.cache[key].val\n\n    def put(self, key: int, val: int) -> None:\n        if self.cap == 0: return\n        if key in self.cache:\n            self.update(key)\n            self.cache[key].val = val\n        else:\n            if len(self.cache) == self.cap:\n                self.evict()\n            self.add(key,val)\n    \n    def update(self,key):\n        node = self.cache[key]\n        self.freq[node.freq].pop(node)\n        if self.freq[node.freq].is_empty() and self.min_freq == node.freq:\n            self.min_freq += 1\n        node.freq += 1\n        self.freq[node.freq].append(node)\n\n    def add(self, key, val):\n        node = Node(key,val)\n        self.freq[1].append(node)\n        self.cache[key] = node\n        self.min_freq = 1\n\n    def evict(self):\n        node = self.freq[self.min_freq].tail.prev\n        self.freq[self.min_freq].pop(node)\n        del self.cache[node.key]\n```\n</details>\n\nAdditionally, you can implement a dynamic balanced binary tree `SortedList()` solution by adding the notion of frequency and time. get and put would be $O(logn)$.\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nfrom sortedcontainers import SortedList\n\nclass LFUCache:\n\n    def __init__(self, capacity: int):\n        self.cap = capacity\n        self.cache = {}\n        self.sl = SortedList() # [counter,time,key,value]\n        self.time = 0\n        \n    def get(self, key: int) -> int:\n        if key not in self.cache: return -1\n        el = self.cache[key]\n        self._increase_count(el)\n        self.time += 1\n        # print(self.sl, self.cache)\n        return el[-1]\n\n    def put(self, key: int, value: int) -> None:\n        if self.cap == 0: return\n        old_freq = 0\n        if key in self.cache:\n            old_freq = self.cache[key][0]\n            self._delete(self.cache[key])\n            del self.cache[key]    \n            \n        if len(self.cache) == self.cap:\n            el = self.sl.pop(0)\n            del self.cache[el[2]]\n            \n        el = (1+old_freq, self.time, key, value)\n        self.cache[key] = el\n        self._add(el)        \n        self.time += 1\n            \n    def _add(self, el):\n        self.sl.add(el)\n        \n    def _delete(self, el):\n        self.sl.remove(el)\n        \n    def _increase_count(self, el):\n        self.sl.remove(el)\n        del self.cache[el[2]]\n        new_el = list(el)\n        new_el[0] += 1\n        new_el[1] = self.time\n        new_el = tuple(new_el)\n        self.sl.add(new_el)\n        self.cache[new_el[2]] = new_el \n```\n</details>\n\n\n# Problems using SortedList, SortedDict\n\n- []()","n":0.018}}},{"i":155,"$":{"0":{"v":"Combinatorics","n":1},"1":{"v":"    Fundamentals\n        Finding Power of Factorial Divisor\n        Binomial Coefficients\n        Catalan Numbers\n    Techniques\n        The Inclusion-Exclusion Principle\n        Burnside's lemma / Pólya enumeration theorem\n        Stars and bars\n        Generating all K-combinations\n    Tasks\n        Placing Bishops on a Chessboard\n        Balanced bracket sequences\n        Counting labeled graphs","n":0.162}}},{"i":156,"$":{"0":{"v":"Binary Tree Traversal","n":0.577},"1":{"v":"\nRecursive traversals are easy. You do a DFS and depending on pre, in, post order you record the node values before, between or after visiting the two children.\n\nIterative traversals are a bit trickier. A few principles you follow:\n- use a stack (every recursive algorithm can be emulated using a stack)\n- each iteration you pop the last node of the stack\n- in in order and post order traversal you need to bring back the node in the stack you you haven't visited left or both children\n- append to stack right and then left node to simulate DFS recursive\n\nWhy would you want to do iterative vs recursive algos? Because in Python the max Call stack is like 1000 depth (unless you explicitly increase it).\n\n# Pre order\n- [leetcode](https://leetcode.com/problems/binary-tree-preorder-traversal/)\n```python\n# Node Left Right\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        def dfs(root):\n            if not root: return\n            pre.append(root.val)\n            dfs(root.left)\n            dfs(root.right)\n        pre = []\n        dfs(root)\n        return pre\n# opposed to post or in order you don't need to check if children were passed thorugh\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        pre = []\n        stack = [root] if root else []\n        while stack:\n            node = stack.pop()\n            pre.append(node.val)\n            if node.right: stack.append(node.right)\n            if node.left: stack.append(node.left)\n        return pre\n```\n\n# In order\n-[leetcode](https://leetcode.com/problems/binary-tree-inorder-traversal/)\n```python\n# Left Node Right\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        def dfs(root):\n            if not root: return\n            dfs(root.left)\n            in_order.append(root.val)\n            dfs(root.right)\n        in_order = []\n        dfs(root)\n        return in_order\n\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        stack = [(root,False)] if root else []\n        in_order = []\n        while stack:\n            node, visited_left = stack.pop()\n            if visited_left:\n                in_order.append(node.val)\n                if node.right: stack.append((node.right,False))\n            else: # visit left if you haven't\n                stack.append((node,True))\n                if node.left: stack.append((node.left,False))\n        return in_order\n```\n# Post order\n-[leetcode](https://leetcode.com/problems/binary-tree-postorder-traversal/)\n```python\n# Left Right Node\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        def dfs(root):\n            if not root: return\n            dfs(root.left)\n            dfs(root.right)\n            post.append(root.val)\n        post = []\n        dfs(root)\n        return post\n\n# definition of post order is to visit children first and then visit node\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n        post = []\n        stack = [(root,False)] if root else []\n        while stack:\n            node, visited_children = stack.pop()\n            if visited_children:\n                post.append(node.val)\n            else:\n                stack.append((node, True))\n                if node.right: stack.append((node.right,False))\n                if node.left: stack.append((node.left,False))\n        return post\n```","n":0.054}}},{"i":157,"$":{"0":{"v":"Algebra","n":1},"1":{"v":"# Fundamentals\n        Binary Exponentiation\n        Factoring Exponentiation\n        Euclidean algorithm for computing the greatest common divisor\n        Extended Euclidean Algorithm\n        Linear Diophantine Equations\n## Fibonacci Numbers\n- [express k as a minimum number of Fibonacci numbers](https://leetcode.com/problems/find-the-minimum-number-of-fibonacci-numbers-whose-sum-is-k/)\n\n**Math behind why greedy works**\n\nAmong all resolution with the minimum number of Fibonacci numbers, we are to find the lexicographically largest one.\n\nFacts about shortest sequence:\n- uses each Fibonacci number at most once `fibo[i] * 2 = fibo[i - 2] + fibo[i + 1]`\n- never uses two consecutive Fibonacci numbers\n\nThen: If no dup, no adjacent, we must take the biggest.\n\n`fibo[0] + fibo[2] + fibo[4] + ... + fibo[2n] = fibo[2n + 1] - 1`\n\n`fibo[1] + fibo[3] + fibo[5] + .... + fibo[2n-1] = fibo[2n] - 1`\n\n\n# Prime numbers\n## Sieve of Eratosthenes\n\n\n- [leetcode problem](https://leetcode.com/problems/closest-prime-numbers-in-range/)\n```python\nsieve = [False,False]+[True]*(n-1)\nfor i in range(2,int(r**0.5)+2):\n    for j in range(i*i,n+1,i):\n        sieve[j] = False\nprimes = [i for i,p in enumerate(primes) if p]\n# O(nlog(logn))\n```\n\n- find all primes less or equal to n\n```python\nprimes = []\nfor d in range(2,n+1):\n    for p in primes:\n        if d%p == 0: break\n    else:\n        primes.append(d)\n```\n\n        Linear Sieve\n        Primality tests\n## Integer factorization\n\n- trial division\n\n'smallest divisor should be prime'\n```python\n# O(sqrt(n))\ndef factorization(n):\n    res = []\n    for d in range(2,int(n**0.5)+1):\n        while n%d == 0:\n            res.append(d)\n            n //= d\n    return res\n```\n\n- simple sieve of eratosthenes + dp\n```python\n@cache\ndef factorization(num):\n    if num == 1: return [1]\n    if sieve[num]: return [num]\n    for p in range(2,num+1):\n        if sieve[p] and num%p==0: return [p]+factorization(num//p)\n\nsieve = [False,False]+[True]*(n-1)\nfor i in range(2,int(n**0.5)+2):\n    for j in range(i*i,n+1,i):\n        sieve[j] = False\n```\n\niterative implementation:\n\n```python\n\nsieve = [False,False]+[True]*(n-1)\nfor i in range(2,int(n**0.5)+2):\n    for j in range(i*i,n+1,i):\n        sieve[j] = False\nprimes = [p for p in range(len(sieve)) if sieve[p]]\nres = []\nfor d in primes:\n    if d * d > n: break\n    while n % d == 0:\n        res.append(d)\n        n //= d\nif n > 1:\n    res.append(n) # n-prime\n```\n- Fermat's factorization method\n- Pollard's  $p - 1$  method\n- Pollard's rho algorithm\n- Floyd's cycle-finding algorithm\n- Brent's algorithm\n\n\n# Number-theoretic functions\n## Euler's totient function\n## Number of divisors / sum of divisors\n    \n## Lagrange's four-square theorem\nAny number can be represented by at most 4 squares. Check proof in wikipedia.\n- [least squares sum](https://leetcode.com/problems/perfect-squares/)\n\n    \n# Modular arithmetic\n## Modular Inverse\n\nA [modular multiplicative inverse](http://en.wikipedia.org/wiki/Modular_multiplicative_inverse) of an integer $a$ is an integer $x$ such that $a \\cdot x$ is congruent to $1$ modular some modulus $m$.\nTo write it in a formal way: we want to find an integer $x$ so that \n\n$$a \\cdot x \\equiv 1 \\mod m.$$\n\nWe will also denote $x$ simply with $a^{-1}$.\n\nIt can be proven that the modular inverse exists if and only if $a$ and $m$ are relatively prime (i.e. $\\gcd(a, m) = 1$).\n\n**Finding the Modular Inverse using Binary Exponentiation**\n\n```python\npow(a,m-2,m)  # m must be prime/Fermat's little theorem\n```\n\n* For an arbitrary (but coprime) modulus $m$: $a ^ {\\phi (m) - 1} \\equiv a ^{-1} \\mod m$\n* For a prime modulus $m$: $a ^ {m - 2} \\equiv a ^ {-1} \\mod m$\n\n**Finding the Modular Inverse using Extended Euclidean algorithm**\n\nUse the identity eqaution (linear diophantine equation in two variables)\n\n$$a \\cdot x + m \\cdot y = 1$$\n\nTake modulu m both sides and you get the result.\n\nThe function below solves $ax+by = 1$, for given $a$ and $b$.\n```python\ndef extended_euclidean(a, b):\n    if b == 0:\n        return 1, 0, a\n    else:\n        x, y, gcd = extended_euclidean(b, a % b)\n        return y, x - (a // b) * y, gcd\n```\n[why this works](https://math.stackexchange.com/questions/747342/extended-euclidean-algorithm-for-modular-inverse)\n\n\n\n## Linear Congruence Equation\n## Chinese Remainder Theorem\n## Factorial modulo p\n## Discrete Log\n## Primitive Root\n## Discrete Root\n## Montgomery Multiplication\n#Number systems\n## Balanced Ternary\n## Gray code\n- [permute binary representation](https://leetcode.com/problems/circular-permutation-in-binary-representation/)\n\nNote how graycode construction adds 0-s and 1-s(reverses these) in the previous solution\n```python\nclass Solution:\n    def circularPermutation(self, n: int, s: int) -> List[int]:\n        graycode = [i ^ (i >> 1) for i in range(1<<n)]\n        i = graycode.index(s)\n        print(graycode)\n        return graycode[i:]+graycode[:i]\n    \nclass Solution:\n    def circularPermutation(self, n: int, s: int) -> List[int]:\n        return [s ^ i ^ (i >> 1) for i in range(1<<n)]\n    \nclass Solution:\n    def circularPermutation(self, n: int, start: int) -> List[int]:\n        # my construct of consecutive bits add 0 and 1 to previous solution\n        # this is the same as gray code...\n        def dp(i):\n            if i == 1: return ['0','1']\n            res,rev = [],[]\n            for code in dp(i-1):\n                res.append('0'+code)\n                rev.append('1'+code)\n            return res + rev[::-1]\n        res = dp(n)\n        res = [int(b,2) for b in res]\n        i = res.index(start)\n        return res[i:] + res[:i]\n```\n\nIt's not super obvious why $n$ XOR $(n>>1)$ and $(n+1)$ XOR $((n+1)>>1)$ would differ by 1 bit.\n\nXOR is commulative. we need to prove that $n$ XOR $(n>>1)$ XOR $(n+1)$ XOR $((n+1)>>1)$ is a power of two.\n\n$n$ XOR $(n>>1)$ and $(n+1)$ XOR $((n+1)>>1)$ $=$ $n$ XOR $(n+1)$ and $(n>>1)$ XOR $((n+1)>>1)$ $=$ $2^{k} - 1$ XOR $2^{k-1} - 1$\n\n## Finding inverse Gray code\n\nGiven Gray code $g$, restore the original number $n$, i.e. $g = n^(n>>1)$, given $g$ find $n$\n\nWe will move from the most significant bits to the least significant ones (the least significant bit has index 1 and the most significant bit has index $k$). The relation between the bits $n_i$ of number $n$ and the bits $g_i$ of number $g$:\n\n$n = n_{k}n_{k-1}...n_{1}$ in binary presentation $n_{i} \\in \\{0,1\\}$\n\nRewrite $n = g$ XOR $n>>1$ to get:\n\n$$\\begin{align}\n  n_k &= g_k, \\\\\n  n_{k-1} &= g_{k-1} \\oplus n_k = g_k \\oplus g_{k-1}, \\\\\n  n_{k-2} &= g_{k-2} \\oplus n_{k-1} = g_k \\oplus g_{k-1} \\oplus g_{k-2}, \\\\\n  n_{k-3} &= g_{k-3} \\oplus n_{k-2} = g_k \\oplus g_{k-1} \\oplus g_{k-2} \\oplus g_{k-3},\n  \\vdots\n\\end{align}$$\n\n```python\ndef gray_to_dec(g):\n    res = 0\n    while g:\n        res ^= g\n        g >>= 1\n    return res\n```\n# Miscellaneous\n## Enumerating submasks of a bitmask\nGiven a bitmask $m$, you want to efficiently iterate through all of its submasks, that is, masks $s$ in which only bits that were included in mask $m$ are set.\n\nConsider the implementation of this algorithm, based on tricks with bit operations:\n```python\nwhile sub:\n    print(sub)\n    sub = (sub-1) & m\n```\n\nThe above code visits all submasks of $m$, without repetition, and in descending order. Suppose we have a current bitmask $s$, and we want to move on to the next bitmask. By subtracting from the mask $s$ one unit, we will remove the rightmost set bit and all bits to the right of it will become 1. Then we remove all the \"extra\" one bits that are not included in the mask $m$ and therefore can't be a part of a submask.\n\n\n**Iterating through all masks with their submasks. Complexity $O(3^n)$**\n\nIn many problems, especially those that use bitmask dynamic programming, you want to iterate through all bitmasks and for each mask, iterate through all of its submasks:\n```cpp\nfor (int m=0; m<(1<<n); ++m):\n    get_all_submasks(m)\n```\n\nLet's prove that the inner loop will execute a total of $O(3^n)$ iterations.\n\n**First proof**: Consider the $i$-th bit. There are exactly three options for it:\n\n1. it is not included in the mask $m$ (and therefore not included in submask $s$),\n2. it is included in $m$, but not included in $s$, or\n3. it is included in both $m$ and $s$.\n\nAs there are a total of $n$ bits, there will be $3^n$ different combinations.\n\n**Second proof**: Note that if mask $m$ has $k$ enabled bits, then it will have $2^k$ submasks. As we have a total of $\\binom{n}{k}$ masks with $k$ enabled bits (see [binomial coefficients](../combinatorics/binomial-coefficients.md)), then the total number of combinations for all masks will be:\n\n$$\\sum_{k=0}^n \\binom{n}{k} \\cdot 2^k$$\n\nTo calculate this number, note that the sum above is equal to the expansion of $(1+2)^n$ using the binomial theorem. Therefore, we have $3^n$ combinations, as we wanted to prove.\n\n## Arbitrary-Precision Arithmetic\n## Fast Fourier transform\n## Operations on polynomials and series\n## Continued fractions","n":0.029}}},{"i":158,"$":{"0":{"v":"Tools","n":1},"1":{"v":"Tools for building web pages:\n\n- [docusaurus](https://docusaurus.io/), [example](https://wingkwong.github.io/leetcode-the-hard-way/)\n- streamlit\n- heroku for deploying\n\nTools for presentation:\n- plotly\n- quarto (rendering tool), better than Jupyter notebooks\n- streamlit\n- cudf (need cuda drivers) GPU usage for woring with dataframes\n- polars fast and memory efficient pandas\n- spark, pyspark","n":0.158}}},{"i":159,"$":{"0":{"v":"Shortcuts","n":1},"1":{"v":"\n**Browser**\n\n- Ctrl + T = create new tab\n- Ctrl + Alt + Left/Right = Move to left right tabs\n- Ctrl + Left/Right = bacward/forward page\n- Ctrl + Shift + T = Open previously closed tab\n- Ctrl + M = minimize window\n- Ctrl + W = close tab\n- Ctrl + N = new window \n- Ctrl + R = reload window\n\n**Coding**\n- Ctrl + Left/Right = move to beginning/end of line","n":0.12}}},{"i":160,"$":{"0":{"v":"SQL","n":1},"1":{"v":"\nMost notes here are from [Stanford lectures](https://www.youtube.com/watch?v=Yceqbp_DKbA&list=PL9ysvtVnryGpnIj9rcIqNDxakUn6v72Hm&index=103) and leetcode.\n# Buzzwords\nrelational algebra, relational data model, relational database, DBMS (database management system), PostgreSQL, MySQL, declarative, power of set theory, projection, persistent data (outlives program), schema (type) vs data(variables), DDL (data definition language) used to set up schema, DML (data manipulation language = query language), database = set of named relations, relation = table, columns = attributes of relation, rows = tuple, schema = structural description of relations in database, ACID properties, select = project operator, cross product, join, inner, outer, left, right, union, join = cross product + where, CTE = common table expression, subquery, normalization, denormalization, sharding, shards, transactions\n\n- SQL is declarative - you say \"what\" you want but not how you want it\n- SQL is not \"Turing complete\" = computationally universal (can do all stuff)\n- basic SQL cannot run unbounded computations (no for loops)\n\n# Code snippets\n\n- join\n```sql\nselect *\nfrom table1 t1\nleft join table2 t2\non t1.id = t2.id\n\nselect *\nfrom t1\nleft join t2 using(product_id)\n```\n\n- no full outer join in MySQL. Emulate it by:\n```sql\nSELECT * FROM t1\nLEFT JOIN t2 ON t1.id = t2.id\nUNION\nSELECT * FROM t1\nRIGHT JOIN t2 ON t1.id = t2.id\n```\n- join on operations [leetcode](https://leetcode.com/problems/leetflex-banned-accounts/)\n```\nsometimes is smart to do joins on operations/conditions\njoin on t1.id = t2.id-1\n```\n- **join is just a cross product with where statements**\n\n- conditional statement case when else end\n```sql\nselect\n(case\n    when condition1 then result1\n    when condition2 then result2\n    when conditionN then resultN\n    else result # else can be dropped too\nend) as field\nfrom table\n```\n\n- if conditional statement\n```sql\nselect if(condition, true_val, false_val) as field\nfrom table\n```\n\n- least, greatest to take min,max of a row [leetcode](https://leetcode.com/problems/number-of-calls-between-two-persons/)\n```sql\nselect least(from_id,to_id) as person1, greatest(from_id,to_id) as person2,\n        count(*) as call_count, sum(duration) as total_duration\nfrom calls\ngroup by person1,person2\n```\n\n- partition by [leetcode](https://leetcode.com/problems/find-the-quiet-students-in-all-exams/discuss/1414605/Become-the-Master-of-Partition-By)\n\n- window question [leetcode](https://leetcode.com/problems/biggest-window-between-visits/)\n\n- date operations\n```sql\nselect \n        datediff(date1,date2) as diff, -- measred in days\n        period_diff('202210','202301') as diff, -- measured in months\n        month(date1) as month,\n        date_add('2018-05-01',INTERVAL 1 DAY) as next_day,\n        dayofweek('2007-02-03') as day_of_week # 1 to 7 (1 and 7 are weekend)\n        dayofyear('2007-02-03') as day_of_week,\n        to_days('2008-01-01') as date_number,\n        date_format(date,'%d/%m/%Y') as formatted_date,\n        least('2018-12-31',date),\n        greatest(d1,d2)\n```\n- date_part, date operations in postgresql - give PART of the DATE\n- [leetcode](https://leetcode.com/problems/tasks-count-in-the-weekend/) you can test your functions here\n```sql\nselect \nDATE_PART('field', column_name) -- need to use these quotation marks ''\nSELECT DATE_PART('year', TIMESTAMP '2024-07-06 15:23:10');    -- 2024\nSELECT DATE_PART('day', DATE '2024-07-06');                   -- 6\nSELECT DATE_PART('month', now());                             -- current month as number\nSELECT DATE_PART('dow', DATE '2024-07-06');                   -- 6 (Saturday, 0 = Sunday)\nfrom t\n```\n- `field` can be 'year', 'month', 'day', 'dow', 'hour', 'minute', 'second'\n- 'dow' gives a number where 0 is Sunday and 6 is Saturday\n\n\n- date_trunc truncates the date column to the specified precision (beginning of month, year and so on)\n```sql\nSELECT DATE_TRUNC('month', TIMESTAMP '2024-07-06 15:23:10');   -- '2024-07-01 00:00:00'\nSELECT DATE_TRUNC('day', TIMESTAMP '2024-07-06 15:23:10');     -- '2024-07-06 00:00:00'\nSELECT DATE_TRUNC('year', now());                              -- start of this year\n```\n\n- TIMESTAMP, DATE, INTERVAL . they all convert string to date-like object\n\n- INTERVAL represents a unit of time that can be added or subtracted from dates\n```sql\nINTERVAL 'N unit' -- unit can be 'day', 'days', 'month', 'week', 'hour' etc\n```\n```sql\nSELECT TIMESTAMP '2025-05-01 15:45:01' + INTERVAL '10 minutes',\nTIMESTAMP '2025-05-01 15:45:01' - INTERVAL '10 minute' as a,\nTIMESTAMP '2025-05-01 15:45:01' + INTERVAL '1 week' as b\n```\n\n- casting in postgreSQL\n1.  Use ::typename or CAST(value AS typename)\n\n2. If in doubt, cast explicitly.\n```sql\nSELECT '2023-08-16'::date;                 -- 2023-08-16\nSELECT '2023-08-16 15:20'::timestamp;      -- 2023-08-16 15:20:00\nSELECT '123'::integer;       -- 123 (text to int)\nSELECT 123::text;            -- '123' (int to text)\n```\n\n- width_bucket\n```sql\nselect width_bucket(column_name, min_value, max_value, number_of_buckets) -- max_value is the upper bound of the last bucket non-exclusive [)\nselect width_bucket(score, 1, 101, 5) -- 5 buckets [1,20], [21- 40] ... \n```\n\n- subquery in select statement example [leetcode](https://leetcode.com/problems/percentage-of-users-attended-a-contest/)\n```sql\nselect contest_id, round(count(*)/(select count(*) from users)*100,2) as percentage\nfrom register\ngroup by contest_id\norder by percentage desc, contest_id asc\n```\n- where clauses\n```sql\nwhere field in ('string', 'string1')\nwhere a = 6 or a = 9\nwhere low < a and a < high\n```\n\n- not in operator with null values CAREFUL [stackoverflow](https://stackoverflow.com/questions/129077/null-values-inside-not-in-clause) [leetcode](https://leetcode.com/problems/tree-node/submissions/)\n[584](https://leetcode.com/problems/find-customer-referee/)\n```\nMySQL uses three-valued logic -- TRUE, FALSE and UNKNOWN. Anything compared to NULL evaluates to the third value: UNKNOWN. That “anything” includes NULL itself! That’s why MySQL provides the IS NULL and IS NOT NULL operators to specifically check for NULL.\n```\n```sql\nselect name\nfrom customer\nwhere referee_id != 2 or referee_id is null\n```\n\n- filling in null values - use the IS operator\n\n```sql\ncase\nwhen field is null then 0 # field = 0 would not work\nelse field end\n```\n\n```sql\nIFNULL(field, 0) AS field\n```\n\n- delete\n```sql\nDELETE FROM table_name WHERE condition;\n```\n- delete [problem](https://leetcode.com/problems/delete-duplicate-emails/), cannot delete from referenced table and need to create a temporary copy\n\n```sql\n# delete from person\n# where id not in (select min(id) from person group by email) \n\n# create temporary copy of table\ndelete from person\nwhere id not in (select t.id from (select min(id) as id from person group by email) t)\n\n```\n\n- substring(col_name, start_index, length), length is optional\n```sql\n SELECT SUBSTRING('SQL Tutorial', 1, 3) AS ExtractString;\n\n```\n\n- union, union all\n```sql\nt1\nunion\nt2 # duplicates are removed\n\nt1\nunion all\nt2 # duplicates are stacked\n```\n\n- between\n```sql\nSELECT column_name(s)\nFROM table_name\nWHERE column_name BETWEEN value1 AND value2;\n```\n- [2388](https://leetcode.com/problems/change-null-values-in-a-table-to-the-previous-value/) Change Null Values in a Table to the Previous Value\n```sql\nwith cte as (select *, row_number() over() as row_num, if(isnull(drink),0,1) as flag from coffeeshop),\n     cte2 as (select *, sum(flag) over(order by row_num) as group_id from cte)\nselect id, first_value(drink) over(partition by group_id) as drink\nfrom cte2\n```\n\n- concatanete two strings\n```sql\nselect concat(s1,s2) as combined \nfrom t\n```\n- lower case, upper case\n```sql\nselect lower(s), upper(s)\nfrom t\n```\n\n- functions in SQL [leetcode](https://leetcode.com/problems/nth-highest-salary/)\n```sql\nCREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT\nBEGIN\n  RETURN (\n      # Write your MySQL query statement below.\n\n  );\nEND\n```\n\n-limit + offset [leetcode](https://leetcode.com/problems/nth-highest-salary/)\n```sql\nSELECT\n    column_list\nFROM\n    table1\nORDER BY column_list\nLIMIT row_count OFFSET offset;\n\n# The LIMIT row_count determines the number of rows (row_count) returned by the query.\n# The OFFSET offset clause skips the offset rows before beginning to return the rows.\n# OFFSET is optional\n```\n\n- rank in over(partition by)\n```sql\nRANK ( ) OVER ( [ partition_by_clause ] order_by_clause )  \n\nProductID   Name                   LocationID   Quantity Rank  \n----------- ---------------------- ------------ -------- ----  \n494         Paint - Silver         3            49       1  \n495         Paint - Blue           3            49       1  \n493         Paint - Red            3            41       3  \n496         Paint - Yellow         3            30       4\n```\n- dense rank [leetcode](https://leetcode.com/problems/department-top-three-salaries/)\n```sql\nDENSE_RANK ( ) OVER ( [ <partition_by_clause> ] < order_by_clause > )  \nProductID   Name                               LocationID Quantity Rank  \n----------- ---------------------------------- ---------- -------- -----  \n494         Paint - Silver                     3          49       1  \n495         Paint - Blue                       3          49       1  \n493         Paint - Red                        3          41       2  \n496         Paint - Yellow                     3          30       3  \n```\n\n- cumulative total\n```sql\nSELECT Id, StudentName,  StudentAge,\nSUM (StudentAge) OVER (ORDER BY Id) AS RunningAgeTotal\nFROM Students\n```\n\n- rolling sum/average\n```sql\nselect visited_on, \n            sum(amount) over(order by visited_on rows between 6 preceding and current row) as amount,\n            avg(amount) over(order by visited_on rows between 6 preceding and current row) as average_amount\nfrom customer\n```\n\n- window functions\n    - sum, avg, first, lead, lag, rank, dense_rank\n\n- lag window function\n\n```sql\nselect *, lag(purchase_date) over(partition by user_id order by purchase_date) as previous_date\nfrom purchases\n```\n\n- lead, window function for value in the next row\n```sql\nSELECT dept_id, last_name, salary,\nLEAD (salary,1) OVER (ORDER BY salary) AS next_highest_salary\nFROM employees;\n```\n\n- like, search in substring, [patients](https://leetcode.com/problems/patients-with-a-condition/)\n```sql\nselect col\nfrom table\nwhere col like '%KUR%'\n```\n\n- to get island values (group of equal consecutive values in a column) use rank- rank trick, [leetcode](https://leetcode.com/problems/longest-winning-streak/)\n\n- CAST to datatypes\n```sql\n-- For Sql server\nSELECT CAST(25.65 AS int);\n/*\nCAST(<value_to_cast> AS <data_type_to_cast_to>)\n*/\n```\n\n- put stuff in one cell table with group_concat\n```\n+------------+------------+\n| sell_date  | product     |\n+------------+------------+\n| 2020-05-30 | Headphone  |\n| 2020-06-01 | Pencil     |\n| 2020-06-02 | Mask       |\n| 2020-05-30 | Basketball |\n| 2020-06-01 | Bible      |\n+------------+------------+\n\nTO\n\n+------------+----------+------------------------------+\n| sell_date  | num_sold | products                     |\n+------------+----------+------------------------------+\n| 2020-05-30 | 3        | Basketball,Headphone,T-shirt |\n| 2020-06-01 | 2        | Bible,Pencil                 |\n| 2020-06-02 | 1        | Mask                         |\n+------------+----------+------------------------------+\n```\n\n```sql\n# Write your MySQL query statement below\nselect sell_date, count(distinct product) as num_sold, \ngroup_concat(distinct product order by product separator ',') as products\nfrom activities\ngroup by sell_date\n```\n\n- pivot trick [student by geography](https://leetcode.com/problems/students-report-by-geography/)\n\n```sql\n# Write your MySQL query statement below\nselect max(case when continent = 'America' then name end) as America,\n       max(case when continent = 'Asia' then name end) as Asia,\n       max(case when continent = 'Europe' then name end) as Europe\nfrom\n(select *, row_number() over(partition by continent order by name) as rn\nfrom student) t\ngroup by rn\n```\n\n- [dynamic pivoting](https://leetcode.com/problems/dynamic-pivoting-of-a-table/)\n```sql\n-- static pivot\n-- select product_id,\n-- sum(case when store = \"store1\" then price end) as store1,\n-- sum(case when store = \"store\"then price end) as store2,\n-- ....\n-- from products\n-- group by product_id\n\n\n-- this procedusre would concatenate all select statements in case_stmtsum(..)\nCREATE PROCEDURE PivotProducts()\n\nBEGIN\n\t# Write your MySQL query statement below.\n    SET SESSION group_concat_max_len = 1000000;\n    --define case statement\n    select group_concat(distinct concat('sum(case when store = \"',store,'\" then price end) as ', store))\n    into @case_stmt\n    from products;\n    -- get whole sql query\n    set @sql_query = concat('select product_id, ',@case_stmt, ' from products group by product_id');\n    \n    prepare final_sql_query from @sql_query;\n    execute final_sql_query;\n    deallocate prepare final_sql_query;\nEND\n```\n\n- [dynamic unpivoting](https://leetcode.com/problems/dynamic-unpivoting-of-a-table/)\n\n\n- [delimit identifiers](https://stackoverflow.com/questions/9917196/meaning-of-square-brackets-in-ms-sql-table-designer), ignore special symbols, characters, spaces etc\n```sql\nselect *\nfrom [my table]\nwhere [order] = 10\n```\n\n- index column\n```sql\nCREATE INDEX [index name] ON [table name] ( [column name] )\n```\n-transactions\n```sql\nBEGIN TRANSACTION\nBEGIN TRY\n\nUPDATE table_name SET BAKIYE = BAKIYE - 1000\nWHERE Name='Name' AND Surname='Surname'\n\nUPDATE table_name SET BAKIYE = BAKIYE + 1000\nWHERE Name='Name' AND Surname='Surname'\n\nupdate salary set sex = if(sex ='f','m','f') \n--  swap f to m and m to f in sex column\nCOMMIT\nEND TRY\nBEGIN CATCH\nROLLBACK\nEND CATCH\n```\n## SQL Variables\n\nIf we want to use a variable in SQL Server, we have to declare it using `DECLARE` statement.\n\nLocal variable names have to start with an at (@) sign.\n\n- declaring variables\n\n```sql\nDECLARE @TestVariable AS VARCHAR(100)\nDECLARE @TestVariable -- NULL initialization\n```\n- assign values to variables\n```sql\nSELECT @TestVariable = 'Save the Nature' --single select line\n\n--- ASSIGN LAST VALUE OF TABLE T\nselect @TestVariable = person_name\nfrom t\nwhere salary = 100000\n```\n\n- assign variable value using set\n\n```sql\nDECLARE @Variable1 AS VARCHAR(100)\nDECLARE @Variable2 AS UNIQUEIDENTIFIER\nSET @Variable1 = 'Save Water Save Life'\nSET @Variable2= '6D8446DE-68DA-4169-A2C5-4C0995C00CC1'\n```\n\n- assign many varaibles to values\n```sql\nDECLARE @Variable1 AS VARCHAR(100), @Variable2 AS UNIQUEIDENTIFIER\nSELECT @Variable1 = 'Save Water Save Life' , @Variable2= '6D8446DE-68DA-4169-A2C5-4C0995C00CC1'\n```\n\n\n- end of batch, `GO` keyword\n```sql \nDECLARE @TestVariable AS VARCHAR(100)\nSET @TestVariable = 'Think Green'\nGO\nPRINT @TestVariable -- ERROR, varibale not declared\n```\n\n- PREPARE, EXECUTE and DEALLOCATE statement in MySQL\n- running strings as statements\n\n```sql\nSET @s = 'SELECT ? + ? AS sumtable';\nPREPARE stmt1 FROM @s;\nSET @a = 4;\nSET @b = 6;\nEXECUTE stmt1 USING @a, @b;\nDEALLOCATE PREPARE stmt1;\n```\nthis would return\n```\n+----------+\n| sumtable |\n+----------+\n|       10 |\n+----------+\n```\n\n## SQL CTE\n\nCTE = Common Table Expression. These are temporary tables which act as variable that you can use through your code.\n[Example problem](https://leetcode.com/problems/change-null-values-in-a-table-to-the-previous-value/submissions/)\n\n```sql\nwith cte as (select *,rank() over(order by money) as r from t),\n     cte2 as (select *,min(r) as minn from cte2),\n     cte3 as (select student from cte2 where r = minn)\nselect *\nfrom cte3\n```\n\nAvoid running lots of subqueries, joining, carrying tables around!\n\n\n## SQL recursion\nBasic SQL cannot express unbounded computations.\n\nTable: ParentOf\n```\nParent |  Child\nSue    |  Mary\nBob    |  Mary\nFred   |  Bob\nJane   |  Bob\n```\nTasks: Find Parent, Grandparent (two instances of ParentOf), Grand-grand parent(three instances of ParentOf)\nNote you need recursion\n\n`With` construct exists in SQL by itself. It often used to have recursion in SQL.\n\n![with_statement.png](assets/images/with_statement.png){width: 600px}\n\n\n\neach relation R is the result of its corresponding query. Final result of the master LAST query. Can thing of `AS` to be an\nassignment for the realtions\n\nWe can specify recursive statements using the `Recursive` keyword after `With`\n```sql\nWITH RECURSIVE cte_name AS (\n    initial_query  -- anchor member\n    UNION ALL\n    recursive_query -- recursive member that references to the CTE name\n)\nSELECT * FROM cte_name;\n```\n\n```sql\nWITH RECURSIVE cte_count\nAS (\n      SELECT 1 as n-- BASE CASE\n      UNION ALL # might enter infinite loop, careful\n      SELECT n + 1\n      FROM cte_count -- Recursive call, should dependt on the relation itself CTE_COUNT!\n      WHERE n < 3\n    )\nSELECT n\nFROM cte_count;\n```\n![recursion_example.png](assets/images/recursion_example.png){width: 600px}\n\n\n- generate consecutive numbers\n```sql\nWITH RECURSIVE seq AS (\n    SELECT 0 AS value UNION ALL SELECT value + 1 FROM seq WHERE value < 100\n    )\n\nSELECT * FROM seq;\n```\n\n\n- with statement [leetcode](https://leetcode.com/problems/the-number-of-seniors-and-juniors-to-join-the-company/)\nBasic SQL cannot express unbounded computations. WITH construct is available in SQL without recursion. But this is the construct used to ADD recursion in SQL. CTE = common table expression. WITH is like a function in SQL\n\n```sql\nwith R1 as (query 1),\n     R2 as (query 2),\n     ...\n     CTE as (query),\n     \n <query involving all R1, R2 ..Rn, CTE + other tables> # return this last query\n```\n\n- recursion\n```sql\nWITH RECURSIVE cte_name AS (\n    cte_query_definition (the anchor member)\n\n    UNION ALL\n\n    cte_query_definition (the recursive member) # often need where statement to stop\n)\n\n \nSELECT *\nFROM   cte_name;\n```\n![recursion_query.png](assets/images/recursion_query.png)\n\n\n![recursion.png](assets/images/recursion.png)\n\n- classic family tree recursion [problem](https://leetcode.com/problems/all-people-report-to-the-given-manager/)\n\n\n## SQL indexing\n[Stanford Lecture](https://www.youtube.com/watch?v=Y7Qlc7f_u0o)\n\n\nIndex column B and you can ask questions such as show me the rows where elements in column B belong to certain interval\nis going to be faster.\n\n![indexing.png](assets/images/indexing.png){width: 600px}\n\n\nUsers don't access indexes! They are used underneath by the query execution engine.\n\n\nUnderlying data structures = type of indexes:\n- hash allows only concrete checks such as table.A = val **O(1)**\n- B+ trees allows checks such as table.A = val, v1 < table.A <= v2 **O(log(n))**\n- R+ trees\n\nNo indexes is **O(n)** time\nMany DBMS's build indexes automatically on `PRIMARY KEY` (and sometimes `UNIQUE`) attributes.\n\nExamples:\n```sql\nselect student_id\nfrom student\nwhere student_name = 'Kuny' and gpa > 3.9\n```\n- index on student_name (hash-based index)\n- index on gpa (tree-based)\n- index on (student_name,gpa)\n\n```sql\nselect s_name, c_name\nfrom student, apply\nwhere student.s_id = apply.s_id\n```\n- no indexing is `O(m*n)`\n- index student.s_id -> `O(n)`\n- index apply.s_id -> `O(m)`\n\nDownsides of indexes:\n- extra space (persistent data structure stored underneath the database) = marginal downside (ok-ish)\n- index created = medium downside/initialization can be slow\n- index maintenance, can offset benefits. If we modify often the const of maintaining the indexes (re-initialization)\ncan be quite expensive. If you do lots of writes and less reads better not use indexes.\n\n```sql\ncreate index your_index_name on your_table_name(your_column_name) using HASH;\nor\ncreate index your_index_name on your_table_name(your_column_name) using BTREE;\n```\n\nThe best way to improve the performance of SELECT operations is to create indexes on one or more of the columns that are tested in the query. The index entries act like pointers to the table rows, allowing the query to quickly determine which rows match a condition in the WHERE clause, and retrieve the other column values for those rows. All MySQL data types can be indexed.\nIndexes are used to find rows with specific column values quickly. Without an index, MySQL must begin with the first row\nand then read through the entire table to find the relevant rows.\n\n- index columns used in `WHERE` clauses\n\n\n**Column Indexes**\nThe most common type of index involves a single column, storing copies of the values from that column in a data structure, allowing fast lookups for the rows with the corresponding column values. The B-tree data structure lets the index quickly find a specific value, a set of values, or a range of values, corresponding to operators such as` =, >, ≤, BETWEEN, IN,` and so on, in a `WHERE` clause.\n\n## Database design\n\n1. Understand the Data – Is it relational or document-based? Do you need to manage complex queries or just store simple key-value pairs?\n\n2. Scalability Is Key – High traffic? Complex joins?\n\n3. Consistency vs. Performance – Do you need strong consistency (ACID) or can you tolerate eventual consistency for better availability and partition tolerance? \n\n4. Account for Future Evolution  – Can your database scale? Is it flexible enough for new features?\n\n5. Keep It Simple\n\n\n\n[Design by decomposition](https://www.youtube.com/watch?v=DFnAakJ4FDg&list=PL9ysvtVnryGpnIj9rcIqNDxakUn6v72Hm&index=31)\n- start with 'mega' relations containing everything\n- decompose into smaller, better relations with same info\n- can do decomosition automatically\n**Final set of relations satisfies normal form** (normalization of a database) = no redundant data, updates are easier to be made.\n\nDenormalised database example:\n```sql\nApply(SSN, student_name, college_name, high_school, city, hobby)\n```\n*Apply* is a table with all columns (ssn = social security number) stored in one place.\nTo record Ann with SSN = 123 from Sofia, studied at SMG and 125, plays tennis and trumpet and applied at Stanford, Berkeley, MIT\nwe need 12 different rows! Studied at SMG will be repeated 6 times, applied at MIT will be repeated 4 times.\n**Redundancy** in the database.\n\nNormalised database:\n```\nStudent(SSN, student_name)\nApply(SSN, college_name)\nHighSchool(SSN, high_school)\nLocated(high_school, city)\nHobbies(SSN, hobby)\n```\nDecide how to normalize your database by looking for:\n- functional dependencies\n- multivalued dependencies\n\n*Buzz word* If you see Functional Dependency  use Boyce-Codd Normal Form (decompose your table)\n\n![functional_dep.png](assets/images/functional_dep.png){width: 600px}\n\n*Buzz word* If you see multivalued Dependency  use Boyce-Codd Normal Form (decompose your table)\n\n![multivalued_dep.png](assets/images/multivalued_dep.png){width: 600px}\n\n\nMaybe check this [youtube](https://www.youtube.com/watch?v=GFQaEYEc8_8&ab_channel=Decomplexify)\n\n\n## ORM\n\nSQL Alchemy\n\nORM\n\nRelational Map, e.g. from SQL to Python\n\nleaky/soft abstraction\n\n\n\n# Problems\n[1285](https://leetcode.com/problems/find-the-start-and-end-number-of-continuous-ranges/)\n[177](https://leetcode.com/problems/nth-highest-salary/)\n[571](https://leetcode.com/problems/find-median-given-frequency-of-numbers/submissions/)\n[1709](https://leetcode.com/problems/biggest-window-between-visits/)\n[1270](https://leetcode.com/problems/all-people-report-to-the-given-manager/)\n[1613](https://leetcode.com/problems/find-the-missing-ids/)\n[569](https://leetcode.com/problems/median-employee-salary/)\n[608](https://leetcode.com/problems/tree-node/submissions/)\n[2308](https://leetcode.com/problems/arrange-table-by-gender/)\n[2228](https://leetcode.com/problems/users-with-two-purchases-within-seven-days/)\n[1285](https://leetcode.com/problems/find-the-start-and-end-number-of-continuous-ranges/submissions/)\n[1369](https://leetcode.com/problems/get-the-second-most-recent-activity/)\n\n# Topics\n\nSpace and Time Complexity intro to Queries: https://medium.com/learning-data/understanding-algorithmic-time-efficiency-in-sql-queries-616176a85d02  \nBasic (important!) Query Optimisation Techniques: https://medium.com/learning-sql/12-tips-for-optimizing-sql-queries-for-faster-performance-8c6c092d7af1  \nAdvanced Topics in SQL: https://sqlpad.io/tutorial/advanced-sql/  \n+ UNION, GROU[ BY (UNDERSTAND HOW IT WORKS), INDEXATION, PARTITION   \nB-TREE: https://builtin.com/data-science/b-tree-index (fundamental Database data structure! - connection to indexing!)  \n","n":0.02}}},{"i":161,"$":{"0":{"v":"Organising Python Packages","n":0.577},"1":{"v":"[blog](http://blog.habnab.it/blog/2013/07/21/python-packages-and-you/)\n\n```\nproject_name/__init__.py\nproject_name/__main__.py\nproject_name/application.py\nproject_name/test/__init__.py\nproject_name/test/test_application.py\nproject_name/test/util.py\nproject_name/util.py\n```\n\n**Use absolute imports only (implcit relative imports in python 3 are NOT recommended)**\n\nIn all your files in the project:\n```python\nfrom project_name.util import *\n```\n\n**Don’t modify sys.path from code in your package**\n\n**Don’t make your project root a package**\n\n**Don’t set PYTHONPATH to try to make it go**\n\n\n# Creating packages\n\n- use the src layout\n```\nbulquant/\n  src/\n    bulquant/\n      ...code...\n  tests/\n  examples/\n  docs/\n  setup.py\n```\n\n- flat layout\n```\nbulquant/\n  bulquant/      # Module/package code here\n  tests/\n  examples/\n  docs/\n```\n\n1. Prevents Import Bugs in Development With a flat layout, if you run pytest or scripts from the project root, Python can \"find\" your package accidentally because it's in the same dir as your test file, not because it's correctly installed via pip.\n\nFrom the top-level (bulquant/), run:\n`pip install -e src`\n\nthen you can import bulquant","n":0.092}}},{"i":162,"$":{"0":{"v":"MIT","n":1}}},{"i":163,"$":{"0":{"v":"SCIP","n":1},"1":{"v":"Book in [[books.SCIP]]","n":0.577}}},{"i":164,"$":{"0":{"v":"Programming for the Puzzled","n":0.5},"1":{"v":"# MIT 6.S095 - Programming for the Puzzled\n\nLecture [videos](https://www.youtube.com/watch?v=14UlXIZzwE4&list=PLUl4u3cNGP62QumaaZtCCjkID-NgqrleA&index=1&ab_channel=MITOpenCourseWare)\n\nLecture [notes](https://ocw.mit.edu/courses/6-s095-programming-for-the-puzzled-january-iap-2018/)\n\n# Puzzle 1. You Will All Conform. Trick\n\nArray of people and they have caps on their head. These caps can be forward or backward.\n\n0, 1, 2, 3, 4, 5, 6\n\nF, B, F, F, B, B, F\n\nWe want all caps to conform, that is all are forward or all are backward. Possible operations: You can say to a person \"\nFlip your cap\". Or you can say \"People from position from 2 to 3 flip your caps\". Find minimum number of operations given\nan array of people with caps.\n\n---\n\nSimple solution is just to count forward and backward intervals\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\narr = ['F','B','F','F','B','F','F']\n\ndef solve(arr):\n    forward, backward = [],[]\n    i = 0\n    while i < len(arr):\n        j = i+1\n        while j < len(arr) and arr[j] == arr[i]:\n            j += 1\n        if arr[i] == 'F': forward.append([i,j])\n        else: backward.append([i,j])\n    return min(len(forward),len(backward))\n```\n</details>\n---\nFirst person in line gives up what set of commands you will say to the people. For example if arr[0] = 'F', then you would say to all 'B-s' to flip their caps.\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n\ndef please_all_conform(arr):\n    arr.append('END')\n    for i in range(1,len(arr)):\n        if arr[i] != arr[i-1]:\n            if caps[i] != caps[0]:\n                print(f'Please people in positions {i}', end = '')\n            else:\n                print(f' through {i - 1} flip your caps!')\n\n```\n</details>\n\n'Nuances with how you solve a problem and how you code it.'\n\n# Puzzle 2. The Best Time to Party. Line Sweep\n\nCelebrities come at a party at particular times (intervals). When should you go to the party to max out the number of celebrities you will meet.\n\n```\n[('Beyonce',6,7), ('Taylor',7,9), ...]\n```\nClosed intervals on the left and open on the right.\n\nSimple solution. Check all hours (starts and ends) and check how many intervals contain it. Depends on granularity of time\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef solve(intervals):\n    start = min([intervals[i][0] for i in range(len(intervals))])\n    end = max([intervals[i][1] for i in range(len(intervals))])\n    times = set(starts + ends)\n    res = 0\n    for t in range(start,end+1):\n        res = max(res,count(t),intervals)\n    return res\n\ndef count(t):\n    return sum([ s <= t < e for s,e  in intervals])\n\n```\n</details>\n\n---\n'Lots of repeated computation = redundancy. Often you could reduce this redundancy by computing things incrementally.'\n\nThe only times that are interesting is when celebrities come and leave\n\nThe only time the result could change is if a celebrity enters or leaves.\n\nLine sweep solution.  Relies on the fact that the result would change only when you reach an event.\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef solve(intervals):\n    events = []\n    for s,e in events:\n        events.append((s,1))\n        events.append((e,0))\n    events.sort()\n    res,curr = 0,0\n    for time,is_start in events:\n        if is_start:\n            curr += 1\n        else:\n            curr -= 1\n        res = max(res,curr)\n    return res\n# trick\ndef solve(intervals):\n    events = []\n    for s,e in events:\n        events.append((s,1))\n        events.append((e,-1))\n    events.sort()\n    return max(accumulate([p for _p in events]))\n```\n</details>\n\n[problem](https://leetcode.com/problems/meeting-rooms-ii/)\n\n# Puzzle 3: You Can Read Minds. Encoding Information\n\nGiven a deck of 52 cards. William takes 5 cards at random and shows to Devas 4 of them. Devas uses an algorithm to determine the 5th card. He uses the info from the 4 cards he sees to guess the fifth card.\n\n\nShow 4 cards, 5th card could be 1 of 48.\n\nHow many bits of information can 4 cards show? Use the order of the 4 cards gets you $4! = 24$. We have just 24 bits of information but have to encode 48. Ordering just uses the numbers on the cards, we need to use the suits.\n\n**Strategy:**\nFirst card will give away the suit. By pigeonhole principle out of 5 cards there would be 2 with the same suits. The other 3 cards would give away the distance from the first card thinking in circular/modular computation.\n\n**Example:** We pick King Diamond, A Diamond, 2 Spade, 3 Clubs, 5 Hearts.\n\nWilliam will hide the Ace and present  K, 2, 3, 5.\n\nDevadas sees King of Diamond, hence the  hidden card is a diamond. 2,3,5 is the first permutation, hence the hidden card is 1 distance away from the Kind, that is and Ace.\n\nWhy this strategy works? The first card is the pivot card giving away the suit, hence there are 12 cards remaining that could possibly be the hidden card. we have 3 cards left t encode the 12 cards. Because we measure distance only in clockwise direction we are covered.\n\n\nIs it possible to pick 4 random cards, hide one of it and encode it using the rest 3.\n\n[Article](https://codegolf.stackexchange.com/questions/165390/professor-at-mit-can-read-minds)\n\n\n# Puzzle 4: Please Do Break the Crystal. Strategy + Flip problem statement\n\n**Problem statement:**\n\nYou are given two identical crystal balls and you have access to a building with n floors labeled\nfrom 1 to 128.\n\nYou know that there exists a floor f where $0 <= f <= 128$ such that any egg dropped at a floor higher\nthan f will break, and any egg dropped at or below floor f will not break.\n\nIn each move, you may take an unbroken egg and drop it from any floor x (where $1 <= x <= 128$). If\nthe egg breaks, you can no longer use it. However, if the egg does not break, you may reuse it in\nfuture moves.\n\nReturn the minimum number of moves that you need to determine with certainty what the value of f is.\n\nLinks to problem: [medium](https://leetcode.com/problems/egg-drop-with-2-eggs-and-n-floors/) with two balls only.\n\n[hard](https://leetcode.com/problems/super-egg-drop/)\nwith arbitrary number of balls.\n\n\n## Thinking\n\nBinary search strategy - begin with first drop at 64. Worse case is if it breaks and then we have to do \\[1,63\\] = 63 drops more: total 64 drops.\n\nImprovement: Use smaller interval - drop from 16, 32, 48, 64...\nWorse case if it breaks at 128 and we have to do 128/16 + 15 = 23 total drops\n\nEven better: be around square root.\nStrategy start dropping first ball at floors:  $k, 2k, 3k, .... n//k * k$\nWorse case is if the first drop does not break until the end.\nTotal drops = $n/k + k-1$. Goal is to minimize this function.\n\nTake derivative and set to 0 -> $k = sqrt(n)$\nTotal worse number of drops is $2sqrt(n)-1$\n\nSo using this strategy of throwing on equal interval range $k, 2k ,...$\nbest is to choose k = sqrt(n) and total worse case is $2sqrt(n)-1$\n\nHowever even more optimal strategy is to have decreasing intervals.\nThey should get smaller as the problem gets easier.\n\n**Math Solution:**\nSuppose least number of worse case throws is $x$.\nThen the highest floor from which you can drop first egg is from floor $x$. If you drop it from floor\n$x+1$ and it breaks then you'd have $x+1$ throws in total\n\nSecond drop can be highest from $x + x-1$. If you drop it from x+x, then your worse case would be $>=$\nif the egg breaks from second drop + try drops from x+1..2x-1 -> total: $2 + x-1 = x+1$\n\nSimilarly kth drop should be from drop $x + (x-1) + (x-2) .. + (x-k+1)$.\n\nTotal number of drops would be: x + (x-1) + (x-2) + ... + 1 >= n\nx(x+1)/2 >= n\n\nNB: MIT solution does not give the most optimal one. It assumes equal intervals for 2 balls and for the general case.\n\n**Another Math Solution:**\n\nReverse engineer problem\nGiven T moves and 2 balls find how many floors I can cover\nFind smallest T with which we can cover n floors\n\nFor k balls:\n\n```math\nf(T,k) = 1 + f(T-1,k-1) + f(T-1,k)\nf(T,2) = 1 + f(T-1,1) + f(T-1,2)\nf(T,2) = 1 + T-1 + f(T-1,2)\nf(T,2) = T + f(T-1,2)\nf(T,2) = T + T-1 + T-2 + ... 1\nf(T,2) = T*(T+1)/2\n```\nThe above solution gives you O(nk) coding solution to the problem.\n\nRemark\n\nMoreover, it can be shown that $f(T,k)=C^{1}_{T}+⋯+C^{k}_{T}$ and using this information, problem can be solved\nin $O(k log n)$, using binary search.\n\n# Problem 5 & 6: Keep those queens apart. Backtracking (Recursion)\n\nThese two lectures consider the 8-queens problem. Lecture 5 gives overview and iterative solution of the\nproblem. Lecture 6 gives recursive/backtracking solution.\n\nExample. Find greatest common divisor.\n\nEuclid's algorithm.\n\n```Python\ndef iterative_gcd(m, n):\n    while n > 0:\n        m, n = n, m % n\n    return m\n\ndef recursive_gcd(m, n):\n    if m % n == 0: return n\n    return recursive_gcd(n, m%n)\n```\n\n\n\n*'You always get intuition about the problem by shrinking it and looking at small cases.'*\n\n\nGiven $n$x$n$ board return the number of different ways we put $n$ queens on the board. [link](https://leetcode.com/problems/n-queens-ii/)\n\n**Solution**\nRepresent a board using just an array where $j$-th element shows on which row we have a queen in column $j$. Exploit the fact there can be at most one queen in a single column.\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Solution:\n    def totalNQueens(self, n: int) -> int:\n        def is_safe(i,j,board):\n            if i in board: return False\n            for col in range(j):\n                if abs(col-j)==abs(i-board[col]): return False\n            return True\n        def backtrack(j):\n            nonlocal res\n            if j == n:\n                res += 1\n                return\n            for i in range(n):\n                if is_safe(i,j,board): # pruning\n                    board[j] = i\n                    backtrack(j+1)\n                    board[j] = None\n        res,board = 0,[None]*n\n        backtrack(0)\n        return res\n```\n</details>\n\nThere is no closed form solution depending on $n$.\n\n# Problem 7: Tile that Courtyard, Please. Divide and Conquer (Recursion).\n\n**Problem.** Consider a $2^n$x$2^n$ courtyard and $L$-shaped tiles (three square-tiles). Can you fill it in with $L$-tiles perfectly? No, total number of squares is $2^{2n}$ and is not divisible by 3. Then we change the problem and put a ban on one of the squares in the courtyard. Total number of squares is $2^{2n}-1$ which is divisible by 3.\n\nDivide and Conquer **Solution:**\n\n![dp_weighted_intervals.png](assets/images/tile_courtyard.png)\n\nThere is always a perfect tiling of the courtyard!\n\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n#Programming for the Puzzled -- Srini Devadas\n#Tile that Courtyard, Please\n#Given n in a 2^n x 2^n checkyard with a missing square at position (r, c), \n#find tiling of yard with trominoes (L-shaped dominoes).\n#This version works directly on the given grid, and does NOT make copies\n#of the grid for recursive calls.\n\nEMPTYPIECE = -1\n\n\n\n#This procedure is the main engine of recursive algorithm\n#nextPiece is the number of next available tromino piece\n#The origin coordinates of the yard are given by originR and originC\ndef recursiveTile(yard, size, originR, originC, rMiss, cMiss, nextPiece):\n\n    #quadrant of missing square: 0 (upper left), 1 (upper right),\n    #                            2 (lower left), 3 (lower right)\n    quadMiss = 2*(rMiss >= size//2) + (cMiss >= size//2)\n    \n    #base case of 2x2 yard\n    if size == 2: \n        piecePos = [(0,0), (0,1), (1,0), (1,1)]\n        piecePos.pop(quadMiss)\n        for (r, c) in piecePos:\n            yard[originR + r][originC + c] = nextPiece\n        nextPiece = nextPiece + 1\n        return nextPiece\n\n    #recurse on each quadrant\n    \n    for quad in range(4):\n        #Each quadrant has a different origin\n        #Quadrant 0 has origin (0, 0), Quadrant 1 has origin (0, size//2)\n        #Quadrant 2 has origin (size//2, 0), Quadrant 3 has origin (size//2, size//2)\n        shiftR = size//2 * (quad >= 2)\n        shiftC = size//2 * (quad % 2 == 1)\n        if quad == quadMiss:\n            #Pass the new origin and the shifted rMiss and cMiss\n            nextPiece = recursiveTile(yard, size//2, originR + shiftR,\\\n                originC + shiftC, rMiss - shiftR, cMiss - shiftC, nextPiece)\n\n        else:\n            #The missing square is different for each of the other 3 quadrants\n            newrMiss = (size//2 - 1) * (quad < 2)\n            newcMiss = (size//2 - 1) * (quad % 2 == 0)\n            nextPiece = recursiveTile(yard, size//2, originR + shiftR,\\\n                            originC + shiftC, newrMiss, newcMiss, nextPiece)\n\n\n    #place center tromino\n    centerPos = [(r + size//2 - 1, c + size//2 - 1)\n                 for (r,c) in [(0,0), (0,1), (1,0), (1,1)]]\n    centerPos.pop(quadMiss)\n    for (r,c) in centerPos: # assign tile to 3 center squares\n        yard[originR + r][originC + c] = nextPiece\n    nextPiece = nextPiece + 1\n\n    return nextPiece\n\n#This procedure is a wrapper for recursiveTile that does all the work\ndef tileMissingYard(n, rMiss, cMiss):\n    #Initialize yard, this is the only memory that will be modified!\n    yard = [[EMPTYPIECE for i in range(2**n)]\n            for j in range(2**n)] \n    recursiveTile(yard, 2**n, 0, 0, rMiss, cMiss, 0)\n    return yard\n\n#This procedure prints a given tiled yard using letters for tiles\ndef printYard(yard):\n    for i in range(len(yard)):\n        row = ''\n        for j in range(len(yard[0])):\n            if yard[i][j] != EMPTYPIECE:\n                row += chr((yard[i][j] % 26) + ord('A'))\n            else:\n                row += ' '\n        print (row)\n\n\nprintYard(tileMissingYard(3, 4, 6))\nprintYard(tileMissingYard(4, 5, 7))\n```\n</details>\n\n# Problem 8: You Won't Want to Play Sudoku Again. Recursive search\n\n**Problem** Given a sudoku grid, find a solution to it. [problem](https://leetcode.com/problems/sudoku-solver/)\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Solution:\n    def solveSudoku(self, board: List[List[str]]) -> None:\n        \"\"\"\n        Do not return anything, modify board in-place instead.\n        \"\"\"\n        def get_empty(board):\n            for i,j in product(range(9),range(9)):\n                if board[i][j] == '.': return (i,j)\n            return -1,-1\n\n        def possible(i,j,board):\n            return set(list(map(str,range(1,10)))) - box(3*(i//3),3*(j//3)) - row(i) - col(j)\n\n        def box(i,j):\n            return set(board[i+di][j+dj] for di,dj in product(range(3),range(3)))\n\n        def row(i):\n            return set(board[i][j] for j in range(9))\n\n        def col(j):\n            return set(board[i][j] for i in range(9))\n\n        def solve():\n            i,j = get_empty(board)\n            if i == -1: return True\n            values = possible(i,j,board)\n            for val in values:\n                board[i][j] = val\n                if solve(): return True\n                board[i][j] = '.'\n            return False\n\n        solve()\n```\n</details>\n\nWhenever you do a recursive search you need to clean what you did (line `board[i][j] ='.'`) - this we call **backtracking**.\n\n\nThe above solution does not do any implications (row and column scans, boxes etc.) It is not the way you would solve sudoku in your head. The `poss()` function gives you potential values to fill in and you just do blind **guessing**. We want to integrate human approach in the algo - **implications!**\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n        def make_implications(board,i,j,e):\n            board[i][j] = e\n            impl = [(i,j)]\n            can_imply,start = False,True\n            while can_imply or start:\n                start,can_imply = False,False\n                for i,j in product(range(9),range(9)):\n                    if board[i][j] =='.':\n                        poss = possible(i,j,board)\n                        if len(poss)==1:\n                            x = poss.pop()\n                            impl.append((i,j))\n                            board[i][j] = x\n                            can_imply = True\n            return impl\n        \n        def undo_implications(board,impl):\n            for i,j in impl:\n                board[i][j] = '.'\n    \n        def solve():\n            i,j = get_empty(board)\n            if i == -1: return True\n            values = possible(i,j,board)\n            for val in values:\n                impl = make_implications(board,i,j,val)\n                if solve(): return True\n                undo_implications(board,impl)\n            return False\n        \n        solve()\n```\n</details>\n\n\n# Problem 9: The Disorganized Handyman. Sorting\n\n**Problem** You have 100 unique nuts and 100 unique bolts, where each bolt has its own unique nut. All nuts and bolts are in a bag. Find all nut-bolt pairs.\n\nAllowed operation:\n- take one nut and take one bolt. If nut is bigger than bolt, then the bolt would go through the nut. If the bolt is bigger, then you cannot put the bolt in. IF nut fits the bolt then done!\n\n\n**Naive solution**. $O(n^2)$ using brute force.\n\nCan't compare the size of nuts to nuts. Can't compare the size of bolts to bolts.\n\n**Solution**. Divide and Conquer - Quick Sort pivoting idea.\n\nQuickSort and Merge sort are Divide and Conquer algorithms.\n\nMergeSort requires $O(n)$ space and $O(nlogn)$ runtime.\n\nRandomized QuickSort requires $O(1)$ space and $O(nlog)$ runtime on average. Worse xas is $O(n^2)$\n\n\nQuickSort can be done in-place, whereas MergeSort cannot.\n\n# Problem 10: A Weekend To Remember. Graph\n\n**Problem**. Nodes are friends. Edge between nodes implies a 'dislike' relation. Your job is to keep your friends happy. You organize 2 parties such that:\n- each friend comes to exactly one party\n- no pair of friends who dislike each other come to the same party\n\nThis is **bipartite** matching problem. Purpose of this puzzle is to say if you can do that or not.\n\nBipartite graphs are those which you can split in two groups of nodes and all edges have one end in one of the groups and the other edge in the other group.\n\n[Leetcode](https://leetcode.com/problems/possible-bipartition/)\n[Leetcode](https://leetcode.com/problems/is-graph-bipartite/)\n\nBipartite graph = 2-colorable (color vertices such that no edge has two same color edges) =\ngraph with no odd-cycles\n\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nclass Solution:\n    def isBipartite(self, graph: List[List[int]]) -> bool:\n\n        def is_two_color(white,black):\n            for u in range(n):\n                for v in graph[u]:\n                    if u in white and v in white: return False\n                    if u in black and v in black: return False\n            return True\n        def colorize(s,is_black):\n            if is_black: black.add(s)\n            else: white.add(s)\n            visited[s] = True\n            for u in graph[s]:\n                if not visited[u]:\n                    colorize(u,1-is_black)\n        n = len(graph)\n        visited,black,white = [False]*n,set(),set()\n        for s in range(n):\n            if not visited[s]:\n                colorize(s,True)\n        return is_two_color(white,black)\n\n```\n</details>\n\n*Never depend of order of keys in a dictionary. Different looping through the same dictionary may give different order.*\n\n# Problem 11: Memory Serves You Well. Dynamic Programming\n\n**Problem.** Given array of coins, pick coins to maximize total value subject to constraint:\n- if you pick a coin you cannot pick the next one. [Leetcode](https://leetcode.com/problems/house-robber/)\n\n```Python\nclass Solution:\n    def rob(self, nums: List[int]) -> int:\n        @cache\n        def dp(i,can_rob):\n            if i == len(nums): return 0\n            if not can_rob: return dp(i+1,True)\n            return max(dp(i+1,False)+nums[i],dp(i+1,True))\n        return dp(0,True)\n```","n":0.02}}},{"i":165,"$":{"0":{"v":"Introduction to Algorithms","n":0.577},"1":{"v":"# MIT 6.001\n\nClassic course which you should know for life. All lectures in one place [here](https://drive.google.com/file/d/1FzGbSqWS9xufckvR5J5HrQ0qt-iVF1IU/view?usp=share_link)","n":0.25}}},{"i":166,"$":{"0":{"v":"Design and Analysis of Algorithms","n":0.447},"1":{"v":"# MIT 6.046\n\nLecture [notes](https://ocw.mit.edu/courses/6-046j-design-and-analysis-of-algorithms-spring-2015/). All notes + your marks in one place [here](https://github.com/ngocuong0105/dendron-wiki/blob/main/vault/assets/files/Engineering/Lectures/Design%20and%20Analysis%20of%20Algorithms%20MIT.pdf)\n\n\nLecture [videos](https://www.youtube.com/watch?v=2P-yW7LQr08&list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp)\n\n\n**Overview of course:**\n- Divide and Conquer (Merge Sort classic example, Fast Fourier transform, algorithm for convex hull, van Emde Boas trees)\n- Amortized analysis, random algos, quick select,quick sort skip lists, perfect and universal hashing\n- Optimization (greedy (Dijkstra), dynamic programming, shortest paths)\n- Network Flow (network, capacities, max flow - min cut problem)\n- Linear Programming\n- Intractability (polynomial vs exponential time problems, approximation problems)\n- Synchronous, Asynchronous algos, Cryptography, RSA \n- Cache, External Memory Model, Cache Oblivious Model, Complexity in terms of memory transfers \n\n# Lecture 1. Interval Scheduling\n\n*Very similar problems can have very different complexity*. Wiki problem [definition](https://en.wikipedia.org/wiki/Interval_scheduling#Group_Interval_Scheduling_Decision).\n\nPolynomial: Shortest path between two vertices. O(V**2)\n\nNP: Determine if a graph has Hamiltonian cycle: Given a directed graph find a simple cycle that contain each vertex V.\n\nNP-complete are the hardest problems in NP. Solve a NP-complete in polynomial time, then you can solve all NP problems.\n\n---\n\nGiven an array of intervals `[(s1,f1), (s2,f2) ... ]` - left closed, right opened. Select the maximum number of non-overlapping intervals.\nThis problem should not be confused with meeting rooms [problem](https://leetcode.com/problems/meeting-rooms-ii/) where we find minimum\nconference rooms needed (time with max overlap). The latter is line sweep.\n\n**Claim** We can solve this problem using a greedy algorithm.\n\n**Definition** A greedy algorithm is a myopic algorithm that processes the input one piece at a time with no apparent look ahead.\n\nNon-working greedy heuristics:\n- pick shortest intervals\n- pick intervals with least amount of overlaps. Counter example:\n\n```\n---- ---- ---- ----\n  ----- ---- ----\n  -----      ----\n  -----      ----\n  -----      ----\n```\nWorking greedy heuristic:\n- pick earliest finish time\n\n*\"Proof by intimidation, proof because the lecturer said so\"*\n\n**Proof by induction**\n\nClaim: Given a list of intervals `L`, greedy algorithm with earliest finish time produces `k*` intervals, where `k*` is maximum\n\nInduction on `k*`! Induction on the number of optimal intervals.\n\n1. Base case `k* = 1`. Any interval can be picked up.\n2. Suppose claim holds for `k*` and we are given a list of intervals whose optimal schedule has `k*+1` intervals $(s_1,f_1), ... (s_{k*+1},f_{k*+1})$\n\nRun our greedy algo and get intervals $(a_1,b_1), ... (a_{k},b_{k})$. $k$ and $k*$ are not comparable, yet. By construction $b_1 <= f_1$. Thus $S = (a_1,b_1), ... (s_{k*+1},f_{k*+1})$ is another optimal solution of size $k*+1$. Let $L'$ be the set of intervals where $s_i > b_2$. $S$ is optimal for $L$ then $S' =  (s_2,f_2)... (s_{k*+1},f_{k*+1})$ is optimal solution of $L'$ and has size $k$. By initial hypothesis we run greedy on $L'$ and produce\n$(a_2,b_2), ... (a_{k},b_{k})$ of size $k-1$. Then $k-1 = k*$ and proves the when we run greedy and get $(a_1,b_1), ... (a_{k},b_{k})$ is also optimal solution.\n\n\n---\n\n[Problem](https://leetcode.com/problems/maximum-profit-in-job-scheduling/). Weighted interval scheduling. Each interval has a weight $w_i$. Find a schedule with maximum total weight.\n\n*Greedy does not work here, need to use DP*\n\n$O(n^2), O(nlogn)$\n\n![dp_weighted_intervals.png](assets/images/dp_weighted_intervals.png)\n\n![dp_weighted_intervals_2.png](assets/images/dp_weighted_intervals_2.png)\n\n1. subproblem space:jobs[i:], dp(i) is max profit given jobs[i:] \n  - need NOT neccessarily include job[i]\n  - note need to sort by start/end time\n  - need to sort by start time and define subproblem jobs[i:] or sort by end time and define subproblem to be s[:i]\n\n2. O(n**2) is easy\n3. improve to O(nlogn) with BS\n\n`\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# O(n^2)\nclass Solution:\n    def jobScheduling(self, start: List[int], end: List[int], profit: List[int]) -> int:\n        @cache\n        def dp(i):\n            if i == len(jobs): return 0\n            val = max(dp(i+1),jobs[i][2])\n            for j in range(i+1,len(jobs)):\n                if jobs[i][1] <= jobs[j][0]:\n                    val = max(val,dp(j)+jobs[i][2])\n            return val\n        jobs = [(s,e,p) for s,e,p in zip(start,end,profit)]\n        jobs.sort(key=lambda x:x[0])\n        return dp(0)\n\n# O(nlog(n))\nclass Solution:\n    def jobScheduling(self, start: List[int], end: List[int], profit: List[int]) -> int:\n        def bs(l,r,num):\n            while l<r:\n                m = l+r>>1\n                if jobs[m][0]>=num:\n                    r = m\n                else:\n                    l = m+1\n            return l\n        @cache\n        def dp(i):\n            if i == len(jobs): return 0\n            j = bs(i+1,len(jobs),jobs[i][1])\n            return max(dp(i+1),dp(j)+jobs[i][2])\n        jobs = [(s,e,p) for s,e,p in zip(start,end,profit)]\n        jobs.sort(key=lambda x:x[0])\n        return dp(0)\n    \n# O(nlog(n))\nclass Solution:\n    def jobScheduling(self, start: List[int], end: List[int], profit: List[int]) -> int:\n        def bs(l,r,num):\n            while l<r:\n                m = l+r>>1\n                if jobs[m][1]>num:\n                    r=m\n                else:\n                    l=m+1\n            return l-1\n        @cache\n        def dp(j):\n            if j < 0: return 0\n            i = bs(0,j,jobs[j][0])\n            return max(dp(j-1),dp(i)+jobs[j][2])\n        \n        jobs = [(s,e,p) for s,e,p in zip(start,end,profit)]\n        jobs.sort(key=lambda x:x[1])\n        return dp(len(jobs)-1)\n\n```\n</details>\n\n\n---\n\nNP-complete problem. Generalization of the problem considers $k > 1$ machines/resources.Here the goal is to find $k$ compatible subsets whose union is the largest. First example had k = 1.\n\nIn an upgraded version of the problem, the intervals are partitioned into groups. A subset of intervals is compatible if no two intervals overlap.\n\n# Lecture 2. Divide & Conquer: Convex Hull, Median Finding\n\nDivide and Conquer Paradigm: Given a problem of size $n$:\n1. Divide it into $a$ subproblems of size $n/b$ where $b > 1$ so that your subproblems have smaller size.\n2. Solve each subproblem recursively\n3. Combine solutions of subproblems into overall solution (this is where the smarts of the algo is).\n\nRun-time: $T(n) = aT(n/b) + MergeWork$. Often to compute $T(n)$ you can use **Master Theorem**.\n\n![master_theorem.png](assets/images/master_theorem.png)\n\n---\n\n**Convex Hull Problem**\n\nGiven $n$ points in a plane $S = \\{ (x_i,y_i) | i = 1, 2, ..., n\\}$ assume no two have the same $x$ coordinate and no two have the same $y$ coordinate and no three line on the same line. The convex hull is the smallest polygon which contains all points in $S$.\n\n![convex_hull.png](assets/images/convex_hull.png)\n\nSimple algorithm:\n- For each pair of points draw a line.\n- This line separates the plane in two half-planes.\n- Check if all points lie in one side on the half-plane. If yes, this line is part of the convex hull (we call it a segment of the convex hull), otherwise it is not a segment.\n\nRun-time: $O(n^2)$ pairs of points, $O(n)$ to test $= O(n^3)$ runtime.\n\nDivide and Conquer algo:\n\n1. Sort the points by x coordinates.\n2. For input set $S$, divide into left half $A$ and right half $B$ by $x$ coordinates.\n3. Compute convex hull for $A$ and for $B$.\n4. Combine solutions.\n\n**Obvious algorithm** looks at all $a_i, b_j$ pairs and takes $T(n) = 2T(n/2) + O(n^2) = O(n^2)$ runtime\n\n\n![merge_convex_hulls.png](assets/images/merge_convex_hulls.png)\n\nFind upper, and lower tangent using two finger and a string algorithm. Compute intercept of the vertical line and all $(a,b)$ points. Algo [demo](https://youtu.be/EzeYI7p9MjU?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp&t=2161).\n\n*Merge step:*\nAfter you find the tangents $(a_i,b_j)$ and $(a_k,b_m)$ you link $a_i$ to $b_j$ go down all $b-s$ till you see $b_m$, then link to $a_k$ then go up through all $a-s$ till you see $a_i$.\n\n**Run time**\n\n$T(n) = 2*T(n/2) + O(n) = O(nlogn)$\n\nOther Convex Hull solutions:\n\n- [dbabichev](https://flykiller.github.io/patterns/geometry) chain multiplication?\n\n\n---\n\n**Median Finding Algorithm**\n\nIt is just quick-select algo. Randomized pivot point has $O(n)$ expected run time and $O(n^2)$ worse case. Smart pivot point is a deterministic algo (groups of 5 elements, median of medians)  has $O(n)$ worse case time. See CLRS for detailed algo (deterministic) $O(n)$ time to find median.\n\n# Lecture 3. Divide & Conquer: Fast Fourier Transform\n\nPolynomial of order $n-1$ $A(x) = a_0 + a_1x + a_2x^2 + ... + a_{n-1}x^{n-1}$\n\nOperations on polynomial:\n- evaluation $A(x_0) = ?$. Use **Horner's rule** to do it in $O(n)$. $A(x) = a_o + x(a_1 + x(a_2 + ... (xa_{n-1})))$ - $n$ multiplications and additions\n- addition $O(n)$\n- multiplication $A(x)*B(x) = C(x)$, The coefficients of $C$ are $c_k = \\sum_{j = 0}^{k} a_jb_{k-j}$ and that takes $O(k)$ time, so in total brute force multiplication of polynomials takes $O(n^2)$ run time.\n\nPolynomial multiplication is the same as doing **convolution** between vectors.\n```\nu = [1 1 1];\nv =  [1 1 0 0 0 1 1];\n1  1  1 -> 1\n   1  1 1 -> 2\n      1 1 1 -> 2\nconv(u,v) =  [1     2     2     1     0     1     2     2     1]\n```\n$u$ moves as a filter through $v$. Think convolutional neural networks! In polynomial multiplication $a = u, b = v$\n\n**Polynomial representation**\n- coefficient vector = $(a_0,a_1 ... a_{n-1})$\n- roots $r_0, r_1, ..., r_n$ (allowing multiplicity). $A(x) = c(x-r_0)(x-r_1)..(x-r_{n-1})$\n- samples/points (x_k,y_k) for $k = 0, 1 .. ,n-1$ for $A(x_k) = y_k$ has unique solution by the Fundamental Theorem in Algebra\n\nBy FTA $n-roots$ allowing multiplicity define uniquely the polynomial.\n\nroot representation is not good. Hard to go from polynomial to root representation. Addition is also very hard. Multiplication is easy.\n\n\n![poly_ops.png](assets/images/poly_ops.png)\n\nNo representation is perfect! Our aim is to convert from coefficient to sample representation in $O(nlogn)$ time using Fourier transform.\n\n*Coefficient repr -> Samples repr* can be done in $O(n^2)$ trivially\n\n![matrix_view.png](assets/images/matrix_view.png)\n\n- $V * A$ gets you the sample representation on $O(n^2)$\n- Samples to coefficients ($V$ and $y$ are known how would you find $a$):\n  - [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination) $O(n^3)$\n  - Multiply by inverse $V^{-1} * y$ - $O(n^2)$ computing inverse once and use for free.\n\n**Divide & Conquer Algorithm**\n\nWe have coefficient representation and want to get samples in $O(nlogn)$ time.\n\n$A(x) = (a_0,a_1,a_2...a_{n-1})$\n\n1. Divide into even and odd coefficients.\n\n$A_{even} = \\sum_{k=0}^{n/2} a_{2k}x^{k} = (a_0,a_2...)$\n\n$A_{odd} = \\sum_{k=0}^{n/2} a_{2k+1}x^{k} = (a_1,a_3...)$\n\n2. Conquer: Recursively compute $A_{even}(z)$ and $A_{odd}(z)$ for $z \\in \\{x^2: x\\in X\\}$\n\n3. Combine: $A(x) = A_{even}(x^2) + xA_{odd}(x^2)$\n\n*Run time*\n\n$T(n,|X|) = 2*T(n/2,|X|) + O(n+|X|))$\n\n\n![dc_FFT.png](assets/images/dc_FFT.png)\n\nTo get **collapsing** set $X$:\n\n![roots_of_unity.png](assets/images/roots_of_unity.png)\n\n[dbabichev](https://flykiller.github.io/patterns/number%20theory) FFT implementation.\n\n`np.convolve` is much slower as it does not use FFT. `scipy` has convolve function and uses FFT.\n\n# Lecture 4. Divide & Conquer: van Emde Boas Trees\n\nEmde Boas Tree.\n\n**Goal**: Maintain $n$ elements among $\\{ 0, 1, ... u-1\\}$ subject to insert, delete, successor (given a value I want to know the next greater value in the set). We know how to do all these in $O(logn)$ time using balance BST-s (AVL, Red-Black). Emde Boas Trees can doo these in $O(loglog[u] )$ which is an exponential improvement.\n\n**Intuition:** Binary search on the levels of the tree where the number of levels is $log(u)$\n\nCan we do better than $O(log(n)))$ and does not depend on the universe of numbers $\\{ 0, 1, ... u-1\\}$? **No.**\n\nIn each of previous tree data structures, where we do stuff dynamically, at least one important operation took $O(log(n)))$ time, either worst case or amortized. In fact, because each of these data structures bases its decisions on comparing keys, the $O(nlog(n)))$ lower bound for sorting tells us that at least one operation will have to take $O(log(n)))$ time. Why? If we could perform both the INSERT and EXTRACT-MIN\noperations in o.lg n/ time, then we could sort $n$ keys in $o(nlog(n)))$ time by first performing $n$ INSERT operations, followed by $n$ EXTRACT-MIN operations.\n\n\nFirst attempt to store an array of size $n$ with elements among the set $\\{ 0, 1, ... u-1\\}$:\n\n![bit_vec.png](assets/images/bit_vec.png)\n\nInsert and Delete are constant, Successor is linear.\n\nSecond attempt:\n\n![tree_emde.png](assets/images/tree_emde.png)\n\n\n![clusters.png](assets/images/clusters.png)\n\nThink each cluster as a binary tree built bottom up using the OR operation. All roots of the different clusters will be a 'summary' vector. As this vector will tell you if there is a one in each of the clusters.\n\nInsert is constant - need to change the value in the clusters and mark in the summary vector.\n\nSuccessor is:\n\n![successor.png](assets/images/successor.png)\n\n\nWe have not yet improved the runtime complexity. Van Emde Boas is to create clusters which are recursive and depend on other clusters.\n\n![emde_recurse.png](assets/images/emde_recurse.png)\n\n\nCode implementation details look at the lecture and your notes [here](https://github.com/ngocuong0105/dendron-wiki/blob/main/vault/assets/files/lecture4_emde_boas.pdf).\n\n# Lecture 5: Amortization\n\nSo far learning fancy cool data structures. This lecture is on fancy cool techniques of computing complexity of data structures.\n\nAmortized analysis is used to compute complexity of **data structures**. It computes the total run time of a data structure given $n$ executed operations. Amortized analysis is not used in computing run time of algorithms.\n\n**Table doubling**\n\nThat's how dynamic arrays work in Python.\n\nAssume a table of size $m$. We will double the size of the table whenever it is full. For $n$ insertions the total running time is:\n\n$O(2^0 + 2^1  + ... 2^{log(n)}) = O(n)$. Thus amortized (i.e.) per insertion it is $O(n)/n = O(1)$.\n\n**Techniques of amortized analysis:**\n\n- aggregate method\n- accounting method\n- potential method\n\nAll these method give upper bounds to the actual cost of all operations = amortized cost.\n\n**Aggregate method**\n\nThis is what we did in table doubling. Added total cost of $n$ operations then divided by $n$.\nNB: Often when you have a data structure with total of $n$ insert and delete operations you can just think of $n$ insert operations because each element can be deleted at most the number of times it has been inserted.\n\n**Accounting method**\n\nStore credit bank account which should always be **non-negative**. When you do an operation you pay for the operation and can deposit money in you account.\n\nExample. Table Doubling.\n\n- If insertion does not trigger table doubling: I would pay 1 coin for the insertion plus c coin deposit.\n- If insertion trigger table doubling: I can pay from the bank account to cover the table doubling.\n\n- amortized cost for table doubling when the table becomes of size $2*n$ is $O(n) - c * n/2 = O(1)$ for chosen large enough $c$. When table doubles from $n$ to $2n$, only last $n/2$ places have coins.\n\n- amortized runtime per insertion $1+c = O(1)$\n\n\nAside. Table expansion and contraction have insert and delete operations in $O(1)$ amortized if:\n- table doubling when table is full\n- table contraction by a halve when there are $m/4$ elements in the table of size $m$\n\nTo prove this is indeed $O(1)$ amortized you need the Potential method.\n\n**Potential method**\n\nDefine a *potential (energy) function* $\\Phi$ which maps each data structure $D_i$ to a nonnegative integer. Check CLRS for more details and proof of Table doubling and halving is $O(1)$.\n\n# Lecture 6: Randomized Algorithms\n\nRandomized algorithm is one that generates a random number $r$ and makes decisions on $r$'s value.\n\nOn same input and on different execution randomized algos may:\n- run a different number of steps\n- produce different output\n\nTwo types of random algos:\n- Monte Carlo - runs in polynomial time always output is correct with **high probability**\n- Las Vegas - runs in **expected** polynomial time output is always correct\n\n**Monte Carlo = probably correct**\n**Las Vegas = probably fast**\n\nMonte Carlo is for estimation stuff to get almost correct values.\n\nLas Vegas example is quick sort - $O(nlog(n))$ expected time. 'Almost sorted' does not make sense.\n\n\n**Problem. Matrix Product Checker**\n\nGiven $nx$n$ matrices $A, B, C$ the goal is to check if $A$ x $B=C$. We use randomized algo so that we do not checkout the full multiplication.\n\n**Freivalds' algorithm**\n\nProcedure:\n\n1. Generate an $n × 1$ random $0/1$ vector $r$ .\n2. Compute $P = A × (Br) - Cr$\n3. Output \"Yes\" if $P = ( 0 , 0 , … , 0 )$; \"No,\" otherwise.\n\n\nIf $A × B = C$ , then the algorithm always returns \"Yes\". If $A × B \\neq C$ then the probability that the algorithm returns \"Yes\" is less than or equal to one half.\n\nBy iterating the algorithm $k$ times and returning \"Yes\" only if all iterations yield \"Yes\", a runtime of $O(k n^2)$ and error probability of $<= 1/2^k$ is achieved.\n\n*Proof that $P($false negatives$) \\leq 1/2$*. Idea is for every bad $r$ which does not catch that $AB - C \\neq 0$ we create a good $r$ and have 1-1 mapping. See your [notes](https://github.com/ngocuong0105/dendron-wiki/blob/main/vault/assets/files/Engineering/Design%20and%20Analysis%20of%20Algorithms%20MIT.pdf).\n\n\n**Paranoid Quick Sort**\n\n![paranoid_quick_sort.png](assets/images/paranoid_quick_sort.png)\n\n![paranoid_qs_analysis.png](assets/images/paranoid_qs_analysis.png)\n\nParanoid Quick sort is probably fast with expected running time $O(nlog(n))$.\n\n# Lecture 7 Randomization: Skip Lists\n\n A skip list a **probabilistic** data structure that allows $O(log(n))$ search, insert, delete within a set of $n$ elements. It maintains a linked list hierarchy of subsequences with each successive subsequence skipping over fewer elements than the previous one. In the example below we insert 80.\n\nComparing with treap and red-black tree which has the same function and performance, the code length of Skiplist can be comparatively short and the idea behind Skiplists is just simple linked lists.\n\n ![skip_list.png](assets/images/skip_list.gif)\n\nDesign a skip list, [leetcode](https://leetcode.com/problems/design-skiplist/).\n\n**Motivation for skip lists.**\n\nStep 1. Linked list - search is $O(n)$\n\nStep 2. Sorted Linked list - search is still $O(n)$\n\nStep 3. Two sorted Linked list, where the second one is a subsequence (express line) and skips elements.\n\nStep 4. Add $log(n)$ layers of linked lists\n\n![skip_list_motivation.png](assets/images/skip_list_motivation.png)\n\n\n- Insert is randomized using a fair coin\n- Search and Delete are deterministic\n\n\n**Why skip lists are good?**\n\n**Warm-up Lemma,** The number of levels in $n-$element skip list is $O(log(n))$ with high probability, that is as $n$ grows we the probability converges to 1. This is stronger than having expected running times stuff where you have no guarantees of how likely the worst case is.\n\nWith this lemma we can say thing like:\n- number of levels in the skip list is at most $2log(n)$ with $90\\%$ probability\n- at most $4log(n)$ with $99.9\\%$ probability etc\n\n**Proof.**\n$P(>= clog(n) levels) = P($some element got $>=clog(n)$ promotions $) = (1/2)^{clog(n)} \\leq \\dfrac{n}{n^c} = \\dfrac{1}{n^{c-1}}$\n\n\n**Search**\n\nTheorem.  Any search in $n$-element skip list costs $O(log(n))$\n\nSteps:\n1. We track the search moves **backwards**\n2. Backwards search makes up and left moves each with probability 1/2\n3. Number of ups is less than number of levels  $\\leq c(log(n))$ with high probability\n4. The BUM: Total number of moves = number of coin flips until you get $c(log(n))$ heads (up moves)\n\nClaim: Number of coin flips until we see $c(log(n))$ heads is $O(log(n))$ with high probability.\n\nWe need to proof that for to see $clog(n)$ heads, there exist a constant $d$ such that if $Y$ is a random vairable counting the number of heads after $dlog(n)$ coin flips then $P(Y < clog(n)) = \\dfrac{1}{n^{\\alpha}}$. Idea is to use [Chernoff bound](https://en.wikipedia.org/wiki/Chernoff_boundhttps://en.wikipedia.org/wiki/Chernoff_bound).\n\n# Lecture 8: Randomization: Universal & Perfect Hashing\n\n**Buzz words:** Hashing with chaining, open addressing, load factor $n/m$, Simple Uniform Hashing assumption, Hash functions, linear probing, quadratic probing, universal hashing, perfect hashing\n\n**Dictionary problem.** Abstract Data Type (Math definition, interface):\n- maintain a dynamic set of items\n- each item has a key, item is a key value pair\n- insert(item)\n- delete(item)\n- search(key)\n\nGoal is to run all 3 operations in $O(1)$ expected time (amortized).\n\nIn MIT 6.001- [[engineering.MIT.Introduction to algorithms]] you saw hashing with chaining and open addressing. you proved that the insert, delete and search take $O(1+\\dfrac{n}{m})$, where n is the number of elements you have in the table and m is the number of slots (table size, number of buckets). So as long as you chose $m = O(n)$ (you can keep that dynamically using  table doubling and shrinking) you would have $O(1)$ operations.\n\nHowever, you **assumed** that you have **simple uniform hashing**. That is your keys are mapped at each slot of the table with probability $O(\\dfrac{1}{m})$ So you had to choose a smart hashing function that would map your keys uniformly. However you want, a hashing function that work work well no matter what the keys are. That is in the worst case scenario for the keys you still want $O(1)$ operations. Our analysis in MIT 6.001 considered average case scenario, where for random keys we would have simple uniform hashing.\n\nWant to avoid the assumption that the keys are random.\n\n**Universal Hashing**\n\nWorks for dynamic sets - allows insert and delete\n- choose $h$ randomly from a hash family $H$.\n- assume $H$ ti be universal hash family:\n  - for all keys $k,k'$: $P(h(k)=h(k')) \\leq \\dfrac{1}{m}$, probability over choosing $h$.\n\nYou can prove using indicator variables that $E($number of keys hashing to the same place as $k_i) \\leq 1+n/m$\n\n*Dot product hash family*\n- assume $m$ m is prime\n- assume $u = m^r$ for integer $r$\n- view key $k$ in base $m$, $k = (k_0,k_1, .. k_{r-1})$\n- for a key $a = (a_0, .. a_{r-1})$ define $h_a(k) = (a \\times k) \\mod m$ (dot product)\n\n$H = \\{h_a| a \\in 0...u-1\\}$. To choose random $h_a$ choose a random $a$.\n\nAnother universal hashing family:\n\n\n![universal_hashing.png](assets/images/universal_hashing.png)\n\nWe achieved $O(1)$ expected time for all operations.\n\n**Perfect Hashing**\n\nThis works for static keys and support search only. It is perfect hashing because it achieves $O(1)$ search worst case, that is keys are stored perfectly with no collisions.\n\n- polynomial build time with high probability\n- worst case $O(1)$ run time\n- worst case memory $O(n)$\n\nIdea: Use 2-level hashing.\n\n![2-level-hashing.png](assets/images/2-level-hashing.png)\n\n\n# Lecture 9: Augmentation. Range Trees\n\n\n**Easy Tree Augmentation.**\n\nThe goal here is to store $x.f$ at each node $x$, which is a function of the node, namely $f($subtree rooted at $x)$. If $x.f$ is computable **locally** from its children then updates take $O(h)$ runtime where h is the height of the tree. To update $x.f$, we need to walk up the tree to the root.\n\n**Order-statistic trees.**\n\nADT (interface of the data structure):\n- insert(x), delete(x), successor(x)\n- rank(x)\n- select(i): find me the element of rank i\n\nWant all of these in $O(log(n))$\n\n\nWe can implement the above ADT using easy tree augmentation on AVL trees (or 2-3 trees or any balance BST) to store subtree size: $f($subtree$)$ = $#$ of nodes in it.\n\n![rank_select.png](assets/images/rank_select.png)\n\nNB: Need to choose augmentation functions which can be maintained easier such as subtree size. Above we could thing to store the rank of each node (rank and select would be very easy). However if you insert elements it would be $O(n)$, e.g. if you insert a minimum element you need to update all node ranks.\n\n\n#TODO Finger Search Trees (this is tree augmentation on 2-3 threes)\n\n\n**Range Trees**\n\nSolves the problem orthogonal range search.\n\nSuppose we have $n$ points in a $d$-dimension space. We would like a data structure that supports range query on these points: ﬁnd all the points in a give axis-aligned box. An axis-aligned box is simply an interval in 1D, a rectangle in 2D, and a cube in 3D.\n\nGoal is to run in $O(log^d(n) + ($ output size $))$.\n\nNB: Our data structure would be static and support only search points in box query.\n\n**1D case**\n\nWe have array `[a_1,a_2...a_n]` and for a query `search(x,y)` we want to output all numbers in the array which are in the interval `[x,y)`. Simple solution is to use sorted array and then do binary searches in $O(log(n))$.\n\nSorted arrays are inefficient for insertion and deletion. For a dynamic data structure that supports range queries such as AVL, Red-Black trees.\n\nHowever, neither of the above approaches generalizes to high dimensions.\n\n**1D range trees**\n\n![range_tree.png](assets/images/range_tree.png)\n\nThat is like doing rightful rank for node $a$ and left rank for node $b$.\n\n**Analysis.** $O(lg n)$ to implicitly represent the answer (showing just the roots). $O(lg n + k)$ to output all k answers. $O(lg n)$ to report k via subtree size augmentation.\n\n![range_tree_pic.png](assets/images/range_tree_pic.png)\n\n**2D range trees**\n\nCreate a 1D range tree on the x coordinates. Do a search to find $O(lg(n))$ nodes which satisfy the interval provided by the $x$ coordinate. **Data Augmentation**: for each of these trees we store another range tree on the y coordinate. So we have dictionary with keys being the nodes of the first range tree, and values is range tree by the y coordinate. There is lots of data duplication. Then you do a little search on the $y-coordinate range tree$. Run time $O(log^2(n))$\n\nSpace complexity is $O(n lg n)$. The primary subtree is $O(n)$. Each point is dupli­cated up to $O(lg n)$ times in secondary subtrees, one per ancestor.\n\n*Aside*\n\nRange trees are used in database searches. For example if you have 4 columns in a database and you do searches like col1 should be inside one interval col2 in another interval, etc. range trees would allow fast queries. This is called indexing in database columns.\n\n\n# Lecture 10: Dynamic Programming. Advanced DP\n\nWe consider 3 problems:\n\n- longest palindromic subsequence\n- optimal binary search tree\n- alternating coin game\n\n**DP steps**:\n1. Determine Subspace of problems\n2. Define Recursive relation based on optimal solutions of subproblem\n3. Compute value of an optimal solution (bottom-up, top down)\n4. Construct optimal solution from computed information (typically involves some backtracing)\n\n**Longest Palindromic Sequence**\nGiven a string $s$ find the longest subsequence which is a palindrome (not necessarily contiguous).\n\n*Solution*\n`\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# O(n**2)\ndef solve(s):\n    @cache\n    def dp(i,j):\n      if i == j: return 1\n      elif  i > j: return 0\n      if s[i] == s[j]: return 2 + dp(i+1,j-1)\n      return max(dp(i+1,j),dp(i,j-1))\n```\n</details>\n\nRun time = Number of subproblems * Time per subproblem (assumes lookup is $O(1)$)\n\nIf you use arrays for lookup you would have constant access, in hash tables you get it constant amortized (collisions).\n\n\n**Optimal Binary Search Trees**\n\nGiven keys $K_1, K_2 ... K_n$, where $K_1 < .. < K_n$ WLOG $K_i = i$. There are many different BST-s with these set of keys.\nWe assign weights for each of these keys $W_1,W_2 ... W_n$. You can think that the weights are probabilities of searching each of the keys (search probabilities). Find/construct a BST that minimizes:\n\n$\\sum W \\times ($ depth $(K_i)  + 1)$.\n\nThe root has depth 0. Application: needs a structure which would minimize the expected search cost.\n\n\n![optimal_bst.png](assets/images/optimal_bst.png)\n\n\nBefore doing DP, try greedy!\n\nWe choose the root $K_r$ to be the key with largest weight. Then you know which nodes are on the left and which on the right. Continue do greedy approach in recursive fashion. But this does not work:\n\n![optimal_bst_greedy.png](assets/images/optimal_bst_greedy.png)\n\n\n**DP solution**\n\nSubproblem space $e(i,j) =$ cost of optimal BST on $K_i, K_{i+1} ... K_j$\n\n![optimal_bst_dp.png](assets/images/optimal_bst_dp.png)\n\n\n**Alternating Coin Gamse**\n\nRow of $n$ coins of values $V_1, ... , V_n$, where $n$ is even. In each turn, a player selects either the first or last coin from the row, removes it, and receives the value of the coin\n\nThe first player never looses (win or equal)! He can make it so that he pick all values on odd indices or on even indices, depending which sum gives the larger sum.\n\n\nHow to maximize the amount of money won assuming you move first?\n\nSubproblem space: $V(i,j)$ is the max value we can definitely win if it is *our* turn and only conis $V_i .. V_j$\n\n\n![coin_game.png](assets/images/coin_game.png)\n\n[leetcode](https://leetcode.com/problems/stone-game/)\n\n\n# Lecture 11. Dynamic Programming. All-Pairs Shortest Paths\n\nTwo types of shortest path problems:\n\n- single source (from one source $s$ find the shortest path to all vertices $v \\in V$)\n- all pairs shortest path\n\nThe problem with one source and one destination cannot be solved faster than the single source shortest path, so when you solve it you would use the single source shortest path solutions such as Dijkstra and Bellman Ford.\n\n![single_source.png](assets/images/single_source.png)\n\nNote that Dijkstra works only for non-negative weights. It is a type of **greedy algorithm**.\n\nBellman Ford works for general graphs. It is a DP algo.\n\n\n**APSP = All pairs shortest path**\n\nOne way to solve this problem is to run $V$ times Dijkstra or Bellman Ford.\n\n\n\nDP First attempt.\n\nIf your subproblem space is $dp[u][v]$ that is shortest path from $u$ to $v$, then you would not have a DAG subproblem space! Enter an infinite recursion when trying to resolve your subproblems.\n\nNatural improvement of subspace is $dp[u][v][m] = $ weight of shortest path from $u$ to $v$ $\\leq m $ edges.\n\nBelow are the 5 steps of DP thinking from Eric:\n\n\n![dp_shortest_path_1.png](assets/images/dp_shortest_path_1.png)\n\n`\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\n# Bottom-up via relaxation steps\nfor m = 1 to n by 1\n  for u in V\n    for v in V\n      for x in V # this is the min step\n        if duv > dux + dxv # can put wxv too\n            duv = dux + dxv\n```\n\n</details>\n\nRuntime is $O(V^4)$, which is the same as running $V$ time Bellman Ford.\n\n\n**Matrix Multiplication**\n\nGiven $n \\times n$ matrices $A$ and $B$ compute their product $C = A \\times B$.\n\n- $O(n^3)$ standard algo\n- O(n^{2.807}) via Strassen\n\nMatrix multiplication is the same as the above recurrence relation\n\n$c_ij = \\sum_k a_{ik} \\dot b_{kj}$ is similar to $d_{uv} = min(d_ux + w(x,v))$ for $x \\in V$\n\nDefine the summation operand to be a min, and the multiplication operand to be $+$.\n\nWe can **redefine** the DP problem using Matrix multiplication language.\n\n![matrix_mult_short_path.png](assets/images/matrix_mult_short_path.png)\n\nThe shortest distance matrix we want to compute $D^m$ equals $W^m$ where the powers is defined in circle land.\n\nAll pairs shortest path problem requires computing $W^n$ in circle land. Single matrix multiplication is $n^3$, hence total complexity is $O(V^4)$ - same algo as above just expressed in another language. However with matrix multiplication you can use **repeated squaring** trick and get running time $O(n^3lgn)$.\n\n\n**Floyd-Warshall**\n\n![floyd_warshall.png](assets/images/floyd_warshall.png)\n\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\nC = (w(u, v))\nfor k = 1 to n by 1\n  for u in V\n    for v in V\n      if c uv > c uk + c kv\n        c uv = c uk + c kv\n\n```\n\n</details>\n\nRun time $O(V^3)$\n\n**First loop on k, otherwise it will error. k is the phase count.**\n\n**Jonhson's algorithm**\n\nIdea is to do graph re-weighting so that we have nonnegative weights and run Dijkstra. Shortest paths are preserved.\n\n\n![johnson.png](assets/images/johnson.png)\n\n*How to find a function* $h$?\n\nYou want to find $h$ which satisfies $h(v) - h(u) \\leq w(u,v)$ for all $(u,v) \\in V$. This is called a **system of difference constraints**. \n\n**Theorem.** If there is a negative-weight cycle, there there exist **no** solution to the above system.\n\n\n**Theorem.** If $(V, E, w)$ has no negative-weight cycle, then we can ﬁnd a solution to the difference constraints.\n\n**Proof by example.** Add new vertex (source) $s$ and connect it to any other vertex and add 0 weights. Compute single source shortest path from $s$ and get $dist[s][v]$ for every $v \\in V$. This is your function $f$. Prooved by triangle inequality.\n\n**Time complexity**\n\n1. The first step involves running Bellman-Ford from s, which takes $O(V E)$ time. We also pay a pre-processing cost to re-weight all the edges $(O(E))$.\n\n2. We then run Dijkstra’s algorithm from each of the $V$ vertices in the graph; the\ntotal time complexity of this step is $O(V E + V 2 lg V )$\n\n3. We then need to re-weight the shortest paths for each pair; this takes $O(V^2)$ time.\n\n\nThe total running time of this algorithm is $O(V E + V^2 lg V)$.\n\n# Lecture 12: Greedy Algorithms. Minimum Spanning Tree\n\n- Prim's algorithm\n- Kruskal's algorithm\n\nRecall that a greedy algorithm repeatedly makes a locally best choice or decision, but ignores the effects of the future.\n\n**Minimum spanning tree problem.**\n\nA *spanning tree* of a graph $G$ is a subset of the edges of $G$ that form a tree and include all vertices of $G$. Given an undirected graph $G = (V, E)$ and edge weights $W : E → R$, find a spanning tree $T$ of minimum weight sum $\\sum w(e)$. We take some edges of the graph, hit all vertices and minimize the weight sum.\n\n\n**Properties of a greedy algorithm:**\n\n- Optimal Substructure: the optimal solution to a problem incorporates the optimal solution to subproblem(s)\n- Greedy choice property: locally optimal choices lead to a globally optimal solution\n\nIn DP you would do guessing, unlike greedy where you are greedy and take the best local option.\n\n**Lemma 1.** If $T'$ is a minimum spanning tree of $G/e$, then $T' \\cup {e}$ is an MST of $G$.\n\n*Contract edge $e$ - idea*. This is to combine two nodes into one and solve the smaller problem.\n\n![edge_contraction.png](assets/images/edge_contraction.png)\n\nThe statement can be used as the basis for a dynamic programming algorithm, in which we guess an edge that belongs to the MST, retract the edge, and recurse. At the end, we decontract the edge and add e to the MST. The lemma proves correctness of the algo but it would be exponential. At each step you guess one random edge of all possible edges.\n\n**We need an intelligent way to choose edge $e$**.\n\n\n**Lemma 2** (Greedy-Choice Property for MST). For any **cut** $(S, V \\ S)$ in a graph $G = (V, E, w)$, any least-weight crossing edge $e = {u, v}$ with $u \\in S$ and $v \\in S$ is in some MST of $G$.\n\n**This lema is your golden ticket to use greedy algo.**\n\n\n**Prim's algorithm**\n\nIt is Dijkstra like.\n\nIdea is to start with one vertex $s$. This is your initial cut $s$ vs the rest.\n\n`\n<details>\n<summary> <b>CODE</b> </summary>\n\n```Python\ndef dist(a,b):\n    return abs(a[0]-b[0]) + abs(a[1]-b[1])\nh = [(0,0)] # dist,s\nedges = {s:float('inf') for s in range(len(points))}\nedges[0] = 0\nparent = {}\nvisited = set() # keeps track of your cut, in Dijkstra you do not need that\nwhile h:\n    d,s = heappop(h)\n    visited.add(s)\n    for u in range(len(points)):\n        if u not in visited and dist(points[u],points[s]) < edges[u]:\n            edges[u] = dist(points[u],points[s])\n            parent[u] = s\n            heappush(h,(edges[u],u))\n# edges has the weights of the MST tree\n```\n</details>\n\n![run_time_prim.png](assets/images/run_time_prim.png)\n\nRuntime just like Dijkstra.\n\n**Kruskal's Algoritm**\n\nKruskal constructs an MST by taking the globally lowest-weight edge and contracting it.\n\n```\nsort the edges in nondecreasing weights\nfor edge in edges:\n     add edges consecutively to the DSU in this order (keep tree)\n```\n![runtime_kruskal.png](assets/images/runtime_kruskal.png)\n\n# Lecture 13: Incremental Improvement. Max Flow, Min Cut\n\nAll about **Ford-Fulkerson** max-flow algorithm and Max Flow - Min Cut Theorem.\n\n## Flow network\n\nDefinition. A flow network is a directed graph $G = (V, E)$ with two distinguished vertices: a source $s$ and a sink $t$. Each edge $(u, v) \\in E$ has a nonnegative capacity $c(u, v)$. If $(u, v) ∉ E$,\nthen $c(u, v) = 0$.\n\n\n![flow_network.png](assets/images/flow_network.png)\n\n**Maximum-flow problem** Given a flow network $G$, fund a flow of maximum vale on $G$ = max flow rate you can send from source to the sink.\n\n**Net Flow**\n\nA net flow on $G$ is a function $f: V \\times V -> R$ satisfying:\n- capacity constraint: for all $u,v \\in V$: $f(u,v) \\leq c(u,v)$\n- skew symmetry: for all $u,v \\in V: f(u,v) = -f(v,u)$\n- flow conservation: for all $u \\in V - \\{s, t\\}$: $\\sum f(u,v) = 0$\n\n**Definition** The **value** of a flow $f$, denoted by $|f|$ is given by $|f| = \\sum_{v \\in V} f(s,v) = f(s,V)$\n\nWe use *implicit* summation notation. Example for writing flow conservation:\n\n$f(u,V) = 0$ for all $u \\in V - \\{s,t\\}$.\n\n**Theorem** The value of a flow satisfies: $|f| = f(V,t)$, what goes out of the source equals what enters the sink.\n**Proof.**\n\n$|f| = f(s,V) = f(V,V) - f(V-s,V) = 0 + f(V,V-s) = f(V,t) + f(V,V-s-t) = f(V,t)$ (last step by conservation law)\n\n\n**Cuts**\n\nDefinition. A cut $(S, T)$ of a flow network $G = (V, E)$ is a partition of $V$ such that $s ∈ S$ and $t ∈ T$. If $f$ is a flow on $G$, then the flow across the cut is $f(S, T)$.\n\n![cut.png](assets/images/cut.png)\n\n**Lemma** For any flow and any cut $(S,T)$ we have $|f| = f(S,T)$\n**Proof**  $f(S,T) = f(S,V) - f(S,S) = f(S,V) = f(s,V) + f(S-s,V) = f(s,V) = |f|$\n\nYou've got a flow, the flow of the value is the flow value of the cut, as long as you have the source in one place of the cut and the sink on the other part of the cut.\n\n\nNote $f(S-s,V) = 0$ because $S$ does not contain $t$ and we use flow conservation.\n\n**Definition** The capacity of a cut $(S,T)$ is $c(S,T)$\n\n![capacity_cut.png](assets/images/capacity_cut.png)\n\nUpper bound on the maximum flow value:\n**Theorem**. The value of any flow is bounded above by the capacity of any cut.\n\nResidual network points you where in the network you have free capacities to put flow through. It has the same vertices as the original graph but different edges.\n\n![residual_network.png](assets/images/residual_network.png)\n\nLast two lines above say that your residual network might introduce extra edges which are not in the original network. Residual networks depend on the flow $f$.\n\n![residual_network_example.png](assets/images/residual_network_example.png)\n\n**Definition** Any path from $s$ to $t$ in $G_f$ is an augmenting path in $G$ with respect to $f$. If you have an augmenting path your flow $f$ is not a maximum flow. The flow value can be increased along an augmenting path $p$ by $c_f(p) = min(c_f(u,v)) $ for $(u,v) \\in p$\n\nYour augmenting path tells you which edges in the original graph $G$ with your flow $f$ how you change the values on the augmenting path.\n![augmenting_path.png](assets/images/augmenting_path.png)\n\n\n\n# Lecture 14: Incremental Improvement. Matching\n**Max-flow, min-cut theorem**\n\nTheorem. The following are equivalent:\n1. $|f| = c(S, T)$ for some cut $(S, T)$.\n2. f is a maximum flow.\n3. f admits no augmenting paths.\n\nFord-Fulkerson max-flow algo.\n\n```\ninitialize f(u,v) = 9 for all u,v in V\nwhile an augmenting path in G wrt f exists:\n  do augment f by c_f(p)\n```\n\nTo prove correctness of Ford-Fulkerson we need to prove that 3 implies 2.\n\nWe will rove the theorem by proving 3 implies 1, 1 implies 2 and 2 implies 3.\n\n**1 implies 2**. $|f| \\leq c(S,T)$ for any cut. The assumption that $|f| = c(S,T)$ shows $f$ is a maximum flow as it cannot be increased.\n\n**2 implies 3**. If there was an augmenting path then the flow value could be increased, hence contradicts that $f$ is a maximum flow.\n\n**1 implies 3** [proof](https://youtu.be/8C_T4iTzPCU?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp&t=1496)\n\n\nFord Fulkerson depends a lot on which augmenting paths you choose every time. Depending on the the order of your Edges, the DFS would choose different augmenting paths. Some of them would have residual flows which are super small and on each augmentation you would add very small flow.\n\nIf you do BFS augmenting path search (assuming each edge is of weight 1)  then augmentation is proven to be $O(VE)$, hence the final run time of the algo is O(VE(V+E))\n\n**Baseball elimination**\n\nA team survives if it has the largest number of wins. Given a table of standings we want to know which teams still have a chance of surviving.\n\nCheck the [notes](https://github.com/ngocuong0105/dendron-wiki/blob/main/vault/assets/files/Engineering/Lectures/Design%20and%20Analysis%20of%20Algorithms%20MIT.pdf)\n for this example.\n\n\n# Lecture 15: Linear Programming. LP, reductions, Simplex.\n\nYou can formulate the max flow problem as an LP problem. LP is more general than Max Flow.\n\nLinear programming (LP) is a method to achieve the optimum outcome under some requirements represented by linear relationships.\n\n\nLP is polynomial time solvable. Integer LP is NP hard (that is we add extra constraint that the $x$ variables are integers).\n\nIn general, the **standard form** of LP consists of\n\n• Variables: $x = (x_1 , x_2 , . . . , x_d)^T$\n\n• Objective function: $c · x$\n\n• Inequalities (constraints): $Ax ≤ b$, where $A$ is a $n × d$ matrix\n\nand we maximize the objective function subject to the constraints and $x ≥ 0$.\n\n\nThe natural LP formulation of a problem may not result in the standard LP form. Do these transformations:\n\n- Minimize an objective function: Negate the coeﬃcients and maximize.\n- Variable xj does not have a non-negativity constraint: Replace $x_j$ with\n$x'_j − x''_j$, and $x'_j$ , $x'_j,x''_j ≥ 0$.\n- Equality constraints: Split into two diﬀerent constraints; $x = b$ into $x ≤ b, x ≥\nb$.\n- Greater than or equal to constraints: Negate the coeﬃcients, and translate to less than or equal to constraint.\n\n**Linear Programming Duality**\n\nGives you a certificate of optimality. If I get a solution of the LP problem, I can get a certificate that it is the optimal one (only if indeed it is the optimal one).\n\nFor every **primal** LP problem in the form of:\n```\nMaximize c · x\nSubject to Ax ≤ b, x ≥ 0,\n```\nthere exists an equivalent **dual** LP problem\n\n```\nMinimize b · y\nSubject to A^T y ≥ c, y ≥ 0.\n```\n\nThe max-ﬂow min-cut theorem can be proven by formulating the max-ﬂow problem as the primal LP problem.\n\n\n**Maximum Flow**\nGiven $G(V, E)$, the capacity $c(e)$ for each $e \\in E$, the source $s$, and the sink $t$:\n\nMaximize $\\sum_{\\text{over v}}f(s, v)$\n\nSubject to $f(u, v) = −f (v, u) ∀u, v ∈ V$ skew symmetry\n\n$f(u, v) = 0 ∀u ∈ V − \\{s, t\\}$ conservation \n\n$v∈V f (u, v) ≤ c(u, v) ∀u, v ∈ V$ capacity.\n\nLP could be used for multi-commodity problems.\n\n**Shortest Paths**\n\nGiven $G(V, E)$, weights $w(e)$ for each $e ∈ E$, and the source $s$, we want to find the shortet paths from s to all $v \\in V$, denoted $d(v)$.\n\n```\nMaximize \\sum_{v∈V}d(v)\n\nSubject to d(v) − d(u) ≤ w(u,v) ∀u, v ∈ V -  triangular inequality\n\nd(s) = 0.\n```\n\nNote the maximization above, so all distances don’t end up being zero. In the inequalities constrins I have minimim already.\n\n**LP Algorithms**\n\n1. Simplex algorithm\n2. Ellipsoid algorithm\n3. Interior Point Method\n\n**Simplex**\nThe simplex algorithm works well in practice, but runs in ex­ ponential time in the worst case. Steps:\n\n• Represent LP in “slack” form. \n\n• Convert one slack form into an equivalent slack form, while likely increasing the\nvalue of the objective function, and ensuring that the value does not decrease.\n\n• Repeat until the optimal solution becomes apparent.\n\nSlackness is a measure of how tight are our constriants. See Lecture notes for example of algo.\n\nAt each step of the simplex algo, you increase the objective value, while maintaining correctness of constraints.\n\nIn general, simplex algorithm is guaranteed to converge in $(n+m)$ choose $m$,  iterations where $n$ is the number of variables, and $n + m$ is the number of constraints.\n\n# Lecture 16. Complexity: P, NP, NP-completeness, Reductions\n\n**P**\n\n$P = \\{ \\text{problems solvable in polynomial times } O(n^{O(1)}) \\}$\n\n**NP**\n\n$NP = $ {decision problems (answer is yes or no) solvable in nondeterministic polynomial time} \n\nNondeterministic refers to the fact that a solution can be guessed out of polynomially many options in O(1) time. If any guess is a YES instance, then the nondeterministic algorithm will make that guess. NP is biased towards YES.\n\n**P vs NP**\n\nDoes being able to quickly recognize if a solution is correct, means that you can quickly solve the problem?\n\nIf yes, then $P = NP$\n\n\nSudoku is NP-complete when generalized to a $n × n$ grid. \n\n[youtube](https://www.youtube.com/watch?v=YX40hbAHx3s)\n\n|**Example NP problem. 3SAT**\n\nSAT = satisfiability\n\nAT is satisfiability problem - say you have Boolean expression written using only AND, OR, NOT, variables, and parentheses. The SAT problem is: given the expression, is there some assignment of TRUE and FALSE values to the variables that will make the entire expression true?\n\nSAT3 problem is a special case of SAT problem, where Boolean expression should be divided to clauses,such that every clause contains of three literals.\n\nGiven a boolean formula of the form $(x_1 ∨ x_3 ∨ x_{6}^{not} ) ∧ (x_2 ∨ x_3 ∨ x_7 ) ∧ . . .$ where $∨$ is `and` and $∧$ is `or`. Can you set the variables $x_1, x_2...$ such that the boolean formula results in True (satisfiable).\n\nThis is NP problem beacuse:\n- guess x_1 = T or F\n- guess x_2 = T or F\n\nIf the answer to the SAT3 problem is YES, you can start guessing and then you can check in polynomial time (polynomial verification) if the answer from the guesses is a YES.\n\nNP problems allow you to check the answers in polynomial time.\n\n$NP = $ {decision problems with poly-size certificates and poly-time verifiers for YES outputs}\n\n**NP-complete**\n\n$X$ is NP-complete if $X \\in NP$ \\intersect $NP$-hard.\n\n$X$ is NP-hard if every problem $Y\\in NP$ reduces to $X$. X is NP-hard if it is at least as hard as all NP-problems.\n\n\n![p_np_line.png](assets/images/p_np_line.png)\n\nHow to prove X is NP-complete.\n1. Show $X \\in NP$ (come up with polynomial verification)\n2. Show $X \\in NP-hard$ by reducint from known NP-complete problem from Y to X.\n\n**Super Mario Brothers**\n\nWe show that Super Mario Brothers is NP-hard by giving a reduction from 3SAT.\n\n**Dimensional Matching (3DM)**\n\nDeﬁnition 3. 3DM: Given disjoint sets $X, Y$ , and $Z$, each of $n$ elements and triples $T ⊆ X × Y × Z$ is there a subset $S ⊆ T$ such that each element $∈ X ∪ Y ∪ Z$ is in\nexactly one triplet $s ∈ S$?\n\n3DM is NP. Given a certiﬁcate, which lists a candidate list of triples, a veriﬁer can check that each triple belongs to T and every element of X ∪ Y ∪ Z is in one triple.\n\n3DM is also NP-complete, via a reduction from 3SAT.\n\n\n**Subset Sum**\n\nDeﬁnition 4. Subset Sum Given $n$ integers $A = {a1 , a2 , . . . , an }$ and a target sum $t$, is there a subset $S ⊆ A$ such that\n\n$\\sum S = t$\n\nLectures reduce this problem to 3DM and prove it is NP-complete and NP-weakly hard\n\n\n**Partition**\n\nDeﬁnition 4. Subset Sum Given $n$ integers $A = {a1 , a2 , . . . , an }$ and a target sum $t$, is there a subset $S ⊆ A$ such that\n\n$\\sum S = \\sum A/2$\n\nreduce it to Subset Sum problem (harday direction) and prove it is NP-complete\n\n\n# Lecture 17. Complexity: Approximation Algorithms\n\nConsider optimization problems. Instead of finding the right answer we approximate the optimal answer.\n\nDefinition. An algoithm for a problem of size $n$ has an approximation ratio $\\rho(n)$ if for any input, the algo produces a solution with cost $c$ s.t. $max(C/C_{opt},C/C_{opt} \\leq \\rho(n))$ where $C_{opt}$ is the cost of the optimal algorithm.\n\nWe take the max because the optimization problem can be maximization or minimization.\n\nThis says we are a factor of $\\rho(n)$ from an optimal solutions.\n\nDefinition. An approximation scheme that takes as input \\$eps > 0$ and produces a solution such that $C = (1 + \\eps)C_{opt}$ for any fixed $\\eps$, is a $(1 + \\eps)$-approximation algorithm.\n\n$O(n^{2/\\eps})$ if PTAS is polynomial time approximation scheme.\n\n\n$O(n/\\eps})$ if FPTAS is fuly polynomial time approximation scheme.\n\n\n**Vertex Cover**\n\nFor an undirected graph $G$ find a smallest subset of vertices such that all edges are covered. An edge is covered if one of its endpoints is in the subset of vertices. (NP-complete)\n\n\nHeuristics:\n- pick maximum degree vertex\n- pick random edge $(u,v)$, then remove all incident edges to $u,v$\n\n![vertex_cover_max_degree.png](assets/images/vertex_cover_max_degree.png)\n\n\n- pick random edge $(u,v)$, then remove all incident edges to $u,v$ is a factor of 2 apart from the optimal solution. All edges we pick are disjoint and if the number of edges we pick in the end is A, then the number of vertices is 2A.\n\n**Set Cover**\n![set_cover.png](assets/images/set_cover.png)\n\n\nHeuristic: pick subset which covers the most uncovered elements.\n\nAlgo:\n\nStart by initializing the set $P$ to the empty set. While there are still ele­ments in $X,4 pick the largest set $S_i$ and add $i$ to $P$. Then remove all elements in\n$S_i$ from $X$ and all other subsets $S_j$ . Repeat until there are no more elements in $X$.\n\nClaim: This is a $(ln(n)+1)$-approximation algorithm (where $n = |X|$).\n\n\n**Partition**\n\nGiven a set of elements, split it into two subsets and minimize the max of the sums of the two subsets. This is an NP-Complete problem.\n\nPartition [problem](https://en.wikipedia.org/wiki/Partition_problem)\n\nMultiway partition [problem](https://en.wikipedia.org/wiki/Multiway_number_partitioning#Dynamic_programming_solution)\n\n\nApproximation algo.\n\nPhase 1. For smaller subset of size $m<n$ find optimal solution using brute force $O(2^m)$. You get two subsets $A'$ and $B'$. \n\nPhase 2. For the rest of the elements add greedily one by one.\n\nIf m = $(1/ \\eps$) then this is $(1+\\eps)$-approximation algo.\n\n\n# Lecture 18. Complexity: Fixed-Parameter Algorithms\n\nLast 3 lectures:\n\nPick any two:\n1. solve hard problems\n2. solve them fast (poly-time)\n3. exact solution\n\n**Idea:** Aim for exact algorithm, but confine exponential depedence to a parameter.\n\n**Parameter**: A parameter is a nonnegative integer k(x) where x is the problem\ninput. The parameter is a measure of how tough is the problem you are solving.\n\n**Parameterized Problem:** A parameterized problem is simply the problem plus\nthe parameter or the problem as seen with respect to the parameter.\n\n**Goal:** Algo is polynomial in problem size $n$, exponential in parameter $k$.\n\n**$k-$Vertex Cover Problem**\nGiven a graph $G = (V,E)$ and non-negative integer $k$. Question: is there a vertex cover $S\\in V$ of size not greater than $k$\n\nThis is a decision problem for Vertex Cover and is also NP-hard.\n\nObvious choice of parameter is $k$. (natural parameter)\n\n**Brute force**\n\nTry all $n$ choose $k$ subsets of $k$ vertices., test each for coverage. Running time is $O(EV^{k})$\n\nexponent depends on $k$ this is slow. I cannot say that for any fixed $k$ the algo is quadratic for example.\n\n**Fixed Parameter Tractability**\n\nA parameterized problem is ﬁxed-parameter tractable (FPT) if there is an algorithm\nwith running time $≤ f (k) · n^{O(1)}$ , such that $f : N → N$ (non negative) and $k$ is the parameter, and the $O(1)$ degree of the polynomial is independent of $k$ and $n$.\n\n\nQuestion: Why $f(k) · n^{O(1)}$ and not $f(k) + n^{O(1)}$?\nTheorem: $∃f(k)·n^c$ algorithm iff $∃f'(k) + n^{c'}$\n\n\n$k-$vertex cover problem is FPT:\n\n- pick random edge $e = (u,v)$\n- either $u$ or $v$ or both is in the subset $S$\n- guess each one:\n  - add $u$ to $S$, delete u an all incident edges from G, recurse with $k' = k-1$\n  - do the same but with $v$ instead of $u$\n  - return the $OR$ of the two outcomes\n\nRecursion tree: \n```\n                (n,k)\n      (n-1,k-1)       (n-1,k-1)\n(n-2,k-2) (n-2,k-2) (n-2,k-2) (n-2,k-2)\n```\n\nbase case when $k=0$ return true if no edges, else false. At each node we do $O(n)$ work to delete incident edges. The total runtime is $O(2^k(|V|+|E|))$ \n\n**Kernalization** is a polynomial time algorithm that converts an input $(x, k)$ into a small and equivalent input $(x', k')$. Equivalent means that the answer I get in the end are the same. We want $|x'| \\leq f(k)$. The algo you run on the smaller input would not depend on $n$ anymore, your problem size depends on $f(k)$, hence your running time is O(kernalization) + O(run on smaller input) = $O(n^c) + O(f(k))$.\n\n**Theorem** Aproblem is FPT iff there exists a kernelization.\n\nKernelize -> FPT is trivial.\n![kernalization_proof.png](assets/images/kernalization_proof.png)\n\n![kernel_p1.png](assets/images/kernel_p1.png)\n![kernel_p2.png](assets/images/kernel_p2.png)\n\n# Lecture 19. Synchronous Distributed Algorithms: Symmetry-Breaking. Shortest-Paths Spanning Trees\n\n\nWhat are Distributed Algorithms?\n\nAlgorithms that run on networked processors, or on multiprocessors that share memory.\n\nMost computing is on distributed systems.\n\nDifficulties:\n- concurrent activities\n- uncertainty of timing\n\nYou have a different way of framework for thinking about problems here. You cannot just solve graph problems the usaul way you do, e.g. storing everything in adjacency list or matrix. All nodes are similar and you are allowed to send messages. By sending messages across the network you solve algos.\n\nWe consider two distributed computing models:\n• Synchronous distributed algorithms:\n  - Leader Election\n  - Maximal Independent Set\n  - Breadth-First Spanning Trees\n  - Shortest Paths Trees\n• Asynchronous distributed algorithms:\n  - Breadth-First Spanning Trees\n  - Shortest Paths Trees\n\n**Distributed Network**\n\nBased on an undirected graph $G= (V,E)$ associate:\n- a *process* with each graph vertex \n- two directed *communication channels* with each edge\n\nProcesses at nodes communicating using messages.\n\nEach process has output ports, input ports that connect to communication channels.\n\nSelect a **leader** node in a distributed system.\n\nTheorem 2: Let $G = (V, E)$ be an $n$-vertex clique. Then there is an algorithm consisting of deterministic processes with UIDs that is guaranteed to elect a leader in $G$. The algorithm takes only 1 round and uses only $n^2$ point-to-point messages.\n\n• Algorithm:\n- Everyone sends its UID on all its output ports, and collects UIDs received on all its input ports.\n- The process with the maximum UID elects itself the leader.\n\n**Maximal Independent Set**\n\nProblem: Select a subset $S$ of the nodes, so that they form a Maximal Independent Set.\n\n- Independent: No two neighbors are both in the set.\n- Maximal: We can’t add any more nodes without violating independence.\n\nNeed not be globally maximum, you just need to have local independence. Can have more than one MIS sets.\n\n**Distributed MIS?**\n\nYou have a graph representing the distributed system. The problem of finding an MIS in distributed system is not the same as gather all nodes and edges and run algo. Here each node should know if it is in the MIS or not using messages.\n\nAssume:\n- No UIDs\n- Processes know a good upper bound on $n$.\n\nRequire:\n- Compute an MIS $S$ of the entire network graph.\n- Each process in $S$ should output **in**, others output **out**.\n\n\n**Luby’s MIS Algorithm**\n\n• Executes in 2-round phases.\n\n• Initially all nodes are active.\n\n• At each phase, some active nodes decide to be in, others decide to be out, algorithm continues to the next phase with a smaller graph.\n\n• Repeat until all nodes have decided.\n\n• Behavior of active node $u$ at phase $ph$:\n\n• Round 1:\n- Choose a random value $r$ in $1,2, … , n^5$ , send it to all neighbors.\n- Receive values from all active neighbors.\n- If $r$ is strictly greater than all received values, then join the MIS, output in.\n\n• Round 2:\n– If you joined the MIS, announce it in messages to all (active) neighbors.\n– If you receive such an announcement, decide not to join the MIS, output out.\n– If you decided one way or the other at this phase, become inactive.\n\n\n**Termination**\n\n• With probability 1, Luby’s MIS algorithm eventually terminates.\n\n• **Theorem 7**: With probability at least $1 - 1/n$, all nodes decide within $4logn$ phases.\n\n• Proof uses a lemma similar to before: \n• Lemma 8: With probability at least $1 - 1/n^2$ , in each phase 1, ... , $4logn$ , all nodes choose different random values.\n\n• So we can essentially pretend that, in each phase, all the random numbers chosen are differet.\n\n• Key idea: Show the graph gets sufficiently “smaller” in each phase.\n\n• Lemma 9: For each phase ph, the expected number of edges that\nare live (connect two active nodes) at the end of the phase is at\nmost half the number that were live at the beginning of the phase.\n\n**Formal proof with probability bounds and expectation in slides. Animation of Luby algo in slides.**\n\n**Breath-First Spanning Trees**\n\nThat's the tree you get from BFS.\n\n• New problem, new setting.\n\n• Assume graph G = (V, E) is connected.\n\n• V includes a distinguished vertex $v_0$ , which will be theorigin (root) of the BFS tree.\n\n• Generally, processes have no knowledge about the graph.\n\n• Processes have UIDs.\n- Each process knows its own UID.\n- $i_0$ is the UID of the root $v_0$ .\n- Process with UID io knows it is located at the root.\n\n• We may assume (WLOG) that processes know the UIDs of their neighbors, and know which input and output ports are connected to each neighbor.\n\n• Algorithms will be deterministic (or nondeterministic), but not randomized.\n\n**Output**: Each process $i != i_0$ should output parent $j$, meaning that $j$’s vertex is the parent of $i$’s vertex in the BFS tree.\n\nVery similar strategy to standard BFS just need to add te sending and hearing messages part.\n\n\n**Termination**\n\n• Q: How can processes learn when the BFS tree is completed?\n\n• If they knew an upper bound on diam, then they could simply wait until that number of rounds have passed.\n\n• Q: What if they don’t know anything about the graph?\n\nWhen a subtree finishes propagate upwards that you are done, this starts from the leaves and goes upward. Need to send information up on the tree.\n\nNeed to send info upwards the tree.\n\n# Lecture 20. Asynchronous Distributed Algorithms: Shortest-Paths Spanning Trees\n\n\n**Synchronous Network Model**\n\n- Processes at graph vertices, communicate using messages.\n- Each process has output ports, input ports that connect to communication channels.\n- Algorithm executes in synchronous rounds.\n- In each round:\n  - Each process sends messages on its ports.\n  - Each message gets put into the channel, delivered to the process at the other end.\n  - Each process computes a new state based on the arriving messages.\n\nThe way of thinking in distributed systems is fundamentally different. You have a string limitation that every node in the system knows only things about itself and its neighbourhood. It is not aware of the whole system.\n\nEach node of the graph has some process which is not aware of how the graph looks like. Nodes are connected through channels\n\n**Asynchronous Network Model**\n- Complications so far:\n  - Processes act concurrently.\n  - A little nondeterminism.\n- Now things get much worse:\n  - No rounds---process steps and message deliveries happen at arbitrary times, in arbitrary orders.\n  - Processes get out of synch.\n  - Much more nondeterminism.\n\n  Asynchronous networks complexity:\n\n- Message complexity is number of messages sent by all processes during the entire execution\n- Time complexity. Cannot measure rounds like in synchronous networks.\n- A common approach:\n- Assume real-time upper bounds on the time to perform basic steps:\n  - $d$ for a channel to deliver the next message, and\n  - $l$ for a process to perform its next step. (local processing time)\n\nInfer a real-time upper bound for solving the overall problem.\n\n\nNow we have a big machine (queue) where each process is somewhere in the queue. As they execute we push and dequeu from the queue of processes. \n\n**BFS in asynchronous model**\n\nIf you run simply the BFS like in synchronous model you might end up with a tre which is not BFS-output tree and do not have the shortest paths:\n\n![bfs_asynchronous.png](assets/images/bfs_asynchronous.png)\n\n\nMessage complexity:\n\nNumber of messages sent by all processes during the entire execution. $O(E)$\n\nTime complexity:\n\nTime until all processes have chosen their parents. Neglect local processing time.\n$O(diam·d)$\n- Q: Why diam, when some of the paths are longer? To have long paths these processes must run faster than the diameter path.\n\nTo fix it you need  a **relaxation algorithm**, like synchronous Bellman-Ford.\n\nStrategy:\n-  Each process keeps track of the hop distance, changes its parent when it learns of a shorter path, and propagates the improved distances.\n- Eventually stabilizes to a breadth-first spanning tree.\n\n**Shortest Paths**\n\n- Use a relaxation algorithm, once again. (same as synchronous but would be slower)\n- Asynchronous Bellman-Ford.\n- Now, it handles two kinds of corrections:\n  - Because of long, small-weight paths (as in synchronous Bellman-Ford).\n  - Because of asynchrony (as in asynchronous Breadth-First search).\n\n• The combination leads to surprisingly high message and time complexity, much worse than either type of correction alone (exponential).\n\n# Lecture 21. Cryptography: Hash Functions\n\nA hash function $h$ maps arbitrary strings of data to ﬁxed length output. We want it to look random. In practice, hash functions are used for “digesting” large data.\n\n$h: \\{0,1\\}* -> {0,1}^{d}$, the stars says that we can have arbitrary long string of bits.\n\nNo secret keys, all is public. Anyone can compute $h(x)$.\n\nGoal: Make $h$ run fast (polytime) and make it hard to have collisions. Simple stuff with mod is $O(1)$ but is easy to find colissions.\n\nWe want to approximate/create a **random oracle** $h$ with these properties:\n- For input $x \\in \\{0,1\\}*$, if $h$ has not seen $x$ before, then it outputs a random value $h(x)$, Else $h(x)$ returns its previously output.\n- the random oracle gives a random value (get it by throw a coin 256 times (SHA-256)), and gives deterministic answers to all inputs it has seen before.\n\n**Desirable Properties**\n\n1. One-way (pre-image resistance): Given $y ∈ \\{0, 1\\}^d$ , it is hard to ﬁnd an x such that $h(x) = y$.\n2. Strong **collision-resistance**: It is hard to ﬁnd any pair of inputs $x, x'$ such that $h(x) = h(x')$.\n3. Weak **collision-resistance** (target collision resistance, 2nd pre-image resistance): Given $x$, it is hard to ﬁnd $x'$ such that $h(x) = h(x')$.\n4. **Pseudo-random**: The function behaves indistinguishable from a random oracle.\n5. Non-malleability: Given $h(x)$, it is hard to generate $h(f(x))$ (without using $h$)for any function $f$.\n\n# Lecture 22. Cryptography: Encryption\n\n**Symmetric key encryption**\n\n- secret key $k$ assume is 128 bit. It is shared between Alice and Bob.\n\n$c = e_k(m)$\n\n$m = d_k(c)$\n\n$c$ stands for ciphertext, $m$ is message (plaintext), $e$ is the encryption function, $d$ is descryption function.\n\n$e$ and $d$ need to be reversible operations (plus, minus, permutation, shift left, right)\n\n**Key exchange**\n\nHow does secret key $k$ get exchanged/shared?\n\nPuzzle: Alice sends to Bob a box with money, but need to be locked otherwise it would get stolen. \n\nSolution: A locks the box sends to Bob. Bob locks the box and sends to Alice. Alice unlocks her lock and sends back to Bob.\n\nMath assumption: **Communitivy** tbetween the lockers. Locking a box within a Box would not work. \n\n**Diﬃe-Hellman Key Exchange**\n\n![key_exchange.png](assets/images/key_exchange.png)\n\n$g^a$ is Alice's locker that hides $a$, $g^b$ is Bob's locker that hides $b$. The goal is Alice and Bob share secretly the key $k$. Middle man Mal might put their own locker $g^c$, thus we need asymmetric encryptions, Alice needs a way to authenticate she is communicationg with Bob. Authenticate using public/private key enctyption\n\n**Public Key encryption**\n\nBob comes up with public and private keys. Alice sends to Bob cipher text.\n\nmessage + public key = ciphertext\nciphertext + private key = message\n\nThe two keys need to be linked in a mathematical way. Knowing the public key should tell you nothing about the private key.\n \n**RSA** encryption algorithm.\n\n- Alice picks two large secret primes $p$ and $q$\n- Alice computes $N = p * q$\n- Alice chooses an encryption exponent $e$ s.t. $gcd(e,(p-1)(q-1)) = 1$\n- Alice public key is a tuple $= (N,e)$\n- Dectyption exponent obtained using Extended Euclidean Algorithm by Alice (secretly) such that $e*d = 1 mod(p-1)(q-1)$\n- Alice private key $= (d,p,q)$\n\n**Encryption and Decrypton with RSA**\n\n- encryption $c = m^e mod N$\n- descryption $m = c^d mod N$\n\nTo prove it works we need to show that $m^{ed} = m mod N$\n\n\nHardness of RSA\n\n• Factoring: given $N$ , hard to factor into $p, q$.\n\n• RSA Problem: given $e$ such that gcd $(e, (p − 1)(q − 1)) = 1$ and $c$, find $m$ such\nthat $m^e ≡ c$ mod $N$ . Breaking a particular encryption $c$. \n\n\nThe factoring problem: $N = pq$ Given $N$ find $p$ and $q$, is unknown if is $NP$-complete. Weird.\n\nOther problems $k-colorable$ and knapsack are NP-complete and have been used in crypto algos but have been all broken and do not work in practice.\n\nThis is because NP-completeness is telling you stuff about the worse case. For cryptography we want probblems to have hard problems on **average**.\n\n# Lecture 23. Cache-Oblivious Algorithms: Medians & Matrices\n\nIn all algos so far we assumed accessing different data cost the same.\n\n**Modern Memory Hierarchy**\n\n![memory_hierarchy.png](assets/images/memory_hierarchy.png)\n\nThe closer to the CPU the faster you can transfer data.\n\nTwo notion of speed:\n- latency (how quickly you can fetch data)\n- bandwidth (how fat are the pipes between different hierarchies)\n\nlonger latency due to lnger distance data need to travel\n\nA common technique to mitigate latency is **blocking**: when fetching a word of data, get the entire block containing it.\n\nAmortize latency over a whole block. \n\nCost for getting one data point over a block:\n\nlatency/block size + 1/bandwidth\n\nBlock should have useful elements. We require the algorithm to use all words in a block (*spatial locality*) and reuse blocks in cache (*temporal locality*).\n\n\n**Exteranal Memory Model**\n\n![external_model.png](assets/images/external_model.png)\n\nAssume:\n- cache access is free\n- disk is huge (infinitely large)\n\nThis model handles blocks **explicitly**.\n\nWe count number of memory transfers between cache and the disk.\n\n**Cache-oblivious Model**\n\nThe only change from the external memory model is that the algorithm no longer knows $B$ and $M$ . Accessing a word in the memory automatically fetches an entire block into the cache, and evicts the least recently used (LRU) block from the cache if the cache is full.\n\nDifferent computers might have different $B$ and $M$.\n\n\n**External and Cache-oblivious models give you a way to measure the data transfer complexity of algorithms.**\n\n\n**Scanning**\n```\nfor i in range(N):\n  sum += A[i]\n```\nAssume $A$ is stored contiguously in memory. Arrays are stored in contiguous parts of the memory, dictionaries not. This is why **DICTIONARIES READ AND WRITE** is much **SLOWER** \n\nExternal memory model can align $A$ with a block boundary, so it needs $N/B$ memory transfers.\n\nCache -oblivious algorithms cannot control alignment (because it does not know B), so in needs $N/B + 1$ memory transfers. $N$ cound be 2 and these two elements could be at a boundary, so you would need to access 2  blocks of memory.\n\n**Divide and Conquer**\n\nBase case:\n- problem fits in cache $\\leq M$\n- problem fits in $O(1)$ blocks, $MT(O(B)) = O(1)$\n\n\n**Median Finding**\n\n1. view array as partitioned into columns of 5 (each column is $O(1)$ size).\n2. sort each column\n3. recursively find the median of the column medians\n4. partition array by x\n5. recurse on one side\n\nWe will now analyze the number of memory transfers in each step. Let $MT(N)$ be the total number of memory transfers.\n\n1. free\n2. a scan, $O(N/B + 1)$\n3. $MT(N/5)$, this involves a pre-processing step that coallesces the $N/5$ elements\nin a consecutive array\n4. 3 parallel scans, $O(N/B + 1)$, read all elements, check those less than $x$, check those greater than $x$\n5. $MT(7N/10)$\n\n$MT(N) = MT(N/5) + MT(7N/10) + O(N/B + 1)$\n\n$MT(N) = O(N/B + 1)$\n\n# Lecture 24. Cache-Oblivious Algorithms: Searching & Sorting\n\nWhy LRU block replacement strategy?\n\n$LRU_{M} \\leq 2 OPT_{M/2}$\n\n$M$ is the size of the Cache size $M$\n\n$LRU_M$ is the number of block evictions(i.e. number of block reads)\n\nOPT knows the future and gives the most optimal eviction policy which minimizes the number of evictions.\n\nProof in lecture notes.\n\n\n**Final notes on other classes.**\n\n\n- 6.047 - Computational Biology\n- 6.854 - Advanced Algorithms (natural class to take after this one)\n- 6.850 - Computational Geometry\n- 6.849 - Folding algorithms\n- 6.851 - Advanced Datastructure\n- 6.852 - Distributed Algorithms\n- 6.853 - Algorithms on game theory\n- 6.855 - Network optimization\n- 6.856 - Randomized algorithms\n- 6.857 - Applied cryptography\n- 6.875 - Theoretical cryptography\n- 6.816 - Multicore programming\n- 6.045 - Theory of computation classes (NP complete stuff, complexity)\n- 6.840 - Theory of computation grad class\n- 6.841 - Advanced complexity theory\n- 6.842 - Randomized complexity theory\n- 6.845 - Quantom complexity thery\n- 6.440 - Coding theory","n":0.01}}},{"i":167,"$":{"0":{"v":"LLMs","n":1},"1":{"v":"\nHere I put all my thoughts, my resarch, things I've learned and read about LLMs.\n\n# Resources\n- [Dario Modei Blog](https://darioamodei.com/)\n- \n\n# Models\nLLM models so far are decrtibed as reasoning and models that are good for creative tasks and agentic planning.\n\n- DeepSeek-R1 chinese AI lab DeepSeek reasoning model\n- \n\n# OpenAI\n- gpt models, flagship models for OpenAI\n- o-series reasoning modeils\n\n#TODO https://platform.openai.com/docs/models#models-overview\n\n\n\n# DeepSeek\n\n## R1\n\n- [R1 HuggingFace model](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n\nreasoning model with 671 billion parameters. Under MIT License.\n\n#TODO https://huggingface.co/deepseek-ai/DeepSeek-R1\n\n# Prompt engineering\n\n#TODO https://platform.openai.com/docs/guides/prompt-engineering\n\n# Dario Amodei\n\n[https://darioamodei.com](https://darioamodei.com)\n\n# Three Dynamics of AI Development\n\n1. Scaling laws\n- $1M model might solve 20% of important coding tasks\n- $10M model might solve 40%\n- $100M model might solve 60% and so on\n\nAnother factor of 10 might be the differene between an undergraduate and PhD skill level.\n\n2. Shifting the curve\n- improvement on model architecture (tweaks ontransforers that the the underlying of today's models)\n- engineering improvements, finding a way to run the model more efficiently on the underlying hardware\n- CM = \"compute multiplier\". Frontier AI companies are able to find those compute multiplers\n\n3. Shifting the paradigm\n- From 2020-2023 the main thing being scaled was pretrained models\n- In 2024 the idea of using reinforecement learning to train models to generate chains of thought has become the new focus of scaling\n- new paradigm involves starting with the ordinary type of pretrained models, and then as a second stage using RL to add the reasoning skills\n- as of 2025 we are at a unique \"crossover point\" where thre is a powerful new paradigm that is early on the scaling curve and threfore can make big gains quickly.\n\n\n- [read section after DeepSeek's Models](https://darioamodei.com/on-deepseek-and-export-controls)\n\n# Industry impact\n\nThe \"developer loop\" might change substantially.  I.e., today if you're doing a large task you might do something like:\n1. Work with your PM to figure out what they want\n2. Write some code\n3. Iterate with PM that you've built the thing they want\n4. Iterate on a WIP PR with your technical stakeholders\n5. Write some tests\n6. Do more iteration on PR\n\nWhere in the future/soon/now it might be more like\n1. Work with PM (and maybe the AI) to figure out what they want\n2. Work with AI and your technical stakeholders to translate that PM plan to create a technical plan\n3. Have AI implement a large fraction of the technical plan via chaos coding\n4. Do some cleanup on the PR\n\n# P(doom)\n\nmedian 5% , 14.4% average from AI CEOs and researchers responses.\n","n":0.051}}},{"i":168,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n# Resources:\n- [Learn Git Branching](https://learngitbranching.js.org/).\n- [Git reference common commands](https://git.github.io/git-reference/creating/#init)\n\n\n# Commits and branches\n\nA **commit** in a git repository records a snapshot of all the (tracked) files in your directory. It's like a giant copy and paste, but even better!\n\n**commit = snapshot of the project = lightweight safe of project state** \n\nCommits can be visually represented as nodes.\n\n- create a commit\n```s\ngit commit -m 'commit message'\n```\n\n**Branches = pointer to a specific commit**\n\n*'branch early, and branch often'*\n\nBranches are super lightweight and have no storage/memory overhead\n\nBranches are pointers. To get yourself to a branch you need to *checkout* there\n\n- create a new branch\n```s\ngit branch new_branch\n```\n\n- go to the branch\n```s\ngit checkout new_branch\n```\n\n- shortcut\n```s\ngit checkout -b new_branch\n```\n\n# Merge\n\n**Merge** = combining work from two different branches together.\n\nMerging in Git creates a special commit that has two unique parents.\n\n![before_merge.png](assets/images/before_merge.png)\n\n\n![after_merge.png](assets/images/after_merge.png)\n\nAbove we merge bugFix into main and main contains all the work.\n\n- merge\n```s\ngit checkout -b bugFix\ngit commit -m 'Blah'\ngit checkout main\ngit merge bugFix\n```\n\n# Rebase\n\nThe second way of combining work between branches is **rebasing**. Rebasing essentially takes a set of commits, \"copies\" them, and plops them down somewhere else.\n\nRebasing makes a nice linear sqauce of commits. Commit log of the repo will be cleaner.\n\n![before_rebase.png](assets/images/before_rebase.png)\n\n\n![after_rebase.png](assets/images/after_rebase.png)\n\nHere we have two branches yet again; note that the bugFix branch is currently selected (note the asterisk)\n\nWe would like to move our work from bugFix directly onto the work from main. That way it would look like these two features were developed sequentially, when in reality they were developed in parallel.\n\n\nC3 still exists, and the rebase creates a copy C3'\n- **rebase**\n```s\ngit checkout -b bugFix\ngit commit -m 'Blah'\ngit checkout main\ngit commit\ngit checkout bugFix\ngit rebase main\n```\n\n\n- interactive rebase (can squash commits)\n```s\ngit rebase -i main\n```\n\n\n# HEAD\n\n**HEAD** is the symbolic name for the currently checked out commit -- it's essentially what commit you're working on top of.\n\nHEAD is hiding underneath our work on the repo/branches.\n\nHEAD normally point to the branch you are currently working on\n\n**Detaching** HEAD just means attaching it to a commit instead of a branch.\n\nExample: on main branch with one commit:\n\nHEAD -> main -> C1\n\n`git checkout C1 ` makes HEAD ->  C1\n\n\nTo checkout to a commit you need to use its hash (C1)\n\nTo see commits hashes:\n```s\ngit log\n```\n\n# Moving around git\n\nOPERATORS `^ ~`\n\nSpecifying commits by their hash isn't the most convenient thing ever, which is why Git has relative refs\n\nsaying **main^** is equivalent to \"the first parent of main\n- go to parent commit\n```s\ngit checkout main^\n```\n\nYou can also reference HEAD as a relative ref\n```s\ngit checkout HEAD^\n```\n- go to previous commit, if current commit is C4, the one below will go to C0\n```s\ngit checkout HEAD~4\n```\n\n**Advanced moving when there is a merge**\n\n![move_up_adv_0.png](assets/images/move_up_adv_0.png)\n![move_up_adv.png](assets/images/move_up_adv.png)\n\n\n# Branch forcing\n\nOne of the most common ways I use relative refs is to move branches around. You can directly r**eassign a branch to a commit** with the -f option\n\n```s\ngit branch -f main HEAD~3\n```\n\n![before_force.png](assets/images/before_force.png)\n\n![after_force.png](assets/images/after_force.png)\n\n\n# Reversing changes in Git\n\n`git reset` reverses changes by moving a branch reference backwards in time to an older commit. In this sense you can think of it as \"rewriting history;\" `git reset`will move a branch backwards as if the commit had never been made in the first place.\n\n\n![before_reset.png](assets/images/before_reset.png)\n\n![after_reset.png](assets/images/after_reset.png)\n\n```s\ngit reset HEAD~1\n```\n\nreset does not work for remote branches. Need to use\n\n```s\ngit revert HEAD\n```\n\n\n# Moving Work Around\n\nYou know how to move around the source tree using reference operators `^ ~`.\n\nThe next concept we're going to cover is \"moving work around\" -- in other words, it's a way for developers to say \"I want this work here and that work there\"\n\n- ** pick**\n```s\ngit cherry-pick <Commit1><Commit2>\n```\n\nsays that you would like to copy a series of commits below your current location (HEAD).\n\n- **interactive rebase**\n\n```s\ngit rebase -i branch_name\n```\n\nYour current HEAD would copy from branch_name and do a rebase.\n\nIn interactive rebase you can **reorder, omit, squash, change commit messages**.\n\nalternatively using references:\n\n```s\ngit rebase -i HEAD~3\n```\n\nJuggle commits: Say you want to make changes to an older commit but still keep the order. \n\nTrick: Interactive rebase, then reorder, then make changes, reorder again.   \n\n\n# Git Tags\n\nBranches are easily mutated, often temporary, and always changing. You can move them around and refer to different commits. We use **tags** to permanently mark historical points in your project's history - major releases and big merges.\n\nTags never move, even if you create new commits.\n\n```s\ngit tag v1 commit_hash\n```\n\ntags serve as an anchor point:\n\n```s\ngit checkout v1\n```\n\n**git describe**\n\nBecause tags serve as such great \"anchors\" in the codebase, git has a command to describe where you are relative to the closest \"anchor\" (aka tag). And that command is called git describe\n\n```s\ngit describe <ref>\n```\n\nWhere `<ref>` is anything git can resolve into a commit. If you don't specify a ref, git just uses where you're checked out right now (`HEAD`).\n\nThe output of the command looks like:\n\n`<tag>_<numCommits>_g<hash>`\n\nWhere `tag` is the closest ancestor tag in history, `numCommits` is how many commits away that tag is, and `<hash>` is the hash of the commit being described.\n\n# Git Remotes\n\nGit remotes are actually just copies of your repository on another computer. You can typically talk to this other computer through the Internet, which allows you to transfer commits back and forth.\n- remotes serve as a great backup\n- remotes make coding social\n\n- create local copy of remote repositories\n```\ngit clone\n```\non your local you iwll have branches: `main` and `origin/main` (the remote branch)\n\nRemote branches reflect the state of remote repositories\n\nRemote branches have the special property that when you check them out, you are put into detached `HEAD` mode. **Git does this on purpose because you can't work on these branches directly.**\n\n\n```\ngit fetch\n```\ngit fetch performs two main steps, and two main steps:\n- downloads the commits that the remote has but are missing from our local\n- updates where our remote branches point\n\nNB: fetch does **not** update branches\n\nTo reflect those changes  -- once you have new commits available locally you can do git merge `o/main`\n\n\nThese two steps are incorporated in\n```\ngit pull\n```\n\n\n# Collaborative development models\n\n[Shared repo model vs Fork and pull model](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/getting-started/about-collaborative-development-models)\n\n\nIn a shared repository model, developers work on a single repository. All contributors have write access and crete Pull Requests to merge into main. It is best when you work in smaller teams and private repo.\n\nIn fork and pull model, developers for the main repository and can independently work on their fork. They can create branches, merge into the forked main and work independently. If they want to make changes to the upstream repo they will need to create a Pull Request. Only the maintainers of the repo have write access.","n":0.031}}},{"i":169,"$":{"0":{"v":"Concepts","n":1},"1":{"v":"\n- [Python Language Reference](https://docs.python.org/3/reference/datamodel.html)\n- [Fluent Python Book](https://elmoukrie.com/wp-content/uploads/2022/05/luciano-ramalho-fluent-python_-clear-concise-and-effective-programming-oreilly-media-2022.pdf)\n\nIn this document, I write about all the programming concepts I've learned through reading the Fluent Python book, reading documentation and practice. I discuss tips and trips that I found useful in my work with Python.","n":0.154}}},{"i":170,"$":{"0":{"v":"Python Data Model","n":0.577},"1":{"v":"\n# Python Data Model\n\n- [Data Model from Python Language Reference](https://docs.python.org/3/reference/datamodel.html)\n\n\nEvery object has identity (address of the place in memory), type and value. Only value can be changed (for mutable objects). Immutable objects cannot have their value changed.\n\n`is` operator compares the identity of two objects `==` compares their value.\n\n`id()` gives an integer representing the identity","n":0.136}}},{"i":171,"$":{"0":{"v":"Private Methods","n":0.707},"1":{"v":"# Private methods\nPrivate methods are those methods which can't be accessed in other class except the class in which they are declared.\n\n- One underscore is to show it is a internal function: `_function_name()`\n- Two underscores: the idea is to use double __ to fully disclose methods as private so python will convert them form __private_method to _Myclass__my_private_method and will not let a child class overwrite this method. ","n":0.122}}},{"i":172,"$":{"0":{"v":"Network and Internet","n":0.577},"1":{"v":"\n \n\n- curl\nThe command-line tool to transfer data to or from a server using various internet protocols (most commonly HTTP/HTTPS). **It's the \"Swiss Army knife\" for URLs from the command line.**\n- `GET`, `POST`, `PUT`, `DELETE` are the most common HTTP methods.\n- HTTP protocol is how web browsers and servers communicate.\n- web browsers are clients that request web pages from servers.\n \n\n # REST\n- get/request from server\n\n # WebSocket\n - streaming","n":0.12}}},{"i":173,"$":{"0":{"v":"Memory Management","n":0.707},"1":{"v":"\n# Memory Management\n```python\ndel df # does not release memory to the OS\ngc.collect() # still does not release memory\n```\n- Python objects have high-water mark. It is expensive to pull memory from OS so Python interpreter reserves it for future use so in `htop` it looks like memory is being used.\n- So in theory when you continue working in the python process you should be able to reclaim this memory with other python objects.\n- Yes but unfortunately mostly no... In practice data is fragmented and is not usable unless you close the process (close the interactive terminal)\n- Often you will have a *data leak or fragmentation*, meaning the objects are still in memory but not accessible to you. They are scattered around and not usable.\n\n**Hacks**\n```python\nimport os\nimport psutil\n\ndef usage():\n    process = psutil.Process(os.getpid())\n    return process.memory_info()[0] / float(2**20)\n\n\ndef huge_intermediate_calc(something):\n    ...\n    huge_df = pd.DataFrame(...)\n    ...\n    return some_aggregate\n\nimport multiprocessing\n\nresult = multiprocessing.Pool(1).map(huge_intermediate_calc, [something_])[0]\n\n\nwith multiprocessing.Pool(1) as pool: \n    result = pool.map(huge_intermediate_calc, [something])\n\n# However in a ipython environment (like jupyter notebook) I found that you need to .close() and .join() or .terminate() the pool to get rid of the spawned process.\n\n# Tested example:\ndef func():\n    df = pd.read_parquet(TRAIN_DATA_PATH)\n    return df\n\nimport concurrent.futures\n# max_workers = number of CPUs (threads on your machine, e.g 2 threads per core with8 cores is 16)\nprint(\"Number of logical CPUs:\", os.cpu_count())\nwith concurrent.futures.ProcessPoolExecutor(max_workers=1) as executor:\n    start_usage = usage()\n    print('Start: ', start_usage)\n    result = executor.submit(func,).result()\n    print('End: ', usage())\n    # close process\n    executor.shutdown(wait=True)\nprint(usage())\n\n```\n\n\"Then the function is executed at a different process. When that process completes, the OS retakes all the resources it used. Python, pandas, the garbage collector\"\n- no one can do anything to stop that.\n\n**Some notes and definitions of the aboe tricks**\n- Each process is a separate Python interpreter on your local machine (not remote machines).\n- ProcessPoolExecutor will not work in the interactive interpreter [docs](https://docs.python.org/3/library/concurrent.futures.html)\n\n# Tracing Python Memory\n\n#TODO\n\n- python library to trace memory allocations [tracemalloc](https://docs.python.org/3/library/tracemalloc.html)\n\n\n# del and Garbage Collection\n\n\"Objects are never explicitly destroyed; however when they become unreachable hey may be garbage-collected.\"\n\n```python\n1 == (1) # returns true\nx and (x) usually mean the same thing\ndel x # this is a statement not a function\ndel(x) # would do the same thing\n\n\n(1,) # tuple\n```\n\n- `del` deletes the reference, not the object\n```python\nl1 = [1,2] # l1 reference to the object [1,2]\nl2 = l1 # l2 reference to the object\ndel l1 # deletes reference l1\nprint(l2) # [1,2]\n```\n\n```python\n# example of a life of an object\nimport weakref\ndef bye():\n    print('... goodbye my lover')\nender = weakref.finalize(s1, bye)\nender.alive # True\ndel s1\nender.alive # True\ns2 = '' # ... goodbye my lover\nender.alive # False\n```\n\nObjects may be deleted by the garbage collector once they become unreachable! In CPython When the reference count of an object reaches zero, the garbage collector disposes of it. \n\n# == vs `is`\n\n- `==` compares values, `is` compares if it referencing to the same object\n\n```python\nl1 = [1,2]\nl2 = l1[::] # make copy\nl2 == l1 # true\nl1 is l2 # false\n\nl3 = [1,2]\n\nl3 == l2 # true\n```\n\n\n**Garbage Collector**\n\nreference-counting - when it becomes zero it is unreachable and collected.\n\nCPython implementation detail: CPython currently uses a reference-counting scheme with (optional) delayed detection of cyclically linked garbage, which collects most objects as soon as they become unreachable,\nbut is not guaranteed to collect garbage containing circular references.\n\n\n- simple assignment does not create copies - references to the same object\n- function parameters are passed as aliases, which means the function may change any mutable object received as an argument. Need to make local copy to prevent this.\n- Using mutable objects as default values for function parameters is dangerous because if parameters are changed in-place, the default value is changed!","n":0.041}}},{"i":174,"$":{"0":{"v":"Lazy Compute","n":0.707},"1":{"v":"\n\n# Polars lazy API\n\nWrite query plan first and run only when needed, i.e. collect()\n\n\nthe lazy API:\n- has query optimizations like in (SQL). You tell it what to do not how to do it so it arranges the queries is the most optimal way\n- allows to work with larger than memory datasets using streaming\n- [list of optimizations](https://docs.pola.rs/user-guide/lazy/optimizations/) - all is related to optimal query planning\n- schema plays important role. The lazy API does type checking before running all expensive queries!\n- This Polars query optimizer MUST be able to infer the schema at every step of the query plan (hence .pivot() operation is not available - creates columns from values coming in one column). The optimizer does not know in advance these column names\n- visualize optimizations using `.show_graph()` read from bottom to top. sigma is (filtering rows), pi is projection (filtering columns) -> here you will see how polars does predicate pushdown and projection pushdown\n- Remember that LazyFrames are query plans i.e. a promise on computation and is not guaranteed to cache common subplans.\n- sinks - saving data to disk without the need to load the whole dataset in memory. Process data in batches/chunks. I.e. we are streaming the results to storage\n\n**Tricks**\n\n`pl.scan_csv` or `pl.scan_parquet`\n\n- read files larger than memory\n```python\n# With the default collect method Polars processes all of your data as one batch. This means that all the data has to fit into your available memory at the point of peak memory usage in your query.\n# So do: \n.collect(engine='streaming') # to read datasets thar are larger than memory\n```\n\n- Sink\n```python\n# sink = streaming data to storage - saving in batches\nlf = scan_csv(\"my_dataset/*.csv\").filter(pl.all().is_not_null())\nlf.sink_parquet(\n    pl.PartitionMaxSize(\n        \"my_table_{part}.parquet\"\n        max_size=512_000\n    )\n)\n\n# creates\n# my_table_0.parquet\n# my_table_1.parquet\n# ...\n# my_table_n.parquet\n```\n\n- diverging queries (kind of caching..)\n- [Multiplexing queries](https://docs.pola.rs/user-guide/lazy/multiplexing/)! (also group_by does not guarantee order)\n```python\n# Some expensive LazyFrame\nlf: LazyFrame\n\nlf_1 = LazyFrame.select(pl.all().sum())\n\nlf_2 = lf.some_other_computation()\n\npl.collect_all([lf_1, lf_2]) # this will execute lf only once!\n```\n","n":0.057}}},{"i":175,"$":{"0":{"v":"Distributed Computing","n":0.707},"1":{"v":"\n# Spark\n\n**Terminology**\n- node = computer (VM, physical machine)\n- cluster = group of nodes\n- executor = process that runs on a node\n- driver = main process that coordinates the execution of tasks across the cluster\n\n**Abstractions**\n\n- every spark application consists of a **driver** program that runs the user's main function and runs a set of **executor** processes in *parallel* on a cluster\n- RDD = resilient distributed dataset. This is the main *abstraction* that Spark provides. It is a collection of objects that can be ran in parrallel across a cluster\n- Users may ask Sprk to **persis** and RDD in memory\n- RDDs automatically recover from node failures\n- 2nd *abstraction* that Spark provides is **shared variables** that can be used in parallel operations. There are 2 types of shared variables: **broadcast variables** (shared among all nodes) and **accumulators** (sums,counts)\n\n\n**RDD Operations**\n\n- RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. \n- All transformations in Spark are **lazy**\n- By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the **persist (or cache)** method\n- **Shuffle Write** is the amount of data executors had to write to other executors so the other executor could read.\n- **Shuffle Read** is the amount of data executors read that was provided by another executor\n\n\n**Shuffling**\n- Shuffling is the process where data is redistributed across different executors (nodes/machines) in the Spark cluster — usually because it needs to group data differently than it was originally stored.\n- groupBy, distinct, reduceByKey, sortBy reaarange the data so lal records with the same key (join key) end up on the same partition/executor.\n- In ideal cases (no task failures, no data loss, perfect partitioning), **Shuffle Write ≈ Shuffle Read.**\n\n\n# PySpark\n\n## Properties\n\n- To start a spark session you need to pass a SparkConf file in the SparkContext\n\n**Driver**\n\n- spark.driver.memory = 16g Memory for the driver process (the \"main program\")\n- spark.driver.cores = 4 Number of CPU cores for the driver process\n\n**Executor**\n- spark.executor.memory = 28g. Amount of memory for each executor process.\n- spark.executor.cores = 4 Number of CPU cores for each executor. Each core can handle up to 7g data in memory\n- spark.cores.max = 36. total number of cores in the cluster. So you can have 9 executors with 4 cores each\n- spark.executor.instances 0. Static executors to launch. 0 means not set—dynamic allocation will control.\n\n**Executor Dynamic Allocation**\n- spark.dynamicAllocation.enabled = true. Let spark dynamically allocate executors based on workload\n- spark.dynamicAllocation.maxExecutors = 9 Max number of executors Spark will request dynamically.\nspark.dynamicAllocation.executorAllocationRatio\n- spark.dynamicAllocation.executorAllocationRatio = 0.8. Allocate fraction of the estimated required executors. Controls aggressiveness to avoid overloading the cluster.\nSay you have 100 tasks and an executer on averages executes 4 tasks. So we will need 25 tasks. If we set ratio 0.8, spark will allocate 0.8*25=20 executors\n\n**SQL**\n- spark.sql.autoBroadcastJoinThreshold = 134217728 (128MB). If a table is smaller than this, Spark will broadcast it to avoid shuffles in joins.\n- spark.sql.shuffle.partitions = 200. After a shuffle operation (e.g join, groupby) Spark will output the data in chunks (number of output partitions)\n- Too low number, some executors will be overloaded with large partitions, leadint to slow or skewed jobs or even OOM errors\n- Too high number, will lead to scheduling overhead and many small tasks, increasing job time\n- A good value is usually 2–4 × total executor cores in your cluster, but it depends on **data size and job characteristics.**\n\n[Notebook](https://drive.google.com/file/d/1Dz5x9OPOYFs0nczzfeR7QBNY_tbB11v8/view?usp=drive_link)\n\nPySpark is a Python API that allows you to use the power of Apache Spark - fast & scalable big data processing system.\n- you write Python code\n- PySpark lets tat code run on many computers at once (a \"cluster\")\n- analyze, process and transform huge datasets that would not fit on a single computer's memory\n- distributed processing - distribute the data in multiple computers and splits up the work\n- supports Pandas like code + SQL queries\n\n# Optimizations\n\n\n- cache initialized data. Whenever you reuse a dataframe in a for loop \n```python\n \n```\n\n\n\n# PySpark\n\nExample spark configs:\n\n    spark.executor.memory: 28g\n    spark.executor.memoryOverhead: 8g\n    spark.executor.cores: 4\n    spark.executor.instances: 256 # total number of executors, not needed if using dynamic allocation\n    spark.dynamicAllocation.initialExecutors: 64\n    spark.dynamicAllocation.minExecutors: 64\n    spark.dynamicAllocation.maxExecutors: 256\n    spark.driver.memory: 28g # DRIVER!\n\n- spark is a distributed computing framework that allows you to work with large datasets across a cluster of machines/computers\n- **worker node** is typically one physical or virtual computer in the cluster\n- **driver node** is the main process that coordinates the execution of tasks across the cluster. It is responsible for creating the SparkContext, which is the entry point to using Spark.\n- spark executor is a **process** that runs on each worker node in the cluster and is responsible for executing tasks and managing resources. Like a separate Python interpreter that runs on the worker node.\n- node can have multiple executors running on it.\n- each executor has its own memory and CPU resources allocated to it, which are used to execute tasks in **parallel**.\n- **executor.cores**: each executor can have multiple cores (i.e threads) that can execute tasks **concurrently**.\n- **executor instances (spark.executor.instances)**: total number of executors (processes) launched on all worker nodes in the clust\n- If using dynamic allocation, leave spark.executor.instances unset or set min/max via:\n```\nspark.dynamicAllocation.enabled=true\nspark.dynamicAllocation.minExecutors\nspark.dynamicAllocation.maxExecutors\n```\n\n\n**Why to choose dynamic allocation?**\n- Variable Input Size\n- Fluctuating Resource Needs - joins, groupby-s, aggregating and exploding data\n- shared, multi-user cluster : release executors when idle so others can use those resources\n- cost optimization, not paying for idle resources\n- long-running applications, your job scales up and down depending on activity\n\n**When NOT to Use Dynamic Allocation**\n- Your resource needs are steady and predictable (and you’re on a dedicated cluster).\n- Very short jobs or jobs with very short \"bursts\" of high demand (executor startup delays can hurt performance).\n- You depend on RDD caching across all executors (since dynamic allocation can kill executors, losing cached data).\n\nDriver and Workers:\n\n![alt text](./assets/images/spark_driver_worker.png)\n\n\n# Example optimization\n\nHey! After some experiment runs I've updated and chose these spark parameters\n\n        spark.executor.memory: 28g\n        spark.executor.memoryOverhead: 3g\n        spark.executor.cores: 4\n        spark.dynamicAllocation.initialExecutors: 8\n        spark.dynamicAllocation.minExecutors: 8\n        spark.dynamicAllocation.maxExecutors: 32\n        spark.driver.memory: 28g\nRun log\n\nRun time 360 seconds, cost 1.41$ (previously it was 7$)\n\nCPU average usage by the whole Pod is~30-40%. I won't decrease more the amount of executors since 8 to 32 dynamically allocated is already low compared to other jobs in this service.\n\nMemory average usage by the whole Pod is 50-65% - this is ok since we want some room left\n\nI use dynamic number of executors instead of fixed since CAPI has variable size input (as we onboard/churn clients num conversions can change a lot, matches data also depends a lot on the quality of the conversion client send to us which can vary over time). Also there are a few joins/groupbys in the DQS job that require variate amount of resources\n\nSpark recommends memoryOverhead of executor to be about 6-10% of the container size = spark.executor.memory + spark.executor.memoryOverhead. I chose ~10% to be on the safe size\n\n\n# Spark Monitoring\n\n","n":0.03}}},{"i":176,"$":{"0":{"v":"Data Storage","n":0.707},"1":{"v":"When you build a data infrastructure system you will consider tradeoffs between cost, efficiency, pattern accessability (online va offline, key:value pair vs relational), volume.\n\n# Data Storage Technologies\n\nAWS has S3 (e.g DataLake), Redshift, MySQL, Cassandra, ElasticSearch.\n\n\n## S3\nAWS service. Cloud storage. Structured (csv, parquet, csv) and Unstructured (images,videos, documents, parquets) data\n\n- Offline availability\n- Best for logs, historical data, backup - the defacto option for archives\n- Benefits: extremely low $ cost, essentially infinite storage capacity, scales horizontally\n- Disadvantages: no built-in query. Need to load it in some other data store to work with the data\n- S3 is basically a filesystem (buckets instead of folders)\n\nCan query using additional services like Athena, or you load in spark.\n\n## Datalake\nAWS service. Claud storage. Built on top of S3. Structured and Unstructured data. Allows to query data using ATHENA (SQL quering engine for S3)\n\n## Redshift\n\nAWS service. Columnar data storage, fully manged data warehouse service. Relational db design. Structured data only (tables schemas)\n\nStores data in clusters.\n\n- Offline availability\n- Best for analytics, data exploration and analysis\n- Benefits: good options for doing expensive joins. SQL based quering.\n- Disadvantages: high cost\n\n\n## MySQL\n\nRelational database. Used for structured data and transactions, such as user profiles and reviews. Transaction is a row/tuple in sQL table.\n\n- Offline and Online (e.g when Yelp user searches something/ send a message it is directly stored and accessed) availability \n- ACID compliance provides strongest form of data reliability\n\n\n## Casandra\n\n## ElasticSearch\n\n## HDF5\n\nSusquehanna uses this.\n\n- \n\n\n# Data Catalog\n\nThis is a catalog service =  metadata layer that describes data not the data itself.\n\nData catalog technologies offer another layer of abstraction on top of data storage technologies. They help you discover, manage and govern your data assets. \n\n## AWS Glue\nAWS Glue is a centralized metadata catalog that allows:\n- schema management, track schema versions, schema evolution, register schemas.\n- metadata management - catalog of what data exists and where\n- data discovery layer - search and discover datasets, and querable by downstream engines.\n\n\n## Apache Iceberg\n\nApache is a open source foundation project. Iceberg is a table format for huge analytic datasets. It is designed to improve on the limitations of other table formats like Hive.\n\nIceberg is supported in AWS. You can create Iceberg tables. For example in CAPI you had to migrate from normal tables in DataLave v1 to DataLake v2 (these are Yelp related datalakes). DLv2 uses Iceberg table format. It allows you to have:\n- ACID transactions (marketing)\n- Time travel and rollback via its integrated snapshots tooling.\n- Multi-partioned tables (dt=2024-01-01/client_id=sephora). More control over partitioning strategies. When you run a backfill you could select which data to change based on partition filters.\n- No need for physical deletes (use DELETE statements that mark data as deleted without physically removing it right away)\n- tracks schema evolution\n\nMultipartitioning: \n- isolation whenever you insert/update/delete data. Before updating data for one client meant you need to rerun for all.\n- \n\n### Yelp DLv1 to DLv2\n\nSince 2017 Yelp's data lake has used the Hive table format. Since then, Apache Iceberg has gained significant adoption in the industry -- proving to be Hive's successor -- and brings many enhancements.\n\nLike Hive, Iceberg stores Parquet files in S3 and metadata in AWS Glue. Iceberg tables exist in the AWS Glue catalog and work with Spark, Athena, and Redshift just as Hive tables do, but with extra features.\n\n[ICEBERG Tech Spec](https://iceberg.apache.org/spec/#version-1-analytic-data-tables)\n\n\nOne of Iceberg's design goals is to present a data lake table that works as a SQL table does without any surprises, so is not required to know these concepts, however some Iceberg users may be curious to understand them.\n\nIceberg metadata is stored in a tree structure. The image below may look complicated at first glance, but users of Iceberg do not need to understand it in order to use an Iceberg table.\n\n\n\n#### ACID transactions\n\nHive tables have many areas of undefined behavior which introduce data inconsistencies and data quality issues, especially when multiple processes are reading and writing to a table. Iceberg's provides ACID transactions and guarantees about behaviors for reads and writes.\n\nIceberg's tables store the S3 path of the current metadata on the Glue table as a table property. This is the basis for the below behaviors.\n\n- Atomic Operations: Data files are added or removed in a single, complete operation, ensuring that writes are never partially visible. Iceberg writes data to S3 and commits changes by updating a metadata reference in the Glue table, which supports atomic updates.\n\n- Consistency & Isolation: Iceberg provides optimistic concurrency and serializable isolation. Readers only access commited data and will see a consistent and correct view of data even while a writer concurrently commits to the table. Serializable isolation ensures that multiple writers will not introduce inconsistent states by ensuring that only one process writes to a table at a time. In some cases serializable isolation can impede workflows, see Iceberg Known Issues and Limitations for alternatives (parallel backfills can be tricky).\n\n-Durable: Once a transaction is committed, it will not be lost. Durability is provided by S3 similarly to Hive tables. \n\n\n# HDF5\n\nSIG uses this data format (`.hdf5`). Another technology to manage data.\n- HDF5 is a data software library, data management service and storage forhererogeneous data. Built in with fast \"I/O\" processing.\n- storing scientific numerical data\n\nWhat is HDF5?\n- A file specification and associated data model\n- A standard library with API access for C++, Python, Java, etc.\n- A software ecosystem consisting of client applications using HDF5 and analysis platforms like MATLAB, Python\n","n":0.034}}},{"i":177,"$":{"0":{"v":"Concurrency and Parallelism","n":0.577},"1":{"v":"\n\n# Concurrency Models in Python\n\nChapter 19 from \"Fluent Python\"\n\nConcurrency is about dealing with multiple things at once.\n\nParallelism is about doing multiple things at once.\n\nConcurrency is about structure, Parallelism is about execution.\n\nA CPU with 4 cores can run 4 processes in parallel but 100s processes concurrently. \n\n\n\n# Old notes\n\n- concurrent, multithreaded programming, [web crawler](https://leetcode.com/problems/web-crawler-multithreaded/)\n```Python\n# simple DFS\nclass Solution:\n    def crawl(self, start: str, parser: 'HtmlParser') -> List[str]:\n        hostname = lambda x: x.split('/')[2]\n        visited,stack= set([start]),[start]\n        while stack:\n            s = stack.pop()\n            for u in parser.getUrls(s):\n                if u not in visited and hostname(start) == hostname(u):\n                    visited.add(u)\n                    stack.append(u)\n        return visited\n\n# concurrent DFS\nfrom concurrent import futures\nclass Solution:\n    def crawl(self, s: str, parser: 'HtmlParser') -> List[str]:\n        hostname = lambda x: x.split('/')[2]\n        visited = set([s])\n        with futures.ThreadPoolExecutor(max_workers=16) as executor:\n            tasks = [executor.submit(parser.getUrls, s)]\n            while tasks:\n                neigh = tasks.pop().result()\n                for u in neigh:\n                    if u not in visited and hostname(s) == hostname(u):\n                        visited.add(u)\n                        tasks.append(executor.submit(parser.getUrls, u))\n        return visited\n```\n\n# Deadlocks and how to properly acquire release locks\n\n```python\nfrom threading import Lock\n```\n\nCommon examples of the cause of threading deadlocks include:\n\n- A thread that waits on itself (acquires itself twice releases once and waits for itself)\n- Threads that wait on each other (e.g. A waits on B, B waits on A).\n- Thread that fails to release a resource (e.g. mutex lock, semaphore, barrier, condition, event, etc.).\n- Threads that acquire mutex locks in different orders (e.g. fail to perform lock ordering).\n\n\n```python\nfrom threading import Lock\nlock = Lock()\nlock.acquire()\nlock.acquire()\nlock.release() # deadlock\n\n# no deadlock I think\nlock.acquire()\nlock.release() \nlock.acquire()\n\n```\n- [Print Order](https://leetcode.com/problems/print-in-order/)\n\n- [FizzBuzz](https://leetcode.com/problems/fizz-buzz-multithreaded/)\n\nmalko e mazalo...\n\n```python\nfrom threading import Lock\nclass FizzBuzz:\n    def __init__(self, n: int):\n        self.n = n\n        self.finish = False\n        self.fizz_lock = Lock()\n        self.buzz_lock = Lock()\n        self.fizzbuzz_lock = Lock()\n        self.main = Lock()\n        self.fizz_lock.acquire()\n        self.buzz_lock.acquire()\n        self.fizzbuzz_lock.acquire()\n\n    # printFizz() outputs \"fizz\"\n    def fizz(self, printFizz: 'Callable[[], None]') -> None:\n        while True:\n            self.fizz_lock.acquire()\n            if self.finish: return\n            printFizz()\n            self.main.release()\n\n    # printBuzz() outputs \"buzz\"\n    def buzz(self, printBuzz: 'Callable[[], None]') -> None:\n        while True:\n            self.buzz_lock.acquire()\n            if self.finish: return\n            printBuzz()\n            self.main.release()\n\n    # printFizzBuzz() outputs \"fizzbuzz\"\n    def fizzbuzz(self, printFizzBuzz: 'Callable[[], None]') -> None:\n        while True:\n            self.fizzbuzz_lock.acquire()\n            if self.finish: return\n            printFizzBuzz()\n            self.main.release()\n            \n    # printNumber(x) outputs \"x\", where x is an integer.\n    def number(self, printNumber: 'Callable[[int], None]') -> None:\n        for i in range(1,self.n+1):\n            self.main.acquire()\n            if i % 3 == 0 and i % 5 != 0: self.fizz_lock.release()\n            elif i % 3 != 0 and i % 5 == 0: self.buzz_lock.release()\n            elif i % 3 == 0 and i % 5 == 0: self.fizzbuzz_lock.release()\n            else:\n                printNumber(i)\n                self.main.release()\n        \n        self.main.acquire()\n        self.finish = True\n        self.buzz_lock.release()\n        self.fizz_lock.release()\n        self.fizzbuzz_lock.release()\n\n\n```\n\n\nGOAL: Run only one thread at a time!!!!!\n\nRacing conditions are dangerous.\n\n\n# Native Coroutines\n\n\nPython has three ways to run things concurrently\n- coroutines (classic and native)\n- threads\n- processes\n\nNative coroutines use asyncio library using `async def` and `await` syntax\n- `asyncio` is a library that allows the USER to manually create an event loop (with ONE THREAD) and schedule tasks to run concurrently\n- Allows YOU to multitask with functions!\n- asyncio is Python’s **standard library**\n- **Run time of concurrently ran functions = Run time of slowest function**, if you schedule the tasks to run in the background!\n\n## Creating Background Task\n\nsay you have two functions one that takes 10 seconds and one that takes 5 seconds, how can you run them concurrently?\n```python\nimport asyncio\n\nasync def slow():\n    print(\"Starting slow\")\n    await asyncio.sleep(10)\n    print(\"Ended slow\")\n\nasync def calc_that_does_not_wait_slow():\n    await asyncio.sleep(5)\n    print(1)\n\n\nasync def main():\n      await calc_that_does_not_wait_slow()\n      await slow()\nimport time\nprint(start:= time.time())\nawait main()\nprint(end:= time.time())\nprint(end-start)\n\n# takes 15 seconds\n# even if you swap \n#       await calc_that_does_not_wait_slow()\n#       await slow()\n# it will take 15 seconds\n```\n\n**You need to make them one as background task!** Currently you are running code sequentially.\n\n\n```python\nimport asyncio\nasync def slow():\n    print('starting slow')\n    await asyncio.sleep(10)\n    print('ended slow')\n\nasync def calc_that_does_not_wait_slow():\n    await asyncio.sleep(5)\n    print(1)\n\nasync def main():\n    slow_task = asyncio.create_task(slow()) \n    await calc_that_does_not_wait_slow()\n# Outside main:\nprint(start:= time.time())\nawait main()\nprint(end:= time.time())\nprint(end-start)\n# Prints 5 seconds only! And 5 seconds after prints ended slow!\n#  How it runs: \n#\n#      slow() is started in the background (will take 10 seconds).\n#      You immediately start and await calc_that_does_not_wait_slow() (waits 5 seconds, then prints 1).\n#      When calc_that_does_not_wait_slow() is done (after 5 seconds), main() returns—even if slow() is still running!\n#      The program ends; slow() may be cancelled or left unfinished.\n     \n```\n\n**Lets await the task and the calculation now.**\n- await the slow first and then the other -> 15 seconds - sad\n```python\nimport asyncio\n\nasync def slow():\n    print(\"Starting slow\")\n    await asyncio.sleep(10)\n    print(\"Ended slow\")\n\nasync def calc_that_does_not_wait_slow():\n    await asyncio.sleep(5)\n    print(1)\n\n\nasync def main():\n      slow_task = asyncio.create_task(slow()) # background task\n      await slow_task\n      await calc_that_does_not_wait_slow()\nimport time\nprint(start:= time.time())\nawait main()\nprint(end:= time.time())\nprint(end-start)\n\n# Output\n# 1754464873.7352946\n# Starting slow\n# Ended slow\n# 1\n# 1754464888.7525644\n# 15.017269849777222\n```\n\n- await the  slow after and it takes 10 seconds\n```python\nimport asyncio\n\nasync def slow():\n    print(\"Starting slow\")\n    await asyncio.sleep(10)\n    print(\"Ended slow\")\n\nasync def calc_that_does_not_wait_slow():\n    await asyncio.sleep(5)\n    print(1)\n\n\nasync def main():\n      slow_task = asyncio.create_task(slow())\n      await calc_that_does_not_wait_slow()\n      await slow_task\nimport time\nprint(start:= time.time())\nawait main()\nprint(end:= time.time())\nprint(end-start)\n\n# Output \n# 1754464849.4491456\n# Starting slow\n# 1\n# Ended slow\n# 1754464859.4551563\n# 10.00601077079773\n```\n\n## Background tasks are confusing - just use gather!\n\n```python\nimport asyncio\n\nasync def slow():\n    print(\"Starting slow\")\n    await asyncio.sleep(10)\n    print(\"Ended slow\")\n\nasync def calc_that_does_not_wait_slow():\n    print(1)\n\nasync def main():\n    # Both coroutines start at once; don't wait for one to finish before the other\n    await asyncio.gather(slow(), calc_that_does_not_wait_slow())\n\n# will run for 10 seconds as well\n```\n\n### Summary\n\n \n\n**Concurrent async functions:**\n\n- If you want two async functions (coroutines) to run simultaneously, you must schedule them to run at the same time.\n- await-ing one after the other causes them to run sequentially, so the total run time is the sum of both delays.\n     \n\n\n**Background tasks with create_task:**\n\n- Using asyncio.create_task(coro()) starts a coroutine in the background.\n- If you don't later await the task, your program may exit before the background task finishes.\n- If you start the slow task in the background and only await the fast task, the program finishes after the fast task—even if the slow one hadn't finished yet (the slow one is cancelled or left unfinished).\n\n\n","n":0.033}}},{"i":178,"$":{"0":{"v":"Asynchronous Programming","n":0.707},"1":{"v":"```python\nimport asyncio\nimport time\n\nasync def five_sec():\n    \"\"\"\n    Test asyncio.as_completed\n    this takes 5 seconds to complete\n    \"\"\"\n    t1 = asyncio.sleep(5,'5sec')\n    t2 = asyncio.sleep(2,'2sec')\n    for t in asyncio.as_completed([t1,t2]):\n        print(await t)\n\nasync def one_sec():\n    \"\"\"\n    Test asyncio.as_completed\n    this takes 1 second to complete\n    \"\"\"\n    print(await asyncio.sleep(1,'1sec'))\n\n\nasync def six_sec():\n    print(await one_sec())\n    print(await five_sec())\n\nasync def five_sec_and_one_sec():\n    \"\"\"\n    Test asyncio.gather\n    this takes 5 seconds to complete\n    \"\"\"\n    await asyncio.gather(one_sec(), five_sec())\n\ndef create_one_sec_task():\n    \"\"\"\n    Test asyncio.create_task\n    this takes 1 second to complete\n    \"\"\"\n    return asyncio.create_task(one_sec())\n\ndef create_five_sec_task():\n    \"\"\"\n    Test asyncio.create_task\n    this takes 5 seconds to complete\n    \"\"\"\n    return asyncio.create_task(five_sec())\n\nasync def main_five_sec_total():\n    \"\"\"\n    Test asyncio.gather\n    this takes 5 seconds to complete\n    \"\"\"\n    t2 = create_one_sec_task()\n    t1 = create_five_sec_task()\n    await t2\n    await t1\n    \nasync def producer(queue: asyncio.Queue):\n    for i in range(5):\n        print(f\"Producer putting {i}\")\n        await queue.put(i)\n        await asyncio.sleep(0.5)  # simulate work\n\n    await queue.put(None)  # Sentinel to tell the consumer to finish\n\nasync def consumer(queue: asyncio.Queue):\n    while True:\n        item = await queue.get()\n        if item is None:\n            print(\"Consumer got sentinel, exiting.\")\n            break\n        print(f\"Consumer got {item}\")\n        await asyncio.sleep(1)  # simulate processing\n\nasync def main_queue():\n    queue = asyncio.Queue()\n    await asyncio.gather(\n        producer(queue),\n        consumer(queue)\n    )\n\n\nif __name__ == \"__main__\":\n    start = time.time()\n    asyncio.run(main())\n    end = time.time()\n    print(f\"Time taken: {end - start} seconds\")\n\n```","n":0.073}}},{"i":179,"$":{"0":{"v":"C++","n":1},"1":{"v":"\n# My C++ Notes\n\nI solve one leetcode a day in C++ and document all syntax learnings here.\n\n\n\n- Basic data types\n```cpp\n\n```","n":0.224}}},{"i":180,"$":{"0":{"v":"Bash","n":1},"1":{"v":"- search string in filename\n```bash\ngrep string filename\ngrep -e regex filename\n```\n- \n```bash\n```","n":0.302}}},{"i":181,"$":{"0":{"v":"Agile","n":1},"1":{"v":"# Agile Software Development\n\nNotes on MIT Lecture [Youtube](https://www.youtube.com/watch?v=UxMpn92vGXs)\n\n- Definition. Project management is planning organizing, securing, motivate and controlling the resources to successfully complete a project.\n\n\"Fast, good, cheap. Pick two! PM chooses them.\"\n\n## Old Model - Waterfall\n\nWaterfall does each of the stages sequentially:\n1. Gather requirements/spec = define design + determine concept\n- all features, budget, resources, determine constraints to have meaningful design\n2. Pre-production. Given all assumptions in design and concept you test on small case.\n3. Build product, e.g. Alpha/Beta version. Add more testing.\n4. Release/Maintenance.  \n\n**Problem: Testing is all at the end, when the team can't respond to it usefully.**\n\n## Agile\n\n**Agile manifesto:**\n- Individual interactions over processes and tools (talk to someone instead of sending requests over)\n- working software over comprehensive documentation (prototypes > documentation)\n- customer collaboration over contract negotiation\n- responding to change over following a plan\n\nAgile model:\nYou do small iterations on the waterfall stages. Deliver small chunk of results at each iteration: Gather requirements/feedback, pre-production, test, release on each of the features.\n\nSteps of each iteration:\n- You do a little bit of design\n- You do a little bit of implementation\n- Test it does what you expect\n- Review your results, plan the direction for next iteration.\n\nAgile project management helps you spend less time designing features that users do not want and spend more time on features that users do want. Agile does not help you write code faster but helps you focus on writing the code that is important to users.\n\n**Sprint** is the iteration cycle\n\n**Scrum** vision's statement:\n- transparency (everybody in the team should stay informed and does decision)\n- inspection (everything done by the team is inspected)\n- adaptability\n\nScrum is one of the agile methodologies (there are other as well).\nScrum is an agile process most commonly used for product development, especially software development. Scrum is a project management framework that is applicable to any project with aggressive deadlines, complex requirements and a degree of uniqueness. In Scrum, projects move forward via a series of iterations called sprints. Each sprint is typically two to four weeks long.\n\nIn scrum the team has the responsibility to manage the project (no lead).\nScrum puts project management responsibility on the shoulders of the team.\n\nAnatomy of a sprint:\n- meetings: sprint planning, daily scrum (stand ups), sprint review, retrospective\n- artifacts/tools: product backlog (feature list for the whole project), sprint backlog(tasks work in the sprint)\n\nIn every sprint you do a little bit of planning/design/implementation/testing.\n\nAt the start of each sprint the team looks at the product backlog/ holds sprint planning meeting, move from product backlog to sprint backlog, turns them into smaller tasks. Hold daily stand ups talking about your tasks.\n\nAt the end of the sprint you have a sprint review - check your sprint backlog.\nThen have a retrospective where the team improves the way they work together. Ready for a new sprint planning meeting.\n\nScrum teams have 3 type of members:\n- regular team members\n- product owner (creates the product backlog, has a the whole vision of the product)\n- scrum master (runs the meetings, set up meetings and make sure they happen, keep backlog straight, keep communication is good in flow). Does not make decisions, is not the producer. Scrum master is the person who keeps think running smoothly.\n\nProduct backlog is an organized prioritized list of everything that will be needed in the product and is the single source of requirements for any change to be made in the product.\n\nProduct backlog = maintained and prioritized feature list managed by the Product Owner\n\nUser Story = very specific way to describe a feature. Phrase your feature in a clearly testable and understandable manner\n\nUser Story are created by users (to ask for something to get done)\n**As The** (User,Designer, Artist) **I want** (describe something testable here) **So that** (explain reason).\n\nAgile is about making sure you do not spend time on features which users will not use. That is why we have user stories. (explain reason) part in the end is a double checker for the implementer of the feature to be sure that the user indeed needs the feature they are asking for.\n\n\nMaking effective Team meetings:\n- time-boxing (define time you spend on a meeting before the meeting)\n- clear agendas\n- involved participants\n\nall meetings in scrum should be time boxed\n\nSprint planning meetings:\n- create sprint backlog\n- set sprint planning goal and deliverables and time it would take to do it\n\nEnd of sprint. Sprint review meeting\n- show deliverables\n- review and evaluate the product\n- change and update product backlog\n- all about reviewing the product\n\nRetrospective\n- all about reviewing the process\n- generate shortlist of potential improvements in the future\n- forward looking meeting, see what we did in the past to improve the future\n\nDaily Scrum (Stand up)\n- keep team accountable to each other\n- combined with the scrum board (board in the jira)\n- what did you do yesterday, what are you going to do today, what blocks you.\n\n\n## Buzzwords\nAgile software development, Scrum, Project management, Waterfall, Agile manifesto, Sprint, Scrum Master, Product Owner, Scrum Board (Trello), Sprint planning, Sprint, Retrospective, Sprint Backlog, Product Backlog, User Story\n\n\n\n## What is Agile\n\n[Youtube video](https://www.youtube.com/watch?v=Z9QbYZh1YXY) notes\n\nAgile is a set of **Values and Principles** = collections of believes of how to develop software.\n\nAgile gives a foundation of how to develop software and does not make concrete points to follow. Main message of Agile is in its MANIFESTO.\nAgile principles does not tell you what to do but give you only general direction.\n\nAgile principles are used to create *agile practices* which are team-specific. Agile practices are defined by agile frameworks\nsuch as *scrum, kanban, lean, extreme programming XP*\n\n## Atlassian Agile coach\n[lilnk](https://www.atlassian.com/agile)\n\nTeams choose agile so they can respond to changes in the marketplace or feedback from customers quickly without derailing a year's worth of plans. \"Just enough\" planning and shipping in small, frequent increments lets your team gather feedback on each change and integrate it into future plans at minimal cost.\n\nAgile frameworks have emerged such as scrum, kanban, lean, and Extreme Programming (XP). Each embodies the core principles of frequent iteration, continuous learning, and high quality in its own way.\n\nThe way each team practices agile should be unique to their needs and culture.\nThe key to doing agile right is embracing a mindset of continuous improvement. Experiment with different practices and have open, honest discussions about them with your team. Keep the ones that work, and throw out the ones that don't.\n\n\nAgile Methodology, as the name suggests, is a set of methods and practices where software development and project management take place to deliver customer-centric products in a short development cycle known as sprints. It is an iterative approach and each iteration is specially designed to be small and manageable so that it can be delivered in a specific given period of time. Agile methodologies are open to changing requirements over time and encourage constant feedback from end-users. It is the most popular approach because, in this process, customers are also involved so that they can get updates regarding their product and also make sure whether or not they are meeting their requirements.","n":0.03}}},{"i":182,"$":{"0":{"v":"AWS","n":1},"1":{"v":"\n\n\nAWS Certified Practitioner Course\n------------------------------------------------------------------------------------\n\nHow do websistes work?\nClient (your computer)- ---- Network ------- Server (data base system)\n\nNetwork = routes packets(data from the server) \n\nClients and Servers have IP address to find each other\n\nServer send to Client (for client perspestive they get response from server)\n\nWhat is a server? Server contains:\n- CPU (computetion), \n- RAM (temporary memory), \n- Storage data (permanent memory),\n- Database (structured data)\n- Network (routers, switch, DNS server)\n\nRouter forwads pachets between computers in the network\nSwitch knows where to send these packets in the server.\n\nThe cloud gives you all these things above ON DEMAND (scalable).\nNo need for big servers in data centers and on prem.\n\n3. What is Cloud Computing?\n\nCloud Computing is the on-demand delivery of servers which do the things in prev note.\n\nAllows you to choose the right TYPE and SIZE of computing you need. \nAccess to these resources Instantly.\n\nAWS owns and maintains the network-connected HARDWARE. You use only the web application.\n\nCloud Types/Models\n- private cloud (cloud service used by a single organization)\n- public cloud (service delivered over the Internet)\n- hybrid cloud\n\nProblems solved by the cloud:\n- time flexibility (instant resource access)\n- cost-effective (pay as you go)\n- scalability (no need buy servers and maintain them)\n- elasticity (scale out and scale in when needed)\n- availability and fault-tolerance (build across indepenedent data centers)\n- agility - rapidly develop, test and launch software\n\nManaging servers see page 4 picture!:\n\nOn prem - care about EVERYTHING\n\nCloud computing service types:\n- Infrastructure as a Service (IaaS) - - care about OS, Runtime,Middleware, Data, Application\n- Platform as a Service (PaaS) - care about Data, Applications\n- Software as a Service (SaaS) - just use it\n\nExamples:\n- IaaS -> Amazon EC2 instance (Virtual machine on AWS)\n- PaaS -> Heroku, (Elastic Beanstalk on AWS)\n- SaaS -> (Zoom, Dropbox, Gmail, Rekognition on AWS)\n\nAWS services can be any of the cloud computing types.\n\nPricing on the cloud:\n- pay for compute time\n- pay for amount of data stored on the cloud\n- pay for data transfer out of the cloud\n\n\n4. AWS overview\n\nGlobal infrastructure:\n- AWS Regions\n- AWS Availability zones\n- AWS Data Centers\n- AWS Edge Locations/Points of presence\n\nAWS Region e.g eu-west-2\nA region is a cluster of data centers\nUse AWS region close to you to have low latency\n\nAWS services are region scoped! The same service ran in different regions runs it 2 times\n\nHow to choose AWS region?\n- compliance (data govenrnance and legal requirements)\n- proximity (low latency)\n- available services in region\n- pricing in region\n\nAWS zones are SUBSET of AWS regions. AWS zone = AWS availability zone is one or more discrete data centers with their own power, networking and connectivity\n\nShared responsibility Model\nCustomer is responsible for the security IN the cloud\nAWS is responsible for the security OF the cloud\n\n5. IAM = Identity and Access Management global service\n\nProvides access control across all of AWS.\nWith IAM you can specify who can access which services and resources.\n\nIAM policies you manage permissions to other people.\nAccess is denied by default and is granted only if you specify Allow.\n\nRoot account is created by default when you register in AWS.\nShould NOT be shared or used when developing applications.\n\nHow users access AWS:\n- AWS Management Console (screen on the web app - protected by password + MFA(optional))\n- AWS CLI (protected by access keys)\n- AWS SDK (for code protected by access keys)\n\nAccess keys are like passsowrds generated through the AWS Console (screen on the web app)\n\n\nAn IAM user group is a collection of IAM users. \nUser groups let you specify permissions for multiple users.\n\nE.g. For a group called Admins  any user in that user group automatically has Admins group permissions\n\nAn IAM role is an IAM identity that you can create in your account that has specific permissions. \nIAM role vs IAM groups\nShort answer for googlers: you can't assign role to user but can assign group to user.\n    group is a bunch of users with the same policies\n    role is a preset of policies for service(s)\n\nBest practices:\n- dont use root account except for AWS account setup\n- One physical user = One AWS user\n- assign users t groups and assign permissions to groups\n- never share IAM users and access keys\n- Audit permissions of you account with the IAM credentials report\n\n\n6. Amazon EC2 = Amazon Elastic Compute Cloud = Virtual machine\n\nAmazon EC2 is a service allowing you to:\n- rent virtual machines (EC2 instances)\n- storing data on virtual drives (EBS = elastic block storage)\n- distribution across machines (ELB = elastic load balancing)\n- scaliing services using auto-scaling group (ASG)\n\nAWS EC2 is about renting compute/memory on demand (basically what AWS is about).\n","n":0.037}}},{"i":183,"$":{"0":{"v":"S3 vs DL vs Redshift","n":0.447},"1":{"v":"# S3 (Simple Storage Service)\n\n- pure storage, no SQL or any analytics\n- **Object** Storage. **NOT** columnar, log-structured(LSM), or B-trees\n- S3 does not understand tables, rows, columns, indexes, or trees.\n- All structuring is defined by how you write/read files (e.g., CSV, Parquet, JSON), not by S3 itself!\n- stores objects within buckets\n- an object is a file and any metadata that describes the file\n- a bucket is a container for objects\n\n\n# DataLake\n- Still Object Storage (in S3)\n- Built on top of S3\n- An architectural concept or solution—not a product or \"service\".\n- Centralizes and organizes data (usually on S3)\n- in AWS you will go to S3 service and you will have many buckets. Some buckets might be data lakes\n- DLs often support query tools (Athena/Databricks) for direct analytics!\n\n## Note on S3/DL storage\n\n- DL and S3 are object storages. The objects (files) within might have structure, for example columnar format like **Parquet, Avro** and you can define schema on them.\n- **But:** The underlying object store (S3) remains ignorant of the structure; it just stores files. The columnar \"magic\" happens inside the files themselves.\n- you can not have B-trees, LSM-tree storage indexing in S3/DL - you need to use full fledged database management system. \n\n# Redshift\n- Amazon Redshift is a fully managed cloud **data warehouse** service \n- It is not just storage - it is a ful featured SQL analytics database\n- **Columnar Storage:**\n    - Redshift stores table data in a columnar format (not row-based, not LSM, not B-tree).\n    - This means data is stored column by column, which is highly efficient for analytical queries (scanning/aggregating specific columns in very large datasets).\n\nSee [[engineering.AWS.How does Yelp store data?]] or practical usage\n\n## Note on parquet vs csv\n- parquet files are columnar, meaning they store data in columns rather than rows.\n- This is different from CSV files, which are row-based.\n\n\n# Bottom line\n\n- S3 = object storage, NO index/tree, NO enforced schema, NO columnar unless your files are.\n- Data Lake = organization + schema/catalog on S3; columnar if files are (e.g., Parquet); NO B-tree/LSM tree.\n- Databases (like Redshift) = implement B-trees/columnar engines internally as part of their design for querying rows and columns efficiently.\n\n\n# Pricing\n\n**Data Lake**\n- Athena and Spark charge based on the amount of **data scanned**, so inefficient queries or large raw datasets can quickly increase costs.\n- No infrastructure to manage: You don’t pay for servers or clusters—just for the queries you run.\n- Best for\n    - Ad hoc queries\n    - Infrequent or unpredictable workloads\n\n\n**Redshift**\n- Provisioned clusters: Pay for reserved compute/storage capacity (hourly, regardless of usage). OR\n    - Yelp has 4 clusters!!\n- Serverless: Pay for compute seconds used per query, plus storage.\n- Best for:\n    - Frequent, complex analytics\n    - Large-scale, repeated reporting\n    - BI dashboards and heavy workloads\n\n\n# HDFS\n\nHadoop Distributed File System. Like S3 it is used to store large amounts of data.\n\n- Distributed file system that runs on lusters of computers\n- Data stored in HDFS can be of any type, but the system automatically splits these files and stores them redundantly across many machines\n- HDFS is faster than S3 (no network latency)\n- HDFS is a distributed file system you run and maintain on your own cluster, primarily for on-prem or managed Hadoop/Spark environments.\n\n- S3 is not a file system, it is object storage. S3 is a manged service by AWS. HDFS is open source and you have to manage it yourself. It runs on a cluster of connected servers.\n\n- Legacy/On-prem-focused orgs: More likely to use HDFS.\n- Cloud/data-driven orgs (most modern tech): Strongly favor S3.","n":0.042}}},{"i":184,"$":{"0":{"v":"How does Yelp store data?","n":0.447},"1":{"v":"\n# How does Amazon help us store data\n\nWe use two data storage tools:\n- Amazon RedShift\n- Amazon S3 (Data Lake)\n\nEvery second users from all over the world are using Yelp App or Website and generating data. Engineers wrote code to store event logs into S3 (Amazon Simple Storage Service). The data in S3 form a **data lake**.\n\nS3 has a lot of pros:\n\nVery cheap for storage\n- Upon writing, there’s no schema required. Any format is accepted\n- Upon reading, there’s no barrier between files / tables\n\nBut it also have some cons:\n- Data quality is not guaranteed. It might require users to clean and write the schema for the data when reading the data. \n- It’s not the fastest \n\n\nTherefore, for the data that we have high usage, on a daily basis we clean the logs and write them into **Redshift**, where it is expensive to store but very fast and cheap to query. Redshift ensures the data schema is carefully designed before data is written. \n\nProblem is Redshift clusters are isolated with each other, you can’t join two tables in different Redshift clusters and drive insights. \n\n\nSolutions:\n\n1. Let users load tables into a third place then join them. For example, we can use a jupyter notebook to run queries separately on different clusters, then join them together with pandas. - Very very slow\n2. Add tables to the same cluster. - We do it sometimes, that’s why you see the same table in different clusters, but we can’t store all Yelp tables in one cluster, so this can’t completely solve the problem. Plus, this is very very expensive.\nFor example, in “ad-metrics” cluster, we have a table “ad-slot-metrics”, the same table is copied to “Bunsen” cluster, under “bunsen” folder, “ad_event” table\n3. Copy data from Amazon Redshift to Amazon S3. (For example, you can find “ad metrics” in both Redshift and S3: )Then there are a few options:\n- People can query the data lake with Amazon Athena. (Athena directly accesses data from S3 using SQL query.)\n- People can query data lake directly with Spark\n- Set up **Amazon Redshift Spectrum**, which is a connector that connects a Redshift cluster with certain databases / tables in S3. Then people only need to connect with one Redshift cluster and can query tables outside of the cluster just like the tables inside the cluster! (As long as the connection for the specific tables has been set up)","n":0.05}}},{"i":185,"$":{"0":{"v":"Economics","n":1}}},{"i":186,"$":{"0":{"v":"MIT","n":1}}},{"i":187,"$":{"0":{"v":"Microeconomics","n":1},"1":{"v":"\n# MIT 14.01 Principles of Microeconomics\n\n# Lecture 1: Introduction and Supply & Demand\n\nI nthis course we understand microeconomics in three ways:\n- intuitively\n- graphically\n- mathematically\n\n## What is microeconomics?\n\n- Microeconomics is a study of how individuals and firms make decisions in a world of **scarcity**.\n- Microeconomics = series of constrained optimization exercises\n- Microeconomics is a constraied optimization problem\n- One of the most important concepts of microeconomics is **OPPORTUNITY COST**\n- Every action or inaction has a cost in that you could have done something else instead\n- Nothing is free in microeconomics. Everything has a opportunity cost\n- Economics is a social science, models are not necessarily 100% correct, but are pretty good.\n- Adam Smith is the father of economics\n- His book is called the wealth of nations (1776)\n- Adam Smith did not use any maths in the book just words but he captured all concepts\n- e.g. the 'water and diamond' paradox. Water is way more important than diamonds. It is the building block of our civilization.  First thing you look for in a new planet is water. But water is cheaper than diamond. Why? Supply of water is infinite, diamond is not!\n- where supply and demand meet is the market equillibrium\n- Positive analysis is the study of things are\n- Normative analysis is the study of things should be\n- Economics in its core is a right wing science. It is about how the market knows best.\n- Adam Smith invisible hand view is that consumers and firms serving their own best interest will do what is best for society. This is the base view of why capilatism works\n- 'best for society' defined as most stuff are produced and consumed\n- The magic of capitalism. Everybody does the best in their own interest, without caring about anybody else will end up yilding the largest possible productive economy\n\nWhy you would not let everything to be decided by the market? I.e. Why allowing people to sell kidney on ebay might be a bad idea?\n- Market Failures, fraud, imperfect info\n- Inequality (only rich people will have healthy kidney)\n- Behavioral Econ, people make mistakes, they might have all the info but still make the wrong decision\n\n\n![](assets/images/roses_supply_demand.png)\n\n# Lecture 2. Preferences and Utility Functions \n\n**Where does the supply and demand curve come from?**\n\n- Demand curve comes from how consumers make choices\n- Supply curve comes from how firms make production decisions\n\n**Utility maximization model**\n- This model contains: preferences and budget contraint\n- Each model assumptions:\n- we have preference assumtions:\n    - completeness, you always know what you want. You cannot say I don't know. You can say I am indifferent between two things\n    - transitivity, I like a to b and b to c then I prefer a to c\n    - non satiation, assuming more is better than less\n\n\n**Indifference curves**\nI am indifferent to having\n- 2 pizzas and 1 cookie\n- 1 pizza and 2 cookies\n- but I strinctly prefer 2 pizzas and 2 cookies\n- an indifference curve contain all points to which I am indifferent\n\nProperties of indifference curves:\n1. Consumers always prefer higher curves\n2. All curves are downward sloping, otherwise non satiations will be violated\n3. No two curves cross: transitivity will be violated\n4. There is a unique cruve between each set of indifferent points, oterwise completeness will be violated (you cannot know how you feel)\n\n![](assets/images/indifference_curve.png)\n\n**Utility function**\n- Utility function is a way to rank consumer choices\n- Marginal Utility is the derivative of the utility function\n    - Marginal thinking turns out to simplify economics a lot\n    - It is way easier to ask Doyou want the next cookie, than asking how many cookies do you want?\n- Principle: utility functions must have diminishing marginal utility (diminishing marginal returns)\n- marginal utility is always positive, the more the happier\n\n\n- Indifference curves are not concave to the origin (proof using diminishing marginal uttility). as you increase the cookies and decrease pizza, you are \nwilling to give up more cookies to get another slice of pizza\n- MRS = marginal rate of substitution = $delta pizza/delta cookie = -MU_cookie/ MU_pizza$ is the amount of pizza you are willing to give up for an extra cookie, the ratio are flipped. MU_cookie is how much do I want the next cookie, the more I like it the more pizza I'm willing to give\n- Example with Utility function = $sqrt(P*C)$\n\n\n![](assets/images/mrs.png)\n\n## Example – Pricing of different sizes of goods\n\nStarbucks tall coffee for $2.25 and the next one for 70 more cents. McDonald – Small is $1.22 but for 50 more cents you can double the size. Why do they give twice as much for much less than twice as much money? Its all about diminishing MU, you’re desperate for the first soda on a hot day, but not twice as much for the next glass. Those prices reflect market’s reaction to diminishing MU. If you think about demand and supply, the demand for 1st 16 ounces is higher than the demand for the 2nd 16 ounces, but the cost to produce the drink is the same. Since the demand for the next 16 ounces doesn’t shift twice as much, you can only charge 50 cents for the next 16 ounces. Price increments get smaller because of diminishing MU.\n\n**However**, say you buy breakfast cereal in bulk from costco. Then every day the cereal will be my first cereal and will have high utility to me. There is some diminishin but it is much less. Buing in bulk is much cheaper than buying every day a cereal so why does the market does not factor out the small diminishing return of eating a cereal?\n\nThis is related to packaging costs, but also buing in bulk from costco for many things is not much cheaper than buying seperately from supermarkets.\nBuing in bulk is cheaper but it is not as nearly as chaper as the staqrbucks and mcdonalds example. When utility diminishess less then they do not want to charge much less for multiple packages. Here we have the factor of parishable and non-parishable goods. When it is parishable then it has diminishing return when it is non-parishable you can 'reset' the utility.\n\n\n# Lecture 3. Budget Constraints and Constrained Choice \n\n\n## Budget Constraints\n\nWe assume budget contrain = income. We do not consider credit and borrowings.\n\n- MRT = marginal rate of transformation. Given a contrained budget (money) you can spend you can buy certain amount of pizzas and cookies. The more cookies you buy the less pizza you buy and vice verca.\n- Opportunity set = the set of amounts of pizzas and cookies you can buy given the market prices and your income\n- Given utility function you can draw many indiffrence curves\n- Given utility function and budget constraint the tangent indifference curve to the budget constraint is the best one\n- **MRS = MRT** is the main eqation of consumer choice!!!\n- you want the ratio of the benefits to be equal to the ratio of the costs\n- Marginal thinking is much easier than absolute things. Do I want the next pizza vs how many pizzas do I want?\n- Consumer theor is all about the balancing act between what you want and what the market costs allow you to do\n- MB = MC, marginal benefit is equalt to the marginal cost. What I want to do next should be exactly equal to what it costs.\n-\n![](assets/images/budget_constraint_indifference.png)\n\n\n # Lecture 4. Demand Curves and Income/Substitution Effects\n\n- Demand Curve shows the relationsip between the quantity demand and the price of a good\n","n":0.029}}},{"i":188,"$":{"0":{"v":"Doctrines","n":1},"1":{"v":"# Liberalism\n\n1. Historical Context: Liberalism has a broader historical context and refers to a political and moral philosophy that emerged in the 17th and 18th centuries, emphasizing individual freedom, democracy, and equal rights.\n2. Economic Approach: Liberals generally support a market economy but believe in government intervention to regulate and correct market failures. They advocate for social safety nets, public education, and healthcare.\n3. Social Policies: Liberals often support progressive social policies, such as LGBTQ+ rights, environmental protection, and gender equality.\n4. Role of State: Liberals believe in a balanced role for the state, where it ensures individual rights, promotes equality, and provides necessary public services.\n\n# Neoliberalism\n*neo = new*\n\n1. Historical Context: Neoliberalism specifically refers to a revival of classical liberal economic thought in the mid-20th century, advocating for a free-market economy with minimal government intervention.\n2. Economic Approach: Neoliberals emphasize free-market capitalism, deregulation, reduced government spending, and privatization of public services. They believe that market forces should dictate most aspects of the economy.\n3. Social Policies: Neoliberalism focuses primarily on economic policies and does not inherently prescribe specific social policies. It prioritizes economic efficiency over social equality.\n4. Role of State: Neoliberals advocate for a limited role of the state in the economy. They believe that market mechanisms, if left unimpeded, will lead to optimal economic outcomes.\n\n\n# Liberalism:\n1. **Role of Government:** Liberals believe in a significant role for the government in addressing social and economic inequalities. They support government intervention to promote social welfare, education, healthcare, and workers' rights.\n2. **Individual Rights:** Liberals emphasize individual rights and freedoms, including freedom of speech, religion, and assembly. They also advocate for civil liberties and equal protection under the law for all citizens.\n3. **Social Justice:** Liberals often advocate for policies aimed at reducing social inequalities. They support progressive taxation, social safety nets, and affirmative action programs to address historical injustices.\n4. **Economic Approach:** While liberals support market economies, they believe in regulation to prevent market failures and ensure fair competition. They advocate for a balance between free-market principles and social welfare policies.\n\n\n# Libertarianism:\n1. **Role of Government:** Libertarians advocate for minimal government intervention in both the economy and personal lives. They believe in limited government functions, primarily focused on protecting individual rights and maintaining law and order.\n2. **Individual Rights:** Libertarians prioritize individual liberty and personal freedom. They emphasize the right to own property, make personal choices, and engage in voluntary transactions without government interference.\n3. **Social Justice:** Libertarians argue that voluntary exchanges and free markets can address social issues more efficiently than government intervention. They oppose wealth redistribution programs and affirmative action, relying on individual merit and voluntary charity.\n4. **Economic Approach:** Libertarians advocate for free-market capitalism with minimal regulations. They believe that market forces and individual choices should determine economic outcomes, leading to innovation, efficiency, and prosperity.\n\nIn summary, while both liberalism and libertarianism emphasize individual rights, liberalism supports a more active role for government in addressing social inequalities and ensuring social justice, whereas libertarianism advocates for minimal government intervention, relying on voluntary interactions and free markets to solve societal issues.\n\n\n# Conservatism\n\n\n# Socialism\n\n# Marxism\n\n#TODO","n":0.045}}},{"i":189,"$":{"0":{"v":"Deep Learning","n":0.707}}},{"i":190,"$":{"0":{"v":"Coursera","n":1}}},{"i":191,"$":{"0":{"v":"Neural Networks and Deep Learning","n":0.447},"1":{"v":"[Coursera course by Andrew Ng](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning)\n\nThis is a course part of the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning#courses)\n\n# Intro to DL\n\nReLU = Rectified Linear Unit: $max(0,wx+b)$\n\nHidden Layers of a neural network learn more complex patterns in your data. The magic of NN is that you do not need to learn explicitly what are these patters.\n\nEvery hidden layer is a complex function of the input layer.\n\n## Applications of different DL\n\n- Standard NN, Feed Forward NN: Home prices, Ad prediction\n- CNN: images\n- RNN: sequential data (audie, NLP, language, translation), temporal component\n\n\nStructured data = tabular\n\nUnstructured data = text, images, audio,\n\n## Why DL is taking off now?\n\n![](assets/images/dl_performance.png)\n\nWhen data is small, the rank of ML/DL algos depends much on your skills on modelling and architecture building. When data is big, usually NN are better.\n\n\nInnocations:\n- More data\n- faster computation (CPU, GPU...)\n- better algorithms\n\nSigmoid activation function to RELU the computation is much faster, Relu has gradient 1 for positive values, sigmoid can have gradient close to 0.\n\n# Logistic Regression as a Neural Network\n\nBinary classification of an image - flatten all pixels into one vector\n\n![](assets/images/binary_class.png)\n\n\nIn NN the design matrix would have dimension $nxm$ - it is easier that way (transposed of ML design matrix)\n\n![](assets/images/nn_notation.png)\n\n**Logistic regression is just applying sigmoid function to the linear model, $\\hat{y} = \\sigma(wx+b)$ and using logistic loss function.**\n\n![](assets/images/logistic_tregression.png)\n\n![](assets/images/log_reg_defn.png)\n\nNote in logistic regression we predict $\\hat{y} = \\sigma(wx+b)$ which represents a probability between 0 and 1. We use the ogistic losss function so that we have a convex cost function. No matter where you initialize, you would reach the global minimum.\n\n**Logistic loss = - Log likelihood**\n\n## Gradient Descent\n\n$w = w-\\alpha J'(w)$, where J is the loss function\n\n\n![](assets/images/gradient_descent.png)\n\nThe gradient is equal to the slope of the tangent function. Math derivation:\n\nSay for $f(x)$ you want to find the tangent line $ax+b$ at point $x_1$\n\nThen $f(x_1) = ax_1+b$ must have one solution. Take derivative of both sides:\n\n$a=f'(x_1)$\n\nNote that at each step we go to the opposite direction of the derivative with a step size of $\\alpha$. By the graph above you can see how we slowly go to the optimum value. You can use also only the sign of the derivative. But using the derivative itslef gives you smaller stepsize when you are closer to the optimum value.\n\n\n\n![](assets/images/derivative.png)\n\nDerivative = slope = change in $f$/ change in $x$ = how much $f$ would change if you change $x$ by a little bit. In the picure above if you nudge $a$ by 0.001, then $f$ changes by 0.003\n\n## Computation Graph\n\nNN are organised in terms of:\n- forward propagation: compute output of NN\n- backward propagation: compute gradient of the loss function with respect to parameters\n\nThe computation graph organises these two steps.\n\n![](assets/images/computation_graph.png)\n\n\nBackprogation is just an application of the chain rule.\n\nChanging $v$ by 0.001, $J$ changes by 0.003, hence $dJ/dv = 3$. When you change $a$ by 0.001, $J$ changes by 0.003, hence $dJ/da = 3$. But also changing $a$ changes $v$ which changes $J$. $dJ/da = dJ/dv dv/da$. This is the chain rule.\n\n![](assets/images/backpropagation.png)\n\n\n**Logistic regression computational graph**\n\n$a=\\hat{y}$ below\n\nGoal is to compute $dJ/dw$, $dJ/db$, $dJ/dz$, $dJ/da$, where the first two are the gradients of the loss function with respect to the parameters, and the last two are the gradients of the loss function with respect to the intermediate variables.\n\n\n![](assets/images/log_reg_comp_graph.png)\n\n\nOne step gradient descent for logistic regression gradient descent:\n![](assets/images/log_reg_gd.png)\n\n## Vectorization\n\nGreat speedups. Instead of looping over all examples, you can do the computation in one go.\n\n![](assets/images/log_reg_gd_vec.png)\n\n**Broadcasting in Python**\n\nBroadcasting copies automatically the vector to the right shape. For example:\n \n```python\nnp.array([1,2,3,4]) + 100 = np.array([101,102,103,104])\nnp.array([1,2,3,4],[5,6,7,8]) + 100 = np.array([101,102,103,104],[105,106,107,108])\n```\n\n![](assets/images/broadcasting.png)\n\n$3x4$ matrix division by $1x4$ vector gives $3x4$ matrx\n\n\n**NOTE on vectors**\nUse:\n```python\nnp.random.rand(5,1) # shape is (5,1) - column vector\n```\n\nDo not use:\n```python\nnp.random.rand(5) # shape is (5,) - that is nothing\n```\n\nTip:\n- add assert statements to check the shape of the vectors\n```python\nasser (w.shape == (n,1))\n```\n\n**Remember:**\n- the sigmoid function and its gradient $\\sigma'(x)=\\sigma(x)*(1-\\sigma(x))$\n- image2vector is commonly used in deep learning\n- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n- numpy has efficient built-in functions\n- broadcasting is extremely useful\n\n\n# Shallow Neural Networks\n\n**For one training sample:**\n\nOne hidden-layer NN = Two layer NN (inpuut layer is not ussually counted)\n\nFirst layer: $z^{[1]} = Wx + b$, $a^{[1]} = \\sigma(z^{[1]})$\n\nSecond layer: $z^{[2]} = Wa^{[1]} + b$, $a^{[2]} = \\sigma(z^{[2]})$\n\nAssume we have $n_x$ input features, $n_h$ hidden units, $n_y$ output units. Then the dimensions of the matrices are: $W^{[1]}$ is $n_h$ x $n_x$, $b^{[1]}$ is $n_h$ x $1$, $W^{[2]}$ is $n_y$ x $n_h$, $b^{[2]}$ is $n_y x 1$.\n\n**For multiple training sample:**\n\n\n![](assets/images/NN_matrix_calc.png)\n\nThis shows why we have the design matrix X dimensions to be dimension x training_samples (mxn)\n\n## Activation functions\n\n- sigmoid: $\\sigma(z) = \\frac{1}{1+e^{-z}}$ goes between 0 and 1\n- tanh: $\\tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$ goes between -1 and 1\n- ReLU: $a = max(0,z)$\n- leaky ReLU = $max(0.01z,z)$\ntanh would make your hidden layers have mean around 0.\n\nAndrew Ng: \"tanhh is almost always better than sigmoid function, except for the output layer where you have to predict 0 or 1. In that case, sigmoid is better. I almost never use sigmoid function as activation function in hidden layers.\"\n\n**Downside of both sigmoid and tanh is that if $z$ is large than the derivative/slope is almost equal to 0 (vanishing gradient).**\n\n\n**ReLU has derivate 1 when z is positive and 0 when it is negative. If $z=0$ then the derivative is not defined (which does not happen in practice)**\n\n\nRule of thumb: \"ReLU is the default activation function to use if you don't know what activation function to use for hidden layers. For output layer, sigmoid for binary classification, sigmoid for multi-class classification, and no activation for regression.\"\n\nLeaky ReLU is used when you have a lot of negative values in $z$ and you want to avoid the \"dead neurons\" problem. But in practice ReLU is used more often.\n\nReLU is used more often than tanh because the slope is very different than 0 for positive values of $z$ and NN learns much faster.\n\n**Sigmoid is almost never used for hidden layers**\n\n**ReLU is the most common activation function**\n\nYou need linear activation function for the output layer for regression problems. Otherwise, on hidden layers there is no point using linear activation function because the NN would be just a linear function (compozition of linear functions is linear).\n\n## Derivatives of activation functions\n\n- sigmoid: $\\sigma'(z) = \\sigma(z)*(1-\\sigma(z))$, derivative at 0 is $1/4$\n- tanh: $\\tanh'(z) = 1 - \\tanh^2(z)$, $z = 0$, then $tanh'(z) = 1$\n\n\n## Gradient descent in NN with 1 hidden layer\n![](assets/images/NN_gd.png)\n\n\n**NN require random initialization of the weights. If they are all 0-s then all activation functions in one layer would be the same values.**\n\nInitialization when using sigmoid on tanh it might be better to initailize wth random values which are smaller (for large z the derivative vanishes)\n\n\n**Reminder**: The general methodology to build a Neural Network is to:\n1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n2. Initialize the model's parameters\n3. Loop:\n    - Implement forward propagation\n    - Compute loss\n    - Implement backward propagation to get the gradients\n    - Update parameters (gradient descent)\n\n\n# Deep Neural Networks\n\nThe deeper the layr the more complex features can be learned. For example in images:\n- input layer - pixels\n- first layer learns edges\n- second layer learns shapes\n- third layer learns parts of objects\n- fourth layer learns objects\n- fifth layer learns scenes\n- output\n\n**Theory**\n\nThere are functions which can be represented by a deep NN with a small number of hidden units, but require an exponential number of hidden units in a shallow NN.\n\n\nLayer $l$: $W^{[l]}$ weights, $b^{[l]}$ bias\n\nInput: $a^{[l-1]}$, output: $a^{[l]}$\n\n\n![](assets/images/l_layer.png)\n\n\n## Forward and backward propagation\n\n\n![](assets/images/nn_comp_graph.png)\n\nDeep Learning is good at learning very flexible and complex functions.\n\n\nDL and NN has nothing to do with the brain. It is just a function approximation algorithm.\nNeuro science is not very useful for DL - we do not even know how neurons work in the brain.\n\n\n# Implementation of DL\n\nSteps:\n- Initialize the parameters for a two-layer network and for an $L$-layer neural network\n- Implement the forward propagation module (shown in purple in the figure below)\n     - Complete the LINEAR part of a layer's forward propagation step (resulting in $Z^{[l]}$).\n     - The ACTIVATION function is provided for you (relu/sigmoid)\n     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This gives you a new L_model_forward function.\n- Compute the loss\n- Implement the backward propagation module (denoted in red in the figure below)\n    - Complete the LINEAR part of a layer's backward propagation step\n    - The gradient of the ACTIVATION function is provided for you(relu_backward/sigmoid_backward) \n    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function\n    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n- Finally, update the parameters\n\n![](assets/images/NN_impl.png)\n\n**Note**:\n\nFor every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients. \n\nIn the backpropagation module, you can then use the cache to calculate the gradients.\n\n\n```python\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    #(≈ 4 lines of code)\n    W1 = np.random.randn(n_h, n_x)*0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h)*0.01\n    b2 = np.zeros((n_y, 1))\n    # YOUR CODE STARTS HERE\n    \n    \n    # YOUR CODE ENDS HERE\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters  \n```\n\n\nThe initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the `initialize_parameters_deep` function, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. For example, if the size of your input $X$ is $(12288, 209)$ (with $m=209$ examples) then:\n\n![](assets/images/nn_layers_shape.png)\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        #(≈ 2 lines of code)\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\n\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W,A) + b # broadcasting\n    cache = (A, W, b)\n    \n    return Z, cache\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n\n    cache = (linear_cache, activation_cache)\n    return A, cache\n```\n\n\nForward propagation full model with L-1 Relu hidden layers and 1 sigmoid output layer:\n\n```python\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n\n    for l in range(1, L):\n        A_prev = A \n        #(≈ 2 lines of code)\n        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)],parameters['b'+str(l)], 'relu')\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'sigmoid')\n    caches.append(cache) \n    return AL, caches\n```\n\nCompute the cross-entropy cost $J$, using the following formula: $-\\frac{1}{m}\\sum_{i = 1}^{m}(y^{(i)} log(a^{[L] (i)}) + (1-y^{(i)})log(1- a^{[L](i)}))$ :\n\n\n```python\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n    cost = -(np.dot(Y.T,log(AL))+np.dot(1-Y.T,log(1-AL))/m\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    \n    return cost\n```\n\n- Linear backward\n\n```python\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = np.dot(dZ,A_prev.T)/m\n    db = np.sum(dZ,axis=1,keepdims=True)/m # sum by the rows of dZ with keepdims=True\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev, dW, db\n\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ =  relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ =  sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db\n\n\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    #(1 line of code)\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n\n    \n    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n    #(approx. 5 lines)\n    current_cache = caches[-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache ,'sigmoid')\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    # Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache ,'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l+1)] = dW_temp\n        grads[\"db\" + str(l+1)] = db_temp\n\n    return grads\n```\n\n\n![](assets/images/linear_backward.png)","n":0.019}}},{"i":192,"$":{"0":{"v":"Improving Deep Neural Netwo","n":0.5},"1":{"v":"[Andrew Ng's course](https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning#syllabus)\n\n\n# ML Setup\n\nApplied ML is a highly iterative process. Idea -> Code -> Experiment -> Idea -> Code -> Experiment -> ...\n\nYou need setup to that. You need to be able to run experiments quickly.\n\nTrain/Validation/Test sets split\n\nPrivious era was 60/20/20 or 70/30 (train/test split)\n\nThis is fine when you have 100-1000-10000 data points\n\nBut now when you have big data 1M+, then you might need less test set. You use the test set to evaluate of how well your model is doing on unseen data. Say you need 10,000 test data points for test. If you have 1M data points, then you can use 99/1 split.\n\n\n**Aim to have yur validation and test set coming from the same distribution.**\n\n# Bias/Variance\n\nSay we have the task is to classify images of dogs. This is a task human are almot perfect (optimal error is neary 0 percent)\n\n- Train set error: 1 %\n- Dev set error: 11 % \nHigh variance/overfitting\n\n- Train set: 15%\n- Dev set error: 16%\nHigh bias/underfitting, because these errors are much larger than the base error.\n\n## Recipe\n\n1. Ask yourself: \"Does your model have high bias?\" If yes, you need more complicated network (bigger network, train longer, different NN architecture). Try to get tird of this problem first.\n2. What is my dev/validation performance. Am I overfitting? If yes, get more data, regularization, different NN architecture.\n\nIn DL era, there is less talk about bias-variance tradeoff. \n- Making a bigger network usually decreases bias and as long as you regularize, you would not hurt variance (Need more computational time).\n- Training with more data would usually decrease variance and not hurt bias.\n\n# Regularization\nL1 model, makes the model more sparse. L2 model, makes the model more smooth.\n\nL2 regularization results in weight decay. The weights would slowly decrease.\n\nIf you do not have regularization, then your Loss would decrease monotonically as you increase the number of iterations\n\n## Dropout regularization\n\nRandomly eliminate a random set of nodes in each layer. It randomly shuts down some neurons in each iteration. The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \n\n\n![](assets/images/dropout.png)\n\nDrop out would decrese the expected values of the activations of each node. So you need to increase the weights to compensate for that. So you need to increase the weights by a factor of 1/(1-p) where p is the probability of dropout. **Inverted dropout**\n\n![](assets/images/inverted_dropout.png)\n\nThe layers with more neurons would have higher dropout rate.\n\nAt test time, you do not use dropout. You use the whole network.\n\n\nDropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. Dropout roughly doubles the number of iterations required to converge.\n\nSay node A in layer L depends on nodes B,C,D in layaer $L-1$. Dropping nodes in layer $L-1$ would make node A spread out evenly its dependence on nodes B,C,D. It would not put all the weight in one of those.\n\n# Normalizing data\n\nsubstract mean + divide by $\\sigma$\n\nWhy do we need normalizing?\n\nIf x1 and x2 are in different scales, then the weights $w1$ and $w2$ would be very different the cost function would be elongated in one direction (stretched cost function). So the gradient descent would take longer to converge.\n\n![](assets/images/opt_we.png)\n\n# Vanishing / Exploding gradients\n\nProblem of the DEEP networks.\n\nSay we have an indetity activation function $g(z) = z$ and $b^{l}=0$ for all laeyers. Then:\n\n$\\hat{y} = W^{L}W^{L-1}...W^{1}X$. Then the derivatives could increase exponentially or decrease exponentially. \n\n# Weight Initialization\n\nThis can be important to tackle the vanishing/exploding gradients problem.\n\n$z = w_1x_1 +...+w_nx_n $\n\nThe larger $n$, the smaller $w_i$ you want to have.\n\nFor Relu activation people often initialize with $W = np.random.randn(shape) * np.sqrt(2/n^{[l-1]})$\n\n## Gradient Checking\n\nGradient estimation:\n\n$\\frac{dJ}{d\\theta} = \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$ is better than $\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}$\n\n\n![](assets/images/grad_est.png)\n\nUsing these gradient estimation formulas, you can check if your backpropagation is correct.\n\n![](assets/images/grad_check.png)\n\n\n[He et.al 2015](https://arxiv.org/abs/1502.01852s) initialization\n\n\n```python\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*np.sqrt(2/layers_dims[l-1]) # small weights to start with, normal distribution is better than uniform to avoid the extremes, most points in the center where the derivative is larger - see sigmoid function.\n        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n    return parameters\n\n```\n\n# Optimization Algorithms","n":0.035}}},{"i":193,"$":{"0":{"v":"Stanford CS230: Deep Learning","n":0.5},"1":{"v":"\n[Stanford DL course](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)\n\n[Syllabus](http://cs230.stanford.edu/syllabus/)\n\nThis course is youtube videos + coursera videos. Youtube has deeper insights and coursera is for practice and fundamentals.\n\n# Key messages\n- In deep learning, feature learning replaces feature engineering\n- Traditional ML performance **platues** at some point and cannnot utilize more data. For NN we have not reached that point yet.\n- ML,DL is all about approximating/learning a function.\n- Model = Architecture + Parameters\n- Gradient evaluated at particular point = slope of the tangent at particular point\n- Gradient checking (e.g if the signs changes often) is a good way to see if your optimization of the loss function is divergent.\n- Backprogation is just an application of the chain rule.\n- Backpropagation is used to calculate the gradient of the loss function with respect to the parameters. \n- ReLU is the most common activation function\n- Sigmoid is almost never used for hidden layers, tanh is almost always better choice\n- NN require random initialization\n- Aim to have yur validation and test set coming from the same distribution.\n- L2 regularization is also called weight decay $w = (1-\\dfrac{\\alpha\\lambda}{n})w - other terms$\n- Sanity checks for your NN:\n    - cost is decreasing as number of iterations increase (if using dropout, might not be the case)\n    - gradient checking  \n- Regularization is used to reduce overfitting\n    - L1, L2\n    - Dropout\n    - Data augmentation (add more data, rotate image, add some noise, flip image, blur image )\n    - Early stopping\n- Orthogonalisation idea one task at a time. (First focus on minimizing the cost function (overfit!). Then focus on regularization.)\n- Normalize inputs for NN to converge more quickly\n- Careful choice of weights initialization can solve vanishing/exploding gradients\n- Gradient tracking\n- https://arxiv.org/abs/1502.01852\n\n\n# Lecture 1 Deep Learning overview\n\nDeep Learning and Neural Network are almost the same thing. DL is the popular brand word.\n\nTraditional ML performance **platues** at some point and cannnot utilize more data. For NN we have not reached that point yet.\n\n![](assets/images/ML_DL_performance.png)\n\n**Fundamental courses:**\n\n- CS229 (ML most mathematical)\n- CS299A (Applied ML least mathematical and easiest)\n- CS230 (a bit in between, focuses on DL)\n\nFinish C1M1, C1M2 (chap 1 module 2).\n\n\n# Lecture 2: Deep Learning Intuition\n\nML,DL is all about approximating/learning a function.\n\n**Model = Architecture + Parameters**\n\nThe goal of ML/DL is to find the best parameters for the model.\n\nExample:\n\n![](assets/images/lg_nn.png)\n\n**Each hidden layer encodes information from previous layer. E.g in convolutional neural network, each layer encodes more and more complex features. **\n\n![](assets/images/cnn_encoding.png)\n\n\n## Case study 1: Day and Night classification\n\n**Deep Learning Project choices:**\n\n![](assets/images/dl_choices.png)\n\nThe problem of clasifying a cat needed 10,000 images. Is the next problem easier or harder? Depending  on the task you want to solve you would know based on past projets how many data point you need.\n\n- train/test split 80/20 is good for 10k images. If I had 1M images I would choose 98/2 split. Test data is to gauge how well the model is doing on real unseen data. You ask yourself how many data points I need to tell my model is doing good (dawn, sunset, sunrise, evening, morning)\n- bias you want balanced dataset in train and test\n- resolution of images (the smaller the better for computation. 32x32 is better than 400x400) Choose the smallest resolution that human can have perfect performance. If you had unlimited computational power you would choose the highest resolution.\n\n\n\n\n## Case study 2: Face verification\n\n- architecture: encode each image using a DL and then compute distance functions\n- In face verification, we have used an encoder network to learn a lower\ndimensional representation (called “encoding”) for a set of data by\ntraining the network to focus on non-noisy signals.\n- **Triplet loss** is a loss function where an (anchor) input is compared to a\npositive input and a negative input. The distance from the anchor input to\nthe positive input is minimized, whereas the distance from the anchor input\nto the negative input is maximized.\n\n![](assets/images/face_verify1.png)\n\n![](assets/images/face_verify2.png)\n\n![](assets/images/face_verify3.png)\n\n\n## Case study 3: Art generation\n- Do not train model, just optimize the cost function by changing pixels (goal is to generate image)\nKian Katanforoosh\n- In the neural style transfer algorithm proposed by Gatys et al., you optimize\nimage pixels rather than model parameters. Model parameters are\npretrained and non-trainable.\n- You leverage the “knowledge” of a pretrained model to extract the content\nof a content image and the style of a style image\n- ImageNet\n- Take gradient of loss function with respect to the pixels $x$.\n\n![](assets/images/art_gen1.png)\n\n![](assets/images/art_gen2.png)\n\n![](assets/images/art_gen3.png)\n\n## Case study 4: Trigger word detection (Alexa)\n\n- Your data collection strategy is critical to the success of your project. (If\napplicable) Don’t hesitate to get out of the building.\n- You can gain insights on your labelling strategy by using a human experiment\n\n![](assets/images/trigger_word.png)\n\n![](assets/images/data_collection.png)\n\n\n# Lecture 3 - Full-Cycle Deep Learning Projects\n\n1. Select problem (e.g. supervised learning)\n2. Get data (need to be very strategic in that)\n3. Design model\n4. Train model (iterative process with point 2 and 3)\n5. Test model\n6. Deploy\n7. Maintain\n\nFive points when selecting a project:\n- interest\n- data availability\n- domain knowledge\n- feasibility\n- Usefulness\n\nFirst goal when starting to build a ML system is to get a baseline as fast as possible. Then iterate on that. That is steps 1-5 should be done within a couple of days.\n\nML developement is an iterative process. Before you start working on it it is difficult to know what are the hard problems you need to tackle\n\n**Tips:**\n- keep clear notes on experiments runs\n- generally when you have to make choice between multiple options - go with the simpler one first\n- in real projects often data change over time. Non-ML models are more robust, ML models often need to be retrained.\n\nedge deployment vs cloud deployment\n\n# Lecture 4 - Adversarial Attacks / GANs\n\n**Discovery: Several ML models, including state-of-the-art neural networks are vulnerable to adversarial examples.**\n\n[Szegedy et al. (2013): Intriguing properties of neural networks]\n\n[Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy (2015): Explaining and harnessing adversarial examples]\n\n## Attacking a network with adversarial examples\n\nWhat are examples of Adversarial attacks?\n\nGiven a network pretrained on ImageNet, find an input image that is not a iguana but will be classified as iguana.\n\nAttack:\n\n1. Start with some image that is not iguana.\n2. Use pretrained NN and output a vector of probabilities (cat, dog, iguana..)\n3. Compute Loss.\n4. Take gradient of loss wrt to the pixels. (backpropagation, need to have access to the model, parameters, layers etc.)\n5. Change the image pixels.\n\n![](assets/images/adversarial_examples.png)\n\nYou can see that chaning with very little the pixels in the image you can fool the network that the cat is an iguana.\n\n![](assets/images/space_images.png)\n\n## Defense against a network with adversarial examples\n\n1. Add a SafetyNet - one more NN that will detect adversarial examples. (but it can be fooled as well) [Yuan et al. (2017): Adversarial Examples: Attacks and Defenses for Deep Learning]\n2. Train on corrrectly labelled adversarial examples. Generate adversarial examples and train on them.\n\n\n## Why are NN vulnarable to adversarial examples?\n\n[Adversarial Examples: Attacks and Defenses for Deep Learning](https://arxiv.org/pdf/1712.07107.pdf)\n\nYou'd think that NN are vulnaraalbe because they are too complex and overfit the data. But it turns out that the linearity part of the network is the problem.\n\n$y = xw+b$\n\nIt is easy to create $x^{*}=x+\\epsilon$ which would produce massively different y.\n\n**Fast Gradient Sign Method** is a method to create quickly adversarial examples.\n\n## GAN - Generative Adversarial Networks\n\nThe word adversarial is used in different meaning here. In GANs it means that there are two networks that are competing with each other.\n\nGAN-s are networks that generate images that mimic the distribution of the real images/data.\n\n### G/D game\n\nGenerator (is what we want to train and generate fake images),\n\nDisriminator\n\n\nG wants to fool D. D catches fake images. In the beginning it would be very good at catching fake images. But G will learn to generate better images. G and D are trained together.\n\nMin-max trick of chaning the loss function\n\nSaturating cost vs non-saturating cost \n\nIf D does not improve G cannot improve. You can see D as an upper bound on G.\n\n\n```python\nfor num_iterations:\n    for k in range(k_steps):\n        update D\n    update G\n```\n\nTips for training GAN-s:\n- modify loss function\n- keep D up-to-date with respect to G (k update for G and 1 update for D)\n- Virtual Batchnorm\n- one-sided label smoothing\n\n\nCycleGAN\n\nimage with horses, generate same image with zebra.\n\nLoss function contains all losses from normal gans + cycle loss. \n\n\n![](assets/images/cycle_gan.png)","n":0.028}}},{"i":194,"$":{"0":{"v":"MIT - Introduction to Deep Learning","n":0.408},"1":{"v":"# 6.S191 | MIT Introduction to Deep Learning\n\n- [Official Website](https://introtodeeplearning.com/)\n- [GitHub Repo](https://github.com/MITDeepLearning/introtodeeplearning/tree/master)\n\n# Key words\nperceptron,feed forward neural network, activation function, weights and bias, sigmoid, relu, hyperbolic, gradient descent, stochastic gradient descent, backpropagation, chain rule, learning rates, adam, adaptive learning rates, overshoot or stuck at local minimum, overfitting, regularization, dropout, early stopping, sequence modeling, rnn, hidden states in rnn?, backpropagation through time, embedding = vectorization, exploding/vanishing gradient, transformer, attention is all you need, \n\n# Lecture 1: Introduction\n\nDeep Fake vide of Obama. MIT created 2 minutes deep video of OObama saying Welcome to MIT class. In 2020 ot costed 15K $\n\nIntelligence - the ability process information in order to inform some future decision or action\n\nArtificial Intelligence - make computers be able to learn to apply this process.\n\nMachine Learning is a subset of Artificial Intelligence.\n\nMake computers learn and execute tasks from the given data.\n\n\n## Why Deep Learning\n\nClassical ML works by defining features. For example in image detection we would start defining lines, edges, curves ,eyes, noses, face. We need to define the features from low level to high level. We can't detect faces directly. We built composite features. **Feature Engineering**.\n\nDL automates the process of **feature engineering**. DL has been around for a long time (decades). Why it became popular now?\n- More data\n- Compute power\n- Libraries like tensorflow, pytorch\n\n## The perceptron: Forward propagation\n\n- single neuron\n- input vector $x$\n- Linear sum using weights and a bias term\n- non-linear activation function: sigmoid (good for probabilities), ReLu (piecewise linear, non linear at 0), hyperbolic function\n\nThe point of the activation function is to introduce a non-linearity because real data in real world is heavily non-linear.\n\n$\\hat{y} = g(w_{0} + x^{T}w)$ \n\n\n![alt text](./assets/images/perceptron_simplified.png)\n\n**dot product, add bias, apply non-linearity**\n\n\n## Layer\n\n![alt text](./assets/images/one_hidden_layer.png)\n\n\n## Deep network\n\nHas many hidden layers\n\n## Loss\n\nEmpirical Loss\n\nLoss function = Cost function = Objective function\n\n\nCross entropy loss, difference between probabilities. For Binary predictions.\n\nMean Squared Errors, difference between us functions. For real number predictions.\n\n\nOur goal is to find a network that minimizes the loss on the given dataset.\n\nGoal is to find all weights.\n\n\n## Loss optimization\n\n![alt text](./assets/images/loss_optimization.png)\n\n## Gradient Descent\n\nRandomly initialize our weights. Randomly pick a point in our landscape (loss function). Compute the gradient and take the opposite direction of the gradient. Note this is **local optimization**. We go with a small step opposite to the gradient direction. Choosing learning rate = step size.\n\n\n![alt text](./assets/images/gradient_descent_2.png)\n\nHow do we compute the gradient? the process of computing the gradient is called **backpropagation**.\n\nDerivatives, chain rule\n\n\nNeural networks are extremely complex functions with complex loss landscapes.\n\n## Learning rate\n\nYou don't want to set it too small, because you will be stuck in local minimum.\n\nYou don't want it to be too large as you will overshoot and diverge.\n\n\n### Adaptive Learning Rates\n\nChange the learning rate depending on the landscape\n- how fast you are learning\n- how steep\n\n\n## Stochastic Gradient Descent\n\n\nGD computes the gradient over the entire dataset which can be computationally expensive.\n\n\nSGD chooses a subset of the data to estimate the gradient\n\nMini batch Gradient Descent (choose a batch of B data points) to calculate the gradient\n\nLarger batches means you can trust your gradient more and you can use larger learning rate.\n\n\nIf you use 32 data points you can parallelize gradient computation over 32 processors.\n\n## Overfitting\n\n\n## Regularization\n\nTo avoid overfitting.\n\n### Dropout\n\nsets some activation neurons to 0. Forces the network to learn a different pathway. Very power technique as it makes the model that does not rely too much on a fixed set of weights.\n\n\nDropout nodes would not have any update, no gradient to compute.\n\n### Early stopping\n\nStop the training model early.\n\nTraining loss always go down.\n\n\n![alt text](./asstes/images/early_stopping.png)\n\n**In practice you can start plotting this curve and decide when to early stop!**\n\nIdeal Difference between train and test dataset is to be 0. Then you will not know when to stop. This usually happens in Large Language Models. The dataset is so big that the model itself finds it hard to memorize. So the difference between train and test will be almost always 0.\n\n**Language models usually does not the classical overfitting problems.**\n\n\n# Lecture 2: Recurrent Neural Networks, Transformers, and Attention\n\n\n## Deep Sequence Modeling\n\nHow do we model time series or sequential data in Neural Networks?\n\n**Naive**. We can have our standard feed-forward neural network and put input output $(x_0,y_0),(x_1,y_2)...(x_t,y_t)$\n\n![alt text](./assets/images/naive_sequence_modeling.png)\n\nThis approach does not take the sequence info, taking past history into the future.\n\n\n## RNN - Recurrent Neural Networks\n\n\nWe will pass sequentially **hidden state** of the network. Hidden state is a vector that is passed.\n\n![alt text](./assets/images/rnn_from_scratch.png)\n\n![alt text](./assets/images/rnn_computational_graph.png)\n\n\n## Sequence Modeling Requirements\n\n- Handle **variable-length** sequences. Sentences can be short or long. It is not fixed like image where it had high vs width pixels length\n- Handle **long-term** dependency. You can short or long term dependency. Something at the very beginning might dictate the end of the sequence\n- Maintain information about **order**\n- **Share parameters** across sequence\n\nRNNs can handle the above requirements - though it needs a few upgrades. RNN are in the core of Sequence Modeling\n\nThese requirements show why Sequence modeling is rich and complex.\n\n## Example. Predict the next word.\n\n1. We need to represent language to a neural network. We need to vectorize the words as numbers.\n- Encode language to neural network\n- Embedding: transforming words to vectors\n- Vocabulary -> Indexing -> Embedding (one-hot-embedding)\n\nEMBEDDING = VECTORIZATION of words\n\n![alt text](./assets/images/embedding.png)\n\n\nlearned embedding = lower dimensional representation of language\n\n## Backpropagation through time.\n\n\nCarry partial derivatives from errors late in the sequence to the very beginning. You will have to multiply matrices multiple times and multiply derivatives many times.\n\n![alt text](./assets/images/back_through_time.png)\n\n\n- if there are many values > 1 you might have **exploding gradient**\n- if there are many values < 1 you might have **vanishing gradient**\n\n\nThis problem exists in very deep feed forward neural networks. Having many layers mean you will multiply the partial derivatives many time.\n\n\n\n### Unstable gradients makes it hard to lear long term dependencies.\n\nWe cannot pass information from late time steps into initial time steps to promote/update the weights.\n\nCurrent research upgrades RNN-s to be able to tackle this.\n\n\n## LSTM, GRU\n\nIdes: use **gates** to selectively add and remove information passed through hidden states. LSTM (long-short term memory) network uses gate cells to pass information throughout many time steps\n\n\n## Limitations of RNNs\n\n- slow, no parallelization - need to compute derivatives sequentially\n- rnn state (hidden state $h_{t}$) is of fixed size. Encoding bottleneck - we have cap on the information we can keep.\n- not long term memory\n\ntime-step by time-step processing.. brings this bottleneck\n\n\nCan we eliminate the need for recurrence? Squash all inputs into one vector and put it into one network. The naive approach does not work because it does not carry any time information. We've destroyed any notion of order.\n\n**Idea:** Can we define a way to identify the import parts of a sequence and model out the \n\n\nPaper: [Attention is all you need](https://arxiv.org/pdf/1706.03762)\n\nThis landmark paper defines what is a transformer.\n\n**GPT:** The T stands for transformer.\n\nAttention: when we look at an image we do not go pixel by pixel and look which are the parts that we attend to.\n\nFinding the most important pixels is just a simple search.s\n\n\n### Transformers core idea\n\nGoal: Search and attend to the most important features in an input.\n\n1. Encode input (positional encoding) - didn't go into many details into that\n2. Compute query, key, value. Use three weight matrices to get each of those. {key:value} are features\n3. Compute similarity between query and all keys (dot product divided by scaling)\n4. Compute **attention weighting**\n5. Extract values from important features\n\nAttention is the building block of transformers.\n\n\n![alt text](./assets/images/transformers.png)\n\nSelf-attention to handle sequence modeling\n\nSelf-attention is the basis of many larne language models.\n\n\n\n\n","n":0.029}}},{"i":195,"$":{"0":{"v":"Business","n":1},"1":{"v":"\n# Semiconductor Industry\n\n600 billion industry in 2021, projected to become a trillion dollar industry in 2030.\n\nTop 5 Countries by Semiconductor Production\n\n1. Taiwan (TSMC), Taiwan Semiconductor Manuyfacturing Company account for 50% of the world's semiconductor production. They produce for Apple, AMD, Nvidia, Qualcomm, and many more. This is known as the **foundry** model for electronics manufacturing.\n2. South Korea (Samsung) both IMD(Integrated Devices Manufacturer, i.e. for themselve) and foundry. SK has 70+ fabrication plants, accounting for 15% world production.\n3. Japan\n4. United States - 12% of world production. However, US-based companies own about 46% of the world's semiconductor market. They produce in other countries.\n5. China\n\n\nIn 2023: \n\nGovernments across the rich world offering subsidies worth $400bn in the coming years to boost capacity.\n\n\n\n# Kia\n\nOne of the few examples goverment can help make business. It is example of government's “industrial policy”, a sometimes poorly defined concept that encompasses everything from tax breaks to training, gives some hope to the boosters. \n\n- South Korea's car manifacturer, backed by the governement.\n- opened a car plant near Atlanta in 2009\n","n":0.076}}},{"i":196,"$":{"0":{"v":"Books","n":1}}},{"i":197,"$":{"0":{"v":"Engineering","n":1}}},{"i":198,"$":{"0":{"v":"SCIP","n":1},"1":{"v":"Structure and Interpretation of Computer Programs - it's a classic book using Lisp to teach programming concepts. If you really want to understand how programming works, this is the book to read. Berkley has a Python version/course too.\n\nThey develop a language from scratch.","n":0.152}}},{"i":199,"$":{"0":{"v":"Тънкото изкуство да не ти пука","n":0.408},"1":{"v":"# 1. Омагьосаният кръг на ада\n\n'Чувстваш се притеснен и се притеснява затова, че си притеснен и т.н. Ядосваш се, после се ядосваш, че се ядосваш и т.н.'\n\nСтъпки за излизане от кръга:\n1. Признайте, че се притеснявате. Приеми слабостите си.\n2. Действайте. Не се опитвайте да се отървете от притесненията си, ами действайте въпреки тях.\n\nКакво означава да не ти пука?\n\n- не означава да си безразличен\n- означава да погледнеш проблемите право в очите и да действаш независимо от тях\n\nВинаги ще има за нещо, което да ти пука. Въпросът е да избереш добре за какво да ти пука. За да не ти пука за пречките и трудностите, трябва да ти пука за нещо по-важно от тях.\n\n**Търсенето на позитивни преживяваня е само посебе си негативно преживяване. Преодоляването на негативни преживяваня е положително преживяване.**\n\n# 2. Щастието е проблем\n\nВсеки знае какво иска: хубаво семейство, успешна кариера и т.н.\n\nНо по-важния въпрос е какво си готов да пострадаш за това?\n\n**Избери си борбата! Избери проблемите, които искаш да решиш.**\n\n**Негативните емоции са призив към действие. Те те карат да действаш, за да промениш нещата.**\n\n**Проблемите са константа в живота ти. Щастието идва с решаването им**\n\nПриблемите са неизбежни и рядно имаш контрол над тях. Това над, което имаш контрол е как ще реагираш на тях.\n\n# 3. Не сте специални\n\n\nНяма такова нещо като личен проблем. Това, което ти преживяваш като проблем е преживяно от милиони други хора.\n\nНе си специален, не си уникален. Може би си единствен (unique), but nothing special.\n\nIt's ok to be average... Потокът от информация, който достига до тебе е само с изключения...\n\nСамочувствието не се измерва в положителните моменти, а в негативните моменти.\n\n\n# 4. Страданието и твоите ценности\n\nЦенностите ти определят нещата, за които ще страдаш.\n\n\n**Трите нива на самоосъзнаване:**\n1. Какво ме кара да съм юастлив/нещастен? Кои са нещата/случките, които ме карат да се чувствам по определен начин.\n2. Защо ме карат да се чувстват по този начин?\n3. Кои са ценностите ми, които ме карат да се чувствам по този начин.\n\n\n\n\n","n":0.056}}},{"i":200,"$":{"0":{"v":"Психология на убеждаването","n":0.577},"1":{"v":"\nВлиянието. Психология на убеждаването\nРобърт Чалдини","n":0.447}}},{"i":201,"$":{"0":{"v":"Какво би направил Кейнс","n":0.5},"1":{"v":"\n# Глава 1. Житейски Избор\n\n## Редно ли е да съм егоист?\nХипотеза: Ако всеки участник в едно общество максимизира собственото си щастие, това максимизира/увеличава значително общото благоденствие.  Тази хипотеза е вярна ако общото благоденствие $f(x)$ has no interaction terms. Тоест ако щастието на един човек, намалява щастието на друг човек.\n\nАко купуваме най-ефтините стоки, възнаграждаваме най-ефективните фирми.Успешните фирми наемат още хора\nза работа и правят клиентите щастливи. Всички печелят?\n\nПреследването на само и единствено личния интерес не взима в предвид ефектът върху околните. Ако строя сграда, трябва ли да предоставя достатъчно паркоместа?\n\nСвободния пазар е тясно свързан с идеята за преследване на личен ин терес и един от рисковете е, че може да се стигне до монопол.\n\nТрябва ли държавата да се намеси в горните два случая?\n\nЗа паркоместата, мисля че не трябва. Ако има голям недостиг на паркоместа, те ще станат по-скъпи и строителите сами ще почнат да продават повече парко места. Или ще се появят повече паргинги. Това рещение има предполага, че няма нелегално паркиране. Тоест държавата вместо да кара строителите да правят паркинги, трябва да глобява нелегално паркирали автомобили.\n\nМонопол (краен случай на несправедливо разпределение на ресурсите). Държавна намеса вероятно е нужна тук. Тя обаче не трябва да ограничава просперитета на монополиста, а трябва да насърчава малки компании да станат конкурентноспособни. Монополиста реално е супер успешна компания. Не трябва да има КЗК, а трябва да има Комиция по насърчаване на конкуренцията.\n\n## Каква е тайната на щастието?\n\nДжеръми Бентам (утилитарист) казва, че животът е максимизиране на удоволствие и минимизиране на болка. До голяма степен съм съгласен с това и съвпада с начина, по който аз взимам всекидневни решения.\n\nЕдинство трябва да се добави щастието от поемане на рискове и неочаквани приключения/изненади.\n\n- вредни навици/изкушения\n\nДавай си сметка за моментното увеличение на щастгие срещу удовлетворението, което ще получиш в бъдеще ако устоиш на изкушението\n\n- ролята на очакването\nНещаствието идва тогава когато очакванията не съвпадат с реалността.\n\n\n# Глава 2. Потребителят\n\n## Как да управляваме най-добре общите ресурси?\nИмаш 3 варианта:\n- саморегулация (трябват ти читави хора и свръх прозрачност. хората гледайки други хора е най-силния надзор)\n- държавна регулация\n- защита на частната собственост (общите ресурси се разделят на малки ресурси за всеки човек)\n\nМилтън Фридман набляга върху минимална намеса от страна на правителството освен в защита на частната собственост.\n\n\n## Да си платя ли за да мина по-напред?\nБординг с приоритет звучи ок. Купуване на ВИП билет в увеселителен парк звучи ок. Ами трябва ли да има същата услуга за чакане на донор на бъбрек и други здравни грижи?\n\nПлащането за донорство на бъбрек би увеличило предлагането и би ни позволило да предредим опашката. Плащането за тази услуга ще увеличи крайния брой дарени бъбреци. Ако искаме да максимизираме брой дарени бъбреци трябва да има такава услуга.\n\n\n## Фондове и държавни помощи\nЕвропейските фондове вредят на държавите, които получават фондовете. Субсидиите са начин бедните хора от богати държави да помагат на богати корумпирани хора в бедни държави. Те подхранват корупцията в бедните държави. Най-добриият начин богати държави да помагат на бедни е да купуват тяхните стоки (като Китай и Индия, където през последните години е имало най-голям разтеж).\n\nВсяко преразпределение на богатство от страна на държава е начин да се вземт пари от работещи хора и да се дадат на неработещи/по-малко работещи хора.\n\n- благотворителност\nСкъперниците всъщност са супер щедри хора. Те бачкат, за да искарат 1 лев и после не го консумират - истински алтруизъм!\n\n\n# Глава 3. Работа\n\n## Струва ли си да ме повишат ако трябва да работя повече?\n\nУвеличение на заплатата носи краткосрочно щастие. Постигането на едно повишение скоро предизвиква желание за следващо.\n\nПрестижът,подобряването на положението в работата и социалната класа носят по-дългосрочно щастие от заплатата.\n\nКоличеството свободно време също е фактор, който опредделя твоето щастие.\n\nРаботата ме кара да се чувствам полезен и това е коетото ми носи щастие.\n\n\n## Как мога да получа увеличение на заплатата?\n\nФирмите знаят, че изискват много усилия да си търсиш нова работа - подготовка, време, интервюта, стрес. Затова мога т да си позволят да ти дават минималното, което би те задържало при тях, за да не полагаш тези допълнителни усилия.\n\nФактори за искане на увеличение на заплата:\n1. Трябва да си го поискаш.\n2. Доколко са зависими от теб и дали те ще трябва да положат много усиля да те заместят ако напуснеш.\n3. Количество свършена работа\n4. Развитие на умения и клалификация. Преди знаех това и ми плащахте $х$, сега знам друго и искам $х+а$\n5. Външни фактори - други оферти, инфлация, вдигане на всички заплати на пазара\n\n\n\n# Глава 4. Финанси\n\n\n## Как да спечелия от фондовия пазар?\nХипитезата за ефективния пазар гласи, че цената на акциите е справедлива, затова изборът на печеливша акция е въпрос на късмет.\n\nДжон Кейнс: 'Ирационалността на пазара може да продължи много по-дълго от вашата платежоспособност.'\n\nПовечето инвеститори надценяват активите с нисък риск и ниска възвращаемост като облигации и акции на големи компании.\n\nЗатова начинът да спечелиш е като проучваш малки подценявани компании с потенциал за разтеж.\n\nЗа да имаш печалби или инвестираш в конкретни компании, мислейки си че имаш някаква информация, която другите нямат или все още не са попаднали на нея (RESEARCH на компанията).\n\nКупуваш ефтино фондови индекси и чакаш.\n\n\n## Как да пребория инфлацията?\n\nЧрез продължителния процес на инфлация държавата може да конфискува, тайно и незабелязано, съществена част от състоянието на своите граждани.\n\nИнфлацията не насърчава спестяването, а активните действия на пазара на имоти. Цените на имотите обикновенно растат по време на периоди на инфлация и осигуряват запас от богатство.\n\nЕдинствената група, облаготелствала се от висока инфлация, са тези, които притежават физически активи и физически капитал, като индустриалците.\n\nАко фирмите вдигат цените си по-бързо от заплатите, за тях това е възможност да увеличат печалбата си. В периоди на инфлация купувай акции на такива фирми.\n\nВ периоди на инфлация ако имаш заеми, това е хубаво. Тези заеми се обезценяват.\n\nМилтън Фридман: 'Макар, че инфлацията временно увеличава печалбите, в дългосрочен план вреди на икономическата дейност.'\n\n\n# Глава 5. Политика\n\n## Трябва ли да се намалят данъците?\n\nДържавните разходи за неефективни, следователно намалвяването на данъците оставя парите в хората, които ще харчат разумни собствените си пари.\n\n100% данъци ще демотивира напълно хората да работят повече. Разстящи данъци демотивират хората да работят.\n\nНивото на данъците трябва да се определя оттова колко харчи държавата, а не оттова колко изкарват хората. Прогресивните данъци се фокусират основно върху доходите на хората, ако изкарваш повече, плащаш повече. Тоест създаваш повече стойност и трябва да си плащаш за това?? Работиш повече и си плащаш за това? Логика, няма...\n\nДанъците трбява да се определят от разходите на държата за общите блага на обществото. Свободния пазар се справя зле с общите блага като пътища, здравеопазване, национална сигурност.\n\n\n## Да бойкотирам ли продукти, произведени в мизерни фабрики експлоатиращи труда на хората?\n\nБойкотирането може да влоши положението в тези фабрики - загува на работни места, още повече експлоатация, за да са още по-евтини стоките. Често, в тези държави работата във фабриката е по-добра от другите алтернативи.\n\n## Трябва ли да се дават субсидии на местни фирми пред фалит? Спасяване на работни места?\n\nПровалът на неспособните фирми е най-хубавото нещо, което може да се случи за задравяването на една икономика. В дългосрочен план трябва да останат само добри компании с високи заплати и висока добавена стойност. Ще се появят по-добри компании с по-добре платени работни места. Помагането на фирми пред фалит от страна на държавата е черна дупка.\n\n**Няма по-трайно нещо от временна държавна програма.**\n\nБанките може би не заслужават спасителните почси, но често алтернативата е много по-лоша.\n\nПроблема на субсидиите е, че е много видим за хората получаващи ги, и невидим и разпилян за всички останали данъкоплатци. Съответно хората, които взимат субсидии активно ще си ги търсят, а ние всички, които даваме незабелязано го правим.\n\nСубсидиите реално подпомагат неефективни фирми, за сметка на всички останали работещи хора.\n\n## Трябва ли да има вносни мита.\n\nПротекционизъмът защитава местната протекция и облага с данъци вноса. Това е против идеята за свободна търговия. Идеята на вносни мита е да защити новозараждащи се индустрии в дадена развиваща се държава.\n\nНо това е минус за всички в държавата. Протекционизмът спира добри компании да навлезнат в нашия пазар, да създават работни места и да продават успешните си продукти. Конкуренцията идваща от свободния пазар прави компаниите по-ефективни, които се борят за своите клиенти. Клиентите стават по-желани и ухажвани.\n\nКонкуренцията е най-силния начин да се подобри жизнения стандард на потребителите.\n\n## Трябва ли да легализираме наркотиците?\n\nМилтън Фридман казва, че забраната на наркотици само води до увеличаване на черния пазар. Забраната също така вдига цените на наркотиците.\n\nЗабрана на наркотици:\n- високи цени\n- разширяване на черен пазар\n- некачествени/вредни наркотици\nЗабранените наркотици влизат в затворен цикъл.Лолкото повече нелегални наркотици хваша държавата, толкова по-скъпи стават те.\n\nЗащо не трябва да се забраняват:\n- за да може да се следи реална статистика\n- ще се плащат данъци\n- ще станат по евтини\n\nВсичко това звучи правдоподобно, обаче...\n\nПо-време на сухата депресия, се е пиело по-малко алкохол? Тоест като е забранено се употребява по-малко.\n\nСпоред мен за всеки наркотик трябва да се вдигне забраната само, ако сме сигурни, че това ще доведе до по-малко употребяване на самия наркотик. Легални наркотици = по-евтини наркотици = по-достъпни наркотици.\n\n## Държавния дълг трябва ли да ме притеснява?\n\nДържавния дълг е природния лек по време на рецесия, когато производството намалява, стоките и услугите стават по-малко. Търсенето се свива, тъй като хората започвт да спестяват. Държавния дълг може да стимулира икономиката и да се въртят повече пари в нея.\n\nДългът обаче е зло в дългосрочен план. Зло, което неименуемо ще доведе до по-високи данъци. Западните държави имат  големи дългове и високи данъци.\n\nДържавите без собствена валута (еврото) не мога да теглят дълг обаче. Тегленето на дълг = продаване на облигации от централната банка. Държави без собствена валута са уязвими от бягство от капитал. По време на кризата в Гърция, много хора а продавали импоти и активи в Гърця и са харчили еврата си в Западна Европа.\n\n## Разстящото неравенство проблем ли е?\n\nАко неравенството е защото богатите стават по-богати много бързо, а бедните забогатяват по-бавно, то не епроблем. Нали затова едните са богати а другите не са.\nНеравенството е необходима черта на икономиката. Тя стимулира хората да стават по-добри и да се борят да стават по-успешни и по-богати.\n\nНеравенството в догходите е голямо, но неравенството в консумацията и потреблението е значително по-малка. Богатите често пестят, а по-бедните работници имат високо потребление.\n\nПомощите към бедните е сбъркана политика, която вреди на бедните и премахва стимулите да работят и насърчава зависимостта от социални поомощи.\n\nНеравенството породено от наследство и нераномерно разпределена политическа власт, корупция е проблем.\n\n## Здравеопазването държавно или частно трябва да бъде?\n\nМилтон Фридман: Ако здравеопазването е частно, то хората ще внимават повече и ще избират по-ефтиното и по-качественото такова, насърчавайки по-добрите лекари и болници.\n\nОбаче, дори ако държавата осигурява здравеопазване, това няма да накара хората да променят поведението си. Става въпрос за тяхното здраве...\n\nСвободния пазар обикновено е най-ефективния начин за разпределение на ограничени ресурси. Този пазарен принцип обаче не може да се приложи към обществени услуги като здравеопазване.\n\nЗдравеопазването е социалистически пазар. Всички сме равни пред болестта. Болестите са равномерно разпределени в обществото. Бедни и богати се разболяват с равна вероятност. Социалистическо търсене изисква социалистическо предлагане.\n\nПовечето пазари са капиталистически обаче, затова трябва да има капиталистическо предлагане за тях.\n\nПол Кругман: Идеалът за свободния пазар за конкурентно здравеопазване е нереално идея. Когато някой е болен, последното нещо, от което се нуждаете, е да търсите болницата с най-ефективни разходи.","n":0.024}}},{"i":202,"$":{"0":{"v":"Как да печелим приятели и влияем на другите","n":0.354},"1":{"v":"# Думички\nбезапелационен, самоусъвършенстване, сензационен, оскърбления, предотвратяват, петдоларова бакнота, хампарцумян\n\n# Повтарянки\n\nЩастието ми и чувството, че съм ценен зависят до голяма степен от умението ми да общувам с хората.\n\nОбщуването с хората е упражнение, което трябва да тренираш всеки ден. То е като мускул, който ако е трениран, тогава е силен и не се забравя.\n\nНепрекъснато си повтаряй огромните възможности за напредък.\n\nЦелта ти е примципите в тази книга да ти станат автоматизиран навик. Това ще се получи само ако постоянно си ги повтаряш\n\n\n\n# Принцип 1: Не критикувай, не съди и недоволствай\n\nКритиката е безполезна, защото обикновено кара човек да се защитава. Тя е опасна, защото наранява скъпата гордост на всекиму, а човек с наранена гордост ще направи всичко възможно, за да се защити дори да знае, че има напълно грешна позиция. Критиката провикира критикувания да отстоява правотата си, дори когато не е прав.\n\nКолкото силно жадува човек за одобрение, толкави силно се отвращаваме от неодобрение.\n\nКогато човек общува с хора, той има работа не с трезвомислещи същества, а със същества изтъкани от емоции и предубеждения, мотивирани от гордост и суета.\n\nФранклин: \"Не говоря лошо за никого и казвам всичко хубаво, което зная за всекиго\"\n\n\n# Принцип 2: Похвалата ти трябва да бъде искрена\n\nЕдинственият начин на света да накараш някого да направи нещо е като го накараш сам да поиска да го направи. Единственият начин, по който можеш да накараш някого да направи нещо, е като му дадеш онова, което иска. Кое е това?\n\nНай-дълбокият стремеж в природата на човека е желанието да бъдем важна личност. Това даже не е точно желание, не е стремеж, дори не е мечта, това е огромна наутолима жажда да бъдеш важен. **Това е един неутолим, постоянен глад и хората, които наистина го удовлетворяват, са рядкост, но държат другите в ръцете си.**\n\n\nДори прочути, успешни и вече важни хора се борят да станат още по значими. Редица президенти искат да имат свои статуи и улици на свое име.\n\nВсеки човек иска да получи доказателства за своята изключителност.\n\nУмението да предизвикваш ентусиазъм у хората може да бъде най-голямото ти богатство. Одобрявай от сърце и бъди щедър в похвалите. Начинът да развиеш най-доброто у един човек е като го цениш и насърчаваш ИСКРЕНО.\n\nЛаскателството е друго нещо. То е неискрено! То ще ти навреди повече отколкото ще ти помогне. То е фалшива. Похвалата идва от сърцето, ласкателството от устата. Ласкателството е евтина похвала. Ако ласкателството работеше, тогава всеки щеше да е специалист по човешки взаимоотношения.\n\nОпитай се да видиш хубавите страни у другия. Одобрявай от сърже и бъди щедър в похвалите.\n\n\n# Принцип 3: Събудете у другия силно желание\n\n\nНа кукичката трябва да се закачва онова, което е по вкуса на рибата.\n\nЗатова единственият възможен начин да влияете на другите е, като говорите за онова, което те искат, и им покажете как да го получат.\n\nВсичко, което човек прави от деня на раждането досега, го прави, защото иска нещо. Човек като дарява, го прави, защото иска да се почувства добър.\n\nПреди да поискаш от някого да направи нещо, първи си задай въпроса: \"Как мога да накарам този човек сам да поиска да направи това нещо?\"\n\n\"Как мога да свържа това, което аз искам с това, което другия иска?\"\n\nАко въобще има тайна на успеха, тя е в умението да разбереш гледната точка на другия, да се опиташ да видиш нещата от неговия ъгъл, а след това от своя собствен.\n\nИстинското изкуството да поискаш нещо от някого се получава тогава когато споненеш в началото това, което иска другия и му покажеш как може да го получи.\n\nКлиентите обичат да чувстват, че купуват, а не че им продават.\n\n\n# Принцип 4: Проявявайте искрен интерес към другия","n":0.041}}},{"i":203,"$":{"0":{"v":"Всичко може да се договори","n":0.447},"1":{"v":"\n**Глава 1**\n\nМагаре = Когато блажено не осъзнавате възможностите. Там където преобладава невежеството, се утвърждават глупавите избори при преговарянето.\n\nОфца =  Когато другите твърде лесно ви принуждават да правите определен избор, сте като офце на заколение.\n\nЛисица = Бъдете толкова непочтени, колкото искате другите да са непочтени с вас.\n\nБухъл = Когато изборът ви при преговорите показва, че проумявате дългосрочната полза от развиване на **искрени** взаимоотношения, за да получите резултатите от преговорите, които наистина заслужавате.\n\nВсички преговарящи трябва да внимават: Много Бухали са прикрити Лисици!\n\n\n**Въпрос 1**. Когато сте изпревени пред тру ден опонент, е по-добре да отстъпите за нещо дребно, за да породите добра воля.\n- дребността на едно нещо се определя от това колко ценно е това нещо за другата страна, не само за теб!\n- никога не отказвайте и милиметър - изтъргувайте го!\n- никога не отстъпвай нищо, без да получиш нещо в замяна\n\nОтговор: невярно\n\n---\n**Глава 2**\n\nВъзроди отдавна забравените умения за преговаряне! Децата са безмилостни преговарящи. Те знаят какво искат, нямат срам,\nне чувстват срам и вина. Вярват, че родителите са бездъннин ями за лакомства.\n\n\n**Въпрос 2**. Имаме ли богат опит в преговорите? Да! Децата са най-добрите преговарящи - винаги получават това, което искат.\n\n**Въпрос 3**. Преговарящият се интересува само от това да спечели.\n\nОфца: вярно, Лисица: може би, Бухъл: невярно. Печелене != успяване. Ако спечелиш веднъж, а другия загуби, дали ще иска\nдругата страна да играе пак с теб?\n\nМного по-мъдро е да се стремиш двете страни да успявате заедно, вместо да печелиш.\n\nЖивотът не е нито само сладолед нито само зеле. Изкуството на преговарящия е да ги балансира. Онова, което се възприема\nкато зеле от вас е сладолед за другия преговарящ и обратното. Изкуството на сделката е да откриете какво иска другият и ако го постигнете, знаете, че сделката е готова - стига да я желаете.\n\n**Въпрос 4** Можем да преговаряме само ако постигнем съгласие по фактите.\n\nМагаре: да, Бугъл не, Офца: Зависи какво се има предвид под факти. Несъгласие върху фактите поражда възможност за преговори.\n\n\n**Въпрос 5** Ако те не приемат моите разумни предложения, мога и да си тръгна.\n\nЛисица: Не!, Магаре: да, Офца: не. Тръгването от преговори е сигурния начин да получиш нищо!\n\nАко трябва да си тръгвате, не го правете, без да посочите кога ще се върнете, иначе блестящата ви тактика на тръгване ще\nрикошира върху надежността ви, когато се върнете.\n\n\n---\n**Глава 3**\n\nНай-лошото нещо, което можеш дяа направиш на един преговарящ е да му приемеш първата оферта!\n\nПриемането на първата оферта превръща сделката от добър пазарлък в съмнителна сделка.\n\nЖените никога не приемат първата оферта на мъжа!\n\nСделка, за която някой се е потрудил, е сделка, с която той се чувства по-щастлив.\n\nВнимавай с първата оферта (по презумция винаги ще ти я оспорят). Започни отдалеч.\n\nНамаление от 15% = Плащаш 85% от цената.\n\n---\n**Глава 4** Case study. Пицария, значи искаш да купиш моя бизнес?\n\nНай-добрите критерии за началната цена са, че тя трябва да е достоверна, защитима и реалистична.\n\nЗащо другата страна продава?\n\n---\n\n**Глава 5** Защо не можеш да договориш оплакване.\n\nОплакването е човешко. Няма лошо в него. Хората често разочароват.\n\nНедей просто се оплаква. Договори начин за коригиране на дефектите!\n\nЧовек лесно може да потъне в сиклично оплакване. То обаче не дава решение на проблема.\n\nОплакването се фокусира върху некомпетеността на другата страна. Атакувай ги и ще се зашитават. Предлагането на решение:\n\n- е поемане на инициатива и по-вероятно ще отчете твоите интереси а не техните\n- преговорите ще са върху твоето решение и няма да затъвате в спорове за легитимността на оплакването ти\n\nБухалите не прото формулират оплакванията си - те договарят практически решения!                       ","n":0.043}}},{"i":204,"$":{"0":{"v":"When Genius Failed - The Rise and Fall of LTCM","n":0.316}}},{"i":205,"$":{"0":{"v":"Trustworthy Online Controlled Experiments","n":0.5},"1":{"v":"\n# 1. Intro and motivation\n\n*One accurate measurement is worth more than a thousand expert opinions*\n\nSimplest controlled experiments: Control(existing system) vs Treatment(existing system with feature X) (A/B test)\n\n**OEC = Overall Evaluation Criterion** is a quantitative measure of the experiment's objective.\n\nExamples:\n- active days per user\n- sessions per user\n- successful sessions\n- time to success\n- ads revenue\n\n**OEC should be measurable in short periods and drive long-term strategic objectives.**\n\nExample: Profit is not good OEC metric.  Customer lifetime value is a strategically powerfull OEC.\n\nMetrics should meet key characteristics of strategy and not be gameable.\n\nOEC is the perfect mechanism to make the strategy explicit and to align what features ship with the strategy.\n\nIt is about matching top-down-directed perspectives with bottom-up tasks.\n\nGlossary of terms:\n\n**Parameter** in an experiment is a controllable variable (e.g font size). Parameter are assigned *values* also known as *levels*.\n\nIn online world common use **univariate** (one parameter) designs with multiple values (A/B/C/D tests).\n\n**Variant** are the populations being tested - A is Control population, B is Treatment population\n\n**Randomization Unit** is a process to map units (users) to their variants. Need:\n- **persistency** same user should experience consistenly the same variant\n- **independence** assigning users to one variant should not affect the assignment probabilities of oher users \n\nNo factor should be allowed to influence variant assighment.\n\n**Correlation foes not imply causality!**\n\nExample: Microsoft Office 365 observes lower churn rate by users who see more error messages. Does not mean Microsoft should show more errors.\n\nThe reason for this corelation is simply **usage**. Users who use MSOffice more see more messages and have lower churn rate.\n\nControlled experiments give scientific framework to:\n- establish causality with high probability\n- detect small changes\n- detect unexpected changes\n\n\n**Ingredients for Controlled Experiments:**\n\n1. Enough experimental units (users) divided into variants randomly\n2. Key metrics, OEC.\n3. Cahnges are easy made (e.g software changes in algorithms vs aviation engineering system change)\n\nAgile + Controlled experiments + MVP is good combination\n\nUseful concept to have in mind EVI (expected value of information). Running a failed MVP has value.\n\nExperiments evauluate ideas (people could be very poor at this)\n\nExperiments usually make small many improvements over time. 10% improvemnt for 5% of users is 0.5% overall improvement\n\nWinning is done inch by inch.\n\nAlways have a protfolio of ideas:\n- most should optimize near the current location (maximize your current hill)\n- but a few should be radical (whether we can move to a bigger hill)\n\nBig jumps to other hills (big site redesigns) usually fail. It is a high risk high reward endevour.\n\nTwo things to consider when making big jumps:\n\n- The duration of the experiment. Need to overcome **primacy** effect (users are primed to the old design). Run longer the experiment.\n- The number of ideas tested. In big jumps you make lots of changes. Some ideas might be really bad and cancel the really good ideas (which you would not see that are good).\n\n\n\n# 2. Running and analysing experiments\n\nThis chapter contains an example of setting up, running and analysing results of an experiment\n\nExperiment **setup/design**: \n\n**Hypothesis**: Adding a coupon code field to the checkout page will degrade revenue.\n\n**OEC**: Profit itself is bad metric as it depends on the number of users in each variant. Revenue-per-user is better.\n\nWhat should be in the denominator?\n\n- all users: valid but would be quite noise. Lots of users will never initiate checkout\n- only users who complete purchase process. Wrong. The more users purchase the lower metric??\n- only users who start the purchase process. That's the most accurate denominator. Includes all potentially affected users.\n\n**Refined hypothesis:** Adding a coupon code field to the checkout page will degrade revenue-per-users who start the purchase process.\n\nDecide what **statistical significance** p-value to use.\n\nStatistical significance means whether the observed result is by chance or not.\n\nHowever, in practice we need **practical significance** - does it make sense from business stand point to make the change?\n\npractical significance is usually larger than statistical significance. If you are on start-up you would like to have larger practical significance say 10% where as big businesses like Google with billions of revenue  the 0.2% change is practically significant\n\n**randomization unit** - usually these are users\n\n**size** - how large in size should be the population. Larges smaple size needed to detect smaller changes. alternatively you can reduce the variance in the data, e.g. use *purchase indicator* (1 if user buys 0 if not) instead of revenue as OEC and that would have lower variance data, detect easier.\n\ndetecting larger changes requires less data\n\nlower p-value rtequires larger data\n\nsize ~ variance, minimum detectable effect needed, p-value\n\n**power** of the test\n\n**time** - how long do we run thhe experiment\n\n- more time = more users\n- day-of-week effect (seasonality)\n- primacy and novelty effects\n\n\n**Results of experiment**\n\n- do sanity checks for trustworthiness of your experiment\n\nsome metrics should be the same in all variants, e.g. latency\n\nto catch errors looks at invariants between Treatment and Control datasets\n\n\n**Launch/no-launch**\n\nDesisions take into considerations the results of the experiment as well as broader context:\n\n- different metrics tradeoffs, e.g. if CPU utilization usage increases, is the cost of running the new service worth? \n- cost of launching the change (sometimes all the cost is in the setup experiment, sometimes there are add ons)\n- what is the downside of making the wrong decision/risks\n\n\nThe decisions are usually three way:\n\nLaunch, no-launch and continue testing.\n\n- statistical, practical significance -> launch\n- no statistical significance, no practical significance -> no launch\n- statistical significance, no practical significance -> no launch/continue testing\n\n# 3. Twyman's Law and Experimentation Trustworthiness\n\n**Twyman's Law: \"Any figure that looks interesting or different is usually wrong.\"**\n\nExtreme resultss are more likely to be result of an error:\n- in instrumentation (logging)\n- loss/duplication of data\n- computational error\n\nHypothesis testing depends on\n- significance level/value ($\\alpha = 0.05$, that's Type 1 error(false positives))\n- power (high power = low Type 2 error (false negatives)), high power = catch Alternative hypothesis if it is true \n- minimum detectable effect\n- sample size\n\n**Mistakes:**\n\n**A common mistake is to assme that low significance value = no Treatment effect.** It migh be just that our test is **underpowered**.\n\nNeed to ensure you have enough power to detect a change of that magnitude or smaller.\n\n\n**p-value != probability of $H_{0}$ is true.** p-value is GIVEN $H_{0}$ how likely is we observe AS EXTREME VALUE AS the current test statistic value.\n\np-value computes on CONDITIONING\n\n**[Peeking at p-values](https://towardsdatascience.com/wish-tackles-peeking-with-always-valid-p-values-8a0782ac9654)**\n\nRunning online experiments and make conclusiong before reaching the sample size you need. Might lead to infalted false positive rates (underpowered tests).\n\n\nFix it using always valid p-values method\n\n**Multiple Hypothesis testing**\n\nMultiple hypothesis vs single Null\n\nBonferronni correction (conservative). Need it as high probability of false negatives (high probability of incorrectly rejecting the null)\n\n\nDuality between confidence intervals and p-values. For Null hypothesis that the threatment has no effect, a 95% CI containing a zero is equavalent to having significat p value 0.05%\n\n95% CI is referring to the fact that if you run the experiment many times and compute the CI, then 95% of the time you will catch the TRUE value.\n\n**Threat to internal Validity** - is your experiment trustworthy?\n\nCareful about:\n\n**SUTVA** (Stable Unit Treatment Value Assumption):\n- **spillover effect** (networks have this problem)\n- two-sided markets (lowering prices in Treatment might affect the Contol buy side)\n- shared resources (CPU, storage, cache)\n\n\n**Survivor bias**\n\n\n**SRM - Sample Ratio Mismatch**\n\nThe size of the variants must be close to the designed ratio! Good place to do sanity checks.\n\nSmall imbalances might change your result completely (reject null to not reject null)\n\nIn practice it is easy to have this wrong:\n\n- browser redirects (e.g. links can be send by certain users to other users etc. better use **server side mechanism**)\n- gathering data (the way you define stuff like clicks)\n- bots (automatic filtering)\n- residual or carryover effects from previous experiments\n- time-of-day effect (if you send mail campaign for control in working hours and treatment in non-working hours)\n\n\n**Threats to External Validity** - are your experiments generalizable\n\nGeneralizing across population is rarely applicable.\n\nThreats:\n- primacy effect (attached to the old)\n- novelty effect (excited for the shiny)\n\nCatch these effects by ploitting traffic over time.\n\n\nViewing metrics by segments!\n\n- market or country\n- device type\n- time of dayweek\n- user characteristcs\n\n\n**Analysis by segments can be wrong** - thats when you add rates and averages.\n\nFor two segments the OEC can increase for both but decrease overall due to migration from one segment to another.\n\naverage 20 sessions-per-user use feature F, average 10 users-per-session not use feature F\n\nIf users who use F have 15 sessions and stop using it, then both OES metrics would increase, overall would decrese.\n\n**Simpson's Paradox**\n\nRun experiment for two phases and in each the OEC increase. But overall it deacreases\n$\\dfrac{a}{b} < \\dfrac{A}[B], \\dfrac{c}{d} < \\dfrac{C}[D]$ but $\\dfrac{a+c}{b+d}<\\dfrac{A+C}{B+D}$\n\n\nGood data scientists are sceptic! Invoke Twyman's Law.\n\n# 4. Experimentation Platform and Culture\n\nPhases of experimation maturity models:\n\n- Crawl (1-10 experiments/year) - fundamental capabilities, instrumentation, data analytics/science\n- Walk (50 experiments/year) - validating your experiments + defining metrics\n- Run (250 experiments/year) - codifying your OEC, run experiments at scale\n- Fly (1000+ experiments/year) - do not launch anything before running an experiment, automation ,institutional memory, learning form past experiments, automation\n\nTo add experimentation in an organization need learship buy-in espessially in Crawl and Wal phases to define OEC and set up fundamentals\n\n\nInfrastructure:\n\n- Experiment definition, setup, management via UI, or API, store configurations\n- experiment deployment, both server and client side - variant assignment and parametrization\n- experiment instrumentation (logging, triggers, alerting, automation)\n- experiment analysis, aggregate, clean, statistical tests, p-values, power analysis, visualization\n\n*Iterations* - progressively roll out new features to many users (not all at once).\n\nScaling experimentation:\n- need multiple experiments and assign same user to many experiments (multi-layer method)\n- need to do lots of monitoring\n- deal with interactions between experiments\n\n\n\n# 5. Speed Matters\n\nMeasure impact of performance improvements by slowing down artifitially your feature and run A/B test.\n\nAssume that the metrc(e.g revenue) has linear dependence on the performance. This assumption is reaistic locally (for small performance improvements like miliseconds.)\n\n**Measuring Page Load time of search engine.**\n\n1. The user makes a request at time $T_0$ by typing a query into the browser.\n2. The request takes time to reach the server at time $T_{1}$, $T_{1}-T_{0}$ is super small and hard to measure.\n3. On receiving the request the server typically sends the first chunk of HTML at time $T_{2}$. It is often independent of the request (e.f. query or URL). It returns the header. navigation elemtns etc. It tells the user his request was received. User receives this chunk at time $T_{3}$.\n4. At time $T_{4}$ the server sends the rest of the page and the other chunks of HTML. At time $T_{5}$ user receives all chunks\n5. At time $T_{6}$, the browser fires the Onload event, indicating the page is ready. It makes a log request from the server. At time $T_{7}$ the server receives the log request (user clicks, scorlls, hovers).\n\nThe Page load time is $T_{0}-T_{6}$ which we estimate with $T_{1}-T_{7}$ (stuff happening on server side).\n\n\n**Slowdown experiment design**\n\nBest is to slowdown the experiment when the server finishes computing Chunk2 which is the URL dependent HTML - as if it took longer to compute it.\n\n\nAbove is the real performance. User experience **perceived performance** - denoting the idea that users stat to interpret the page once enough of it is showing. E.g. can measure the time to first result instead of displaying all results.\n\n# 6. Organizational Metrics\n\nObjectives and Key Results ([OKRs](https://github.com/joelparkerhenderson/objectives-and-key-results)).\n\nObjecties represent long-term goals and Key results short-term goals.\n\nObjectives are ambitious, and should feel somewhat uncomfortable\n\nKey Results are measurable; they should be easy to grade with a number\n\nTypes of metrics:\n\n- **Goal metrics** reflect what the organization ultimately cares about. Need to articulate your goal in words. Goal metrics are proxies to what you really want to achieve. \n\n- **Driver metrics** tend to be short-term faster-moving and more sensitive metrics than goal metrics. HEART(HAppiness, Engagement, Adoption, Retention and Task Success). PIRATE framework (AARRR! Acquisition, Retention, Referral, Revenue)., user funnel.\n\n- **Guardrail metrics** guard agains violated assumptions and come in two types: metrics that protect the busness and that assess the trustworthiness of experiments. These metrics make sure there is balance in all metrics.\n\nThe same metric may play different role in different teams. Some teams use latency and performance as guardrail metric. That is if you ship new feature and have revenue as driver metric you want to make sure you don't loose a lot on performance. Infrastructure teams might have these swapped.\n\n**Asset vs engagement metrics:** asset metrics are static (number of users), engagement are dynamic and measure the valuea a user receive\n\n**Business vs operational metrics:** business metrics are (daily average users (DAU), revenue per user) track busniess health. Operational are (queries per second) track operational concerns\n\nYou want your goal metrics to be **simple and stable**.\n\nYou want your driver metrics to be:\n- alligned with the goal\n- actionable and relevant\n- sensitive\n- resistant to gaming\n\n**Always remember that metrics are proxies!**, e.g. simply driving high CTR might result in clickbaits.\n\n\nexample metrics: bounce rate(ratio of users went back - away from your content)\n\nexample gurdrail metrics(organizational):\n- HTML responcse size per page\n- Client crashes\n- latency\n\nthese are metrics that usually most teams should NOT affect.\n\n# 7. Metrics for experimentation and the OEC\n\nOrganizational metrics are *goal, driver and guardrail metrics.*\n\nMetrics for experimentation are specific to the experiments themself. They need to be \n- measurable (post-purcahse satisfaction is hard to measure)\n- attributable: metrics values shoule be able to attribute to the variant (measuring high rates of app crashes requires being able to attribute app crash to treatment) \n- sensitive and timely: metrics should be able to capture deltas. Sesitivity depends on the varaince, the effect size (minimum detectable effect), number of data points\n\n\nOEC is usually a single metric which weights in several key metrics.\n\nAlternatively you can use several metrics and decide to launch/not-launch:\n- only if all metrics are significant\n- at least onemetrics is significant\n- look at tradeoffs between different metrics\n\nNote the Probability of false positive increases a lot when having multiple metrics. Setting a p-value of 0.05 for $k$ metrics means $P(at least one metrics is significant) = 1 - (1-0.05)^k$, for $k=5$ you have probability 23%.\n\nExperiment metrics usually are:\n- subset of goal, driver and organizational guardrail metrics \n- more goals and driver metrics\n- more granular metrics such as on feature level\n- diagnostic and debug metrics\n- data quality and trustworthiness guardrails\n\n\n**Example: OEC for Bing's Search Engine**\n\nInital OEC = query share = distict search queries divided by distinct queries from all search engines over one month\n\nSimply trying to improve this OEC drove to negative effect, making the search suggestions of worse quality. Because then users had to search more! Short-term looked good bud long-term it is not!\n\nLets decompose:\n\ndistinct search queries for one month = $\\dfrac{Users}{Month} \\dfrac{Sessions}{User} \\dfrac{Distinct queries}{Session}$\n\nThe first fraction is fixed (A/B test uses 50/50 split)\n\nThe second fration you want to maximize\n\nThe third you want to minimize (measures quality of session)\n\nFurther you might need to difine what is a successfuls session!\n\n# 8. Insitutional Memory and Meta-Analysis\n\nThe digital journal of all historical experiments is called *Instituational MEmory*\n\nMining data from historical experiments is called meta-analysis\n\nIt drives experiment culture:\n- how has experimentation contributed to the growth overall (usually you get many small wins, Bing example)\n- which experiments had big or suprising impact\n- how many experiments positively or negatively (this shows value of experimentation, and shows what happens if we launch features without experimentation)\n- experiment best practices are distributed in the company\n- future innovation\n- improving metrics\n\n# 9. Ethics in controlled experiments\n\nAnonymous data methods:\n- Safe Harbor\n- k-anonymity\n- differential privacy\n\n\n\n\n# 17. The stattistics behind Online Controlled Experiments\n\n**Two-sample** t-tests look at the size of the difference between the means *relative* to the variance.\n\nThe significance of the difference is measured using p-value\n\nThe lower the p-value the strnger evidence that the Treatment is different from the Control.\n\n$T-statistic = \\dfrac{\\delta}{\\sqrt{var(\\delta)}}$\n\n$\\delta = \\bar{Y_t} - \\bar{Y_c}$ \n\nt-statstic $T$ is normalized version of $\\delta$\n\nIntuitively, the larger t, the less likley the means are the same, the more likely we reject the Null hypothesis.\n\n**p-value**\n\nThis is the probability we observe as extreme as the observed $\\delta$ given $H_{0}$ is True\n\n$P(H_{0}|\\delta observed) = \\dfrac{P(H_{0} is True)}{P(\\delta observed)} \\times P(\\delta observed| H_{0} is True) = \\dfrac{prior}{likelihood} \\times p-value$\n\n95% CI is equivalent to p-value of 0.05. If 0 is not in the 95% CI, we reject the null hypothesis (p-value is significant and is less than 0.05).\n\n\n**Normality assumption** when doing t test is for the average $\\bar{Y}$, not the sample $Y_{i}$.\n\nThis assumption holds for large $n$ by the CLT.\n\nRule-of-thumb is to use $355s^2$ data points where $s$ is the skewness coefficient.\n\nTo reduce skewness - transform the metrics or cap them.\n\nTo be more confident if normality assumption holds test in offline simulation.\n\nShuffle samples across Treatment and Control and test for normality using **Kolmogorov-Smirnov** and **Anderson-Darling**.\n\n\n**Type I/II errors and Power**\n\n- Type I error: false positives (reject Null incorrecly)\n- Type II error: false negatives (do not reject Null incorrectly)\n\nTradeoff between the two errors:\n\nhigher p-value threshold -> higher Type I error but lower Type 2 error\n\nYou control Type I error by setting the significance level p-value $\\leq 0.05$.\n\n$Type II error = 1 - Power$\n\nPower = probability detecting difference(reject null) when there is indeed difference.\n\nPower is usually parametrized by $\\delta$ = the minimum delta of practical interest\n\n$Power_{\\delta} = P(|T| \\geq 1.96| true difference is \\delta)$\n\n**power analysis**\n\nTo achieve enough power (industry standard of 80%) you need approximately $n = \\dfrac{16 \\sigma^2}{\\delta^{2}}$\n\nWe do not know the true difference $\\delta$ so we use **practical difference**, known as the **minimum detectable effect**.\n\nWant to spot big difference need less power.\n\n\n**Bias = when the estimate and the true value of the mean are systematically different.**\n\n**Multiple Testsing problem** When you have many alternative hypothesis, your probability of false discoveries increases (false positive).\n\nSimple way to fix it is reducing the p-value threshold (Bonferroni correction).\n\nSimilar problem appears when you have many many metrics. \"Why is this irrelevant metric significant?\"\n\nHere you can separete the metrics in groups and assign varying p-value thresholds.\n\n\n**Fisher's Meta-analysis**\n\nCombining results (p-values) from multiple experiments with the same hypothesis.\n\nFisher's method:\n\n$\\chi_{2k}^{2} = -2 \\sum_{i=1}^{k}ln(p_{i})$\n\n$p_i$ is p-value from $i-$th experiment.\n\nFisher's method increases the power and reduces the false-positives of your experiment.\n\n\ndo orthogonal replications of the experiment and apply Fisher's method.\n\n# 18. Variance estimation and Improved sensitivity\n\n**Variance estimation is the core of experiment analysis**\n\nincorrect estimate of variance leads to incorrect p-value and confidence interval.\n\noverestimated varaince leads to false negatives (reject when we should have) and underestmated variance leads to false positives\n\n\n**Delta vs Delta %**\n\npercent delta is the relative difference $\\delta \\% = \\dfrac{\\delta}{\\bar{Y_{c}}}$\n\n1% session increase more meaningful than 0.01 increase per user.\n\n$var(\\delta \\%) = \\dfrac{var(\\bar{Y_{t}}-\\bar{Y_{c}})}{\\bar{Y_{c}}} = var(\\dfrac{\\bar{Y_{t}}}{\\bar{Y_{c}}})$\n\nestimate it using the delta method (taylor expansion etc.)\n\n\nvariance of Ratio metrics can be estimated using delta method or bootstrap method.\n\n**Ratio metric. When analysis unit is didfferent from the experiment unit**\n\n**NB ASSUMPTION** the varaince formula $var(Y) = \\dfrac{1}{n-1}\\sum(Y_{i}-\\bar{Y})^2$ is true when $Y_i$ are iid.\n\nIf $Y_{i}$ is CTR per user, or other measurement by user, then this assumtion does not hold!\n\nNB: Rmove outliers before computing variance.\n\nImproving sensistivity = Higher power\n\nAchieve this by reducing variance in the data.\n- create an evaluation metric with smaller variance while capturing similar information (instead of purchase price, is_purchase)\n- transform metric, cap ,log\n- triggered analysis\n- stratification, CUPED\n- randomize on more gradular unit\n\n\nVaraince of other statistics (such as quantiles, not only the mean.)\n\nUse bootstrap. By estimating the density you can estimate the variance.\n\n# 19. The A/A Test\n\nA/A test is a sanity check. Establishes trust in your experimentation platform.\n\nSplit users in two identical groups and do A/A test. If the system is operating correctly then in repeated trials about 5% of the time a given metric hould be statistically significant with p-value less than 5%.\n\nWhen conducting t -tests to compute p-values, the distribution of the p-values from repeated trials should be *uniform distribution*.\n\nA/A tests benefits:\n- ensure Type I error are controlled (within 5%)\n- assess metrics variability - larger variance in metric means you need to run longer the A/B test to detect the minimum detectable effect\n- no bias between Control and Treatment (especially if you reuse populations from previous experiments - no carry-over effect)\n- good first validation step\n- distribution mismatch, platform anomalies\n- catch wrong variance estimates (if you underestimate the variance you would have large t statistic hence reject Null too often)\n- catch when independence assumption is violated (and wrong variance estimate)\n- skewed distribution (normal assumption violated)\n\nRun A/A test in parrallel and continously with other experiments.\n\nYou dont need new data to run many A/A tests. Just split the data again and again.\n\nThe p-values should follow iniform distribution.\n\nUse **goodness-of-fit** such as Anderson-Darling or Kolmogorov-Smirnoff to check if it is uniform distribution\n\nIf there is a large p-value of around 0.32, then you might have an outlier. $T = \\dfrac{\\delta}{\\sqrt{var(\\delta)}}$ would be around 1 as the outlier would swamp all other values.\n\n# 20 Triggering for Improved Sensiitivity\n\nSensitivity, Power, Variance are all related.\n\nUsers are triggered into the analysis of the experiment if there is (potentially) difference for this user when being in the varaint they are in or the counterfactual.\n\nExample: Wnat to test a new checkout UI. We should trigger the experiment for only users who initiated checkout.\n\nExample: Chage free shopping cart from 25$ to 35$. The triggered users shold be only those who have shopping cart between 25$ and 35$. Only they differ.\n\nWhen analyzing only triggered users you might need smaller sample sized to reach the same power(high sensitivity) of the experiment.\n\nTriggering only relevant users reduces the noise in the data.\n\n\n**Trigerring Trustworthiness**\n\n1. Check SRM (Sample ratio). If the overall experiment has no SRM, then the triggered data should not have SRM too.\n2. Complement analysis. Run A/B test on the **non**- triggerred users. You should get A/A test results.\n\n**Overall Treatment Effect**\n\nWhen computing Treatment effect on the triggered population, you must *dilute* the effect to the overall user base.\n\nDiluting depends on the triggering metric!\n\nExample. Improving revenue metric with 3% on 10 % of users.\n\n- If the triggered users were those who initiated checkout (and that's the only way to make money), then the overall revenue increased by 3%.\n- If the trigerred users were those who spend 10% of the average user, then the improved revenue is 3%*10%*10% = 0.03%\n\n**Open questions**\n\n1. Should we take data prior tirggerring point? If you take it you oose a bit statistical power. If you do not take it you might have abnormal metrics. Clicks prior checkout to be 0.\n2. Plotting a metric over time with increasing numbers of users usually leads to false trends. Best look at graphs over time where each day shows the user who visited that day. Same problem with triggered users. 100% of users who visited first day were triggered. Smaller portion on second day (as thoe visited first day would not trigger). Best compute triggered user ratio for each day - users who visited that day and were triggered that day. **Issue** is that you need overall Treatment for all days not day per day.\n\n# 21. Sample Ratio Mismatch \n\nSRM is used as a **guardrail metric** making sure your experiment design holds (all assumptions are plausible).\n\nSRM looks at the ratio of users between two variant (Treatment and Control) and checks if it is similar to the experimental design sample ratio.\n\nExample: we expect 1:1 ratio and get:\n- Contol: 821,588 users\n- Treatment: 815,482 users\n\nwe need to run test to check if the p value is significant or not. If it is significant then we have SRM and probaly all other metrics outputed from the experiment would be wrong.\n\nCauses of SRM:\n- Buggy randomization of users\n- data pipeline issues (bot's filtering)\n- residual effect (say we ran expirement and it had a bug. We fix the bug, avoid re-randomization and run the experiment again)\n- bad trigger condition\n\nOther trust related guardrail metrics:\n- cache hit rates\n- click tracking (web beacons)\n- cookie write rate\n\n# 22. Leakage\n\n\nSUTVA = Stable Unit Treatment Value Assumption states that the behaviour of each unit in the experiment is unaffected by variant assignment of other units.\n\nbetween variants **interference** = violation of SUTVA = data leakage = spillover\n\nunits might have direct or indirect connection\n- direct connection in networks (friends)\n- indirect (shared resources, e.g ads budget, CPU) \n\nwhen there is interference between data in variants applying treatment might affect control group as well. So the delta estimation would be wrong.\n\nShared resources example: Say we apply new feature in Airbnb and Treamnet users book more. Revenue generated in Treatment increases, but the number of free rooms decreases which affects the revenue of the control. We would overestimate the treatment effect.\n\n\nTackle data leakage:\n- Isolation (group by geography, time, clusters). When randomizing A/B test take one point per group (decreases the power of the test)\n- Evaluate the effect of data leakage:\n    - total messages send and responded to\n    - total number of posts created and total number of likes/cmments these posts receive\nthese metrics can measure the downstream impact = measure spillover by measuring the first-order impact.\n\n# 23. Measuring Long-Term Treatment Effects\n\nExperiments running for one or two weeks measure short-term effects. Our goal is to design experiments and measure short-term effects which can generalize to long-term effects.\n\nPoor generalization examples:\n- showing poor search results, would initally make users search more, but long term they might abandon the search engine\n- showing many low quality, might initially work and ahve higher CTR and revenue, but long term not\n\nReasons why the treatment effect may differ between short-term and long-term:\n- user-learned effects. User adapt to change. If the search engine or the reccomendation system give bad results users may try it initially, but abandon in the long term. On the other hand if the new feature is useful but complex, they may need time to discover the usefulness.\n- network effects. Treatment effect takes time to propagate through the whole network. Or initial propagationg may result in good results but once it finishes it is no good anymore. For example 'People you may know' feature in Linkdin. Changing the rec algo may improve simply because you reccommend new people.\n- delayed experience and measurement. In Airbnb people can book in advance by several months.\n- ecosystem changes:\n    - other new features are launched\n    - seasonality\n    - competitive landscape\n\nOne way to measure long term effect is to run the experiment for longer time, and the last delta you get is what you consider as the long term delta.\n\nCaveat: The longer you run the experiment the more likely you will have data leakage:\n- users may start using the feature from multiple devices\n- netwrok effect would propagate\n- treatment effect dilution\n\nThe longer yourun the experiment the higher survivorship bias you might have.\n\n**Alternative methods for Long-Running Experiments**\n\n1. Cohort Analysis - divide data using stable ID (addressing survivorship bias and data leakage). You can track for each cohort the bias and leakage.\n\nNeed to combine results from different cohorts in the end\n\n2. Post-Period Analysis.\n\nYou have Control and Treatment for time $T$. Then you turn off the new feature for all users.\n\nFor time $T$ to $T+1$ you check the *learning effect.* There are two types:\n- user-learning effect (say you added more ads in treatment and they are used to clicking adds. Then after time T users from treatment might continue clicking lots of ads)\n- system-learning effect. Your ML models have better parameters.\n\n3.  Time-staggered Treatments\n\nThe experiments so far require to wait 'enough' time before taking the long-term measurement. How to decide what is enough?\n\nRun the same treatment to two variants where one lagged, i.e. we have 2 starting times $t_{0} = t$, $t'_{0} = t+1$.\n\nYou have effectivle A/A test where the second A is has the treatment effect 1 period less.\n\nWhen your test statistic are the same (running A/A test), i.e the p-valu is not significant, you can say that is enough.\n\nAssumes over time the difference between the statistics converges to 0\n\n4. Holdback and reverse experiment\n\nRunng long time and experiment might cost a lot. Not releasing the feautre to the control group might lead to missed opportunities.\n\nThus you can consider *holdback* - i.e. leave 10% control group and 90% Treatment. You loose a bit of power of the test though.\n\n*reverse experiment*. Release for all the treatment and after some time reverse 10% back to Control. Problem is control group might be confused.","n":0.015}}},{"i":206,"$":{"0":{"v":"Theory of Poker","n":0.577}}},{"i":207,"$":{"0":{"v":"The Money Machine","n":0.577},"1":{"v":"\n# Introduction\n\nThis book is about basic financial principles applicable to Mr Smith, the grocery store and Barclays.\n\n## The City\n\n**The business of financial institutions is the handling of money.**\n\nCity of London is the heart of the UK financial system. The primary function f this system is to connect people who lend with people who want to borrow. Financial institutions are middle men. Their job is the job of **handling money**.\n\nWhy do we need middle men? Often the needs of borrowers cannot be matched by lenders. Borrowers such as governments and companies need large sums of money, lenders usually lend small amounts. Also lenders often want to withdraw their money quickly. Companies need money for longer periods of time. People with mortgages want to borrow for the next 25 years.\n\nTwo variables when lending money:\n\n- time to return/withdraw (related to liquidity)\n- amount (related to liquidity)\n\nBorrowers:\n- businesses - using shares\n- governments - using bonds\n\nLenders (need to be net savers)\n- private people\n- working class\n\nMiddle man:\n- banks\n- building societies\n- pension schemes\n- life insurances\n\n\n## The Institutions\n\nBanks (short term loans):\n- commercial (rely on people deposits)\n- investment banks (deal with takeovers, trading)\n\nInvestment institutions (long period loans)\n- pension funds\n- life insurance\n\nTrading companies\n- hedge funds\n- private equity firms (prop trading)\n- market makers\n\nExchanges\n- Stock exchanges are the most popular (NYC, LSE)\n- Commodity exchanges\n\n## The Instruments\n\n**Money is just one of the many financial assets.** What distinguishes all financial assets is their liquidity.\n\n**Greater risk demands greater reward.**\n\n**Lesser liquidity demands greater reward.**\n\n- **Loans.**Extra returns come from interest rate.\n- **Bonds**. Loan from government which you can choose to be paid back at expiry or you can trade it before expiry. Bond also pay interest rate called coupons,\n- **Equities = Shares**. Loan which you would never be returned but you can trade. May receive dividends (small interest rate), Rely solely on growth.\n\nIf company goes south, lenders and bondholders come first and then equity investors. However in case of growth, equity investors benefit more.\n\n\n- **Derivatives** are financial assets that are based on other products; their value is derived from elsewhere. Among the best-known derivative instruments are futures, options and swaps.\n\n- **Leverage** gives the investor an opportunity for a large profit with a small stake.\n\nLiquidity range: The range of financial assets extends from cash to long-term loans.\n\n# The International Financial Revolution\n\nOld system - Bretton Woods had fixed exchange rates with strict controls on capital flows. This system went down in 1970s. Sterling was moved to floating point with the US dollar.\n\nFloating exchange rates added risks for companies trading worldwide. This was opportunity for financial companies to mitigate foreign-exchange rate.\n\nThere are 3 elements of monetary policy:\n- exchange rates\n- interest rates\n- capital controls\n\nUnder Bretton Woods system, countries controlled their exchange rates and capital flows.\n\n\n*The absence of takeovers meant there was no market discipline on poor companies to perform well.*\n\n\nThe free-market model: lower taxes, reduced government deficits and open capital markets.\n\n**Globalization**\n\nWhat is globalization? It is one of those terms that is often used, but more rarely defined. Broadly\nspeaking, it is a trend whereby trade, investment and culture have become ever more international.\n\n\n# Money and Interest Rates\n\n## Money\n\nThere are a variety of functions which money serves:\n\n- measure of value\n- store of value (can be saved until needed)\n\nCreditors will accept money as a future payment, confident that its value will remain stable in the meantime.\n\n\nBarter system (goat for cow) -> Gold and Silver -> Paper money.\n\nWhy change from gold to paper money?\n\n- Supplies of gold and silver were outstripped by the demands of society.\n- Bad money drives out good. People with pure gold would strip it out a bit. Only worst coins circulated.\n\n## Banknotes and Cheques\n\nBanknotes were, in origin, claims on gold and silver.\n\nNow money depends on the confidence of its users in the strength of the economy.\n\nBanknotes, Cheques, Cash, Credit and Debit cards. The system depends on the confidence of all those concerned.\n\nBank accounts are therefore money in the same sense as notes and coins are, since they can be used instantly to purchase goods.\n\n\n## Defining money supply\n\nLevel of inflation are closely related to the rate of increase of the money supply.\n\nnarrow money = notes and coins in circulation with the public\n\nbroad money,known as M4 = consists of lending by UK banks and building societies to the private sector.\n\n## Interest rates\n\n**Interest rate is essentially the price of money**\n\nThe price is paid by the borrower in return for the use of the lender ’s money. The lender is compensated for not having the use of his money.\n\n- simple\n- compound\n\n## Yield\n\nWhen dealing with a bond or with a share, it is more important to talk of the yield than merely of the interest rate or dividend.\n\nThe interest rate or dividend, expressed as a percentage of the price of the asset, is the yield","n":0.036}}},{"i":208,"$":{"0":{"v":"The Mathematics of Poker","n":0.5},"1":{"v":"- Bill Chen and Jerrod Ankenman (SIG guys)","n":0.354}}},{"i":209,"$":{"0":{"v":"The Lean Startup","n":0.577}}},{"i":210,"$":{"0":{"v":"Statistical Methods in Online AB Testing","n":0.408}}},{"i":211,"$":{"0":{"v":"Principles For Navigating Big Debt Crisis","n":0.408}}},{"i":212,"$":{"0":{"v":"Maths","n":1},"1":{"v":"- Forecasting: Principles and Practice Rob Hyndman and George Athanasopoulos, [online textbook](https://otexts.com/fpp3/)\n\n- Mathematics Optimizations\n\n- Introduction to probability, Geoffrey Grimmet & Dominic Welsh. Recommended in Oxford.\n\n- Deep Learning bibliqta","n":0.189}}},{"i":213,"$":{"0":{"v":"Introduction to algorithms","n":0.577},"1":{"v":"A classic and must read book for any serious programmer. Also known as CLRS.","n":0.267}}},{"i":214,"$":{"0":{"v":"Fluent Python","n":0.707},"1":{"v":"- Great book to learn advanced Python concepts and idioms.","n":0.316}}},{"i":215,"$":{"0":{"v":"Elements of Statistical Learning","n":0.5},"1":{"v":"Classic textbook, very heavy on math and recommended by most quants. Not much practicalities though. However if you are able to deeply understand the algorithms you will come up with ideas.","n":0.18}}},{"i":216,"$":{"0":{"v":"DDIA","n":1},"1":{"v":"\nLegendary book about building scalable data systems.\n- Chapters 1-4 are a must read.\n- Chapter 1 defines what system characteristics we care about: scalability, reliability, maintainability.\n- Chapter 2 discusses data models and query languages - from user's perspective how do you work with data.\n- Chapter 3 discusses storage and retrieval - how data is stored on disk, B+ trees, LSM trees - from DB-engineer's perspective. DB has two jobs to store data and to return data when queried.\n- Chapter 4 discusses encoding and evolution - how data is encoded on disk, serialization formats (JSON, XML, Protobuf, Avro, Thrift), schema evolution. This is about data representation on a computer.\n- Part II is about distributed data (a bit harder to read). Chapter 7 is worth reading to understand what transactions are (ACID, where the 'C' is not related to transactions..)\n- Chapter 5& 6 are about replication and partitioning and are ok. Chapter 8-9 not worth so much (or at least I'm not at the stage where I could understand and relate to them).\n- Chapter 10 about batch processing is great. Starts from Unix batch, explain MapReduce and all this will help you understand the internals of Spark.\n","n":0.072}}},{"i":217,"$":{"0":{"v":"Computer Networking A Top Down Approach","n":0.408}}},{"i":218,"$":{"0":{"v":"Clean Code","n":0.707}}},{"i":219,"$":{"0":{"v":"Causal Inteference in Python","n":0.5},"1":{"v":"# Chapter 1. Introduction to Casual Inference\n\n- \"Association is not causation\", but sometimes association is causation..\n- association is when two quantities or random variables move together, whereas causality is when change in one variable causes change in another.\n- you want to know cause-and-effect relationships so taht you can **intervene** on the cause to bring upon a desired effect\n- in forecasting association is enough tbh. \n- predictive models (e.f ARIMA, ML models, regression, neural networks) are effective even witouth establishing causal relationship. They are based on **observed patterns**.\n- limitation of predictive mdoels is although they can provide accurate forecats they may not perform well if the underlying relationship change. They do not provide insights into how or why the change occur.\n- **The fundamental problem of casual inference is that you can never observe the same unit with and without treatment**\n- **spillover or network** effects in experiments. Vaccinating one person will make other people less likely to get illness. Control grou might be affected.\n- this is the SUTVA assumption, stable unit of threatment value, threating one unit does not influece other units behaviour\n\n\n## When Causality Matters\n\nUsually when you need to intervene or control the behaviour of outcomes. For example in policy making, healthcare (you want the drug to work) or at tech companies when you add new features in the app. Causality matters in **decision making** and it the industry it becomes a **decision science**.\n\nCausal models are more robust to changes in the environment since the focus on the underlying mechanisms rather than just on the observed patterns.\n\n- casual inference is great for \"what if\" questions, ML is just awfult at those types of questions\n- \"the new wave of artificial intelligence does not actuallybring intelligence but instead a critical component of intelligence - **prediction**\"\n- ML requirement is to frame the problem as prediction one. e.g want to translate from english to portuguese, then build ML model that predicts Portuguese sentences when given English sentences.\n- ML is very good under problems with rigid boundaries\n\n\n## Causal Models\n\n**Notation:**\n\n$$\nT \\leftarrow f_{t}(u_{t}) \\newline\nY \\leftarrow f_{y}(T, u_{y})\n$$\n\n\nThe treatment is a random variable that depends on exogeneous vairable $u_{t}$. The outcome $Y$ is a random variable that depends on the threatment $T$ and on endogeneous variable $u_{y}$.\n\nNow with this framework, we can start interventions. What would happendi f we set the threatment to $t_{0}$\n\n$$\nT \\leftarrow t_{0}\\newline\nY \\leftarrow f_{y}(T, u_{y})\n$$\n\n**Definition.** When we intervene we define the *do(.)* operator.\n\n$$\nE[Y|T=1] \\neq E[Y|do(T=1)]\n$$\n\nLHS is an obervations, whereas the right hand side is the truth. In practice you apply threatment $T=1$ to only a subset of observations, whereas the RHS tells you what would happen if you apply to all observations.\n\n\n\n### Individual threatment effect\n\nThis is the difference between counterfactual(not able to observe) vs factual (able to observe).\n\nFor an individual unit $i$, the **indivudual threatment effect is**:\n$$\n\\rho_{i} = Y_{i}|do(T=1) -Y_{i}|(T=0) = Y_{1i} - Y_{0i}\n$$\n\nDue to the fundamental problem of causal inference, you can only observe one term of the preceding equation. So, even though you can express the term theoretically, it doesn't necessarily mean you can recover it from data. From data we will make an estimate!\n\n### Casual Quantities of Interest\n\nAverage threatment effect:\n\n$$\nATE = E[Y_{1i}-Y_{0i}]\n$$ \n\nusually we can replace the expectation with sample average, estimation.\n\nIn practice you cannot do this since you cannot observe the two states of the same unit.\n\nAverage threatment effect on the treated:\n\n$$\nATT = E[Y_{1i}-Y_{0i}|T=1]\n$$\n\nyou want to estimate this value in cases you want to understand what is the threatment effect on the threated group. You run a marketing campaign and want to understand the actual profit rather then what would have happed withought the campaign for which you use ATE.\n\n\nConditional average threatment effect\n\n$$\nCATE = E[Y_{1i}-Y_{0i}|X=x]\n$$\n\neffect of a drug on customers older than 45 years old.\n\n\n### What happens in practice when we estimate ATE, ATT and CATE\n\n\nAssume that we have a random variable $Y$ (quantity of intersest in experiment, e.g number of clicks).\n\nthat is the amount of clicks $Y_1$ when treated, i.e $T=1$ and the amount of clicks $Y_0$ when not treated, i.e $T=0$. Note all quantities are random variables.\n\nThe **association** between the treatement and the outcome is measured by $E[Y|T=1] - E[Y|T=0]$ and is the value we can observe/estimate. **Causation** is measured by $E[Y|do(T=1)] - E[Y|do(T=0)]$\n\n$$\nE[Y|T=1] - E[Y|T=0] \\newline \n= E[Y_{1}|T=1] - E[Y_{0}|T=1] \\newline \n= E[Y_{1}-Y_{0}|T=1] + E[Y_{0}|T=1] - E[Y_{0}|T=0]\\newline \n= ATT + BIAS\n$$\n\nSo in practice intead of observing $ATT$ only we observe $ATT + BIAS$. Not that bias is the difference between the averages of the **untreated** samples  that we decide to treat $T=1$ and thos that we do not want to treat $T=0$.\n\n\n**Definition:** Treated and control groups are *interchangable* if\n- treatment and control group are comparable regardless of the treatement $E[Y_{0}|T=1] = E[Y_{0}]|T=0$, that is zero bias, and this means that association = causation\n- treatment and control group respond similarly under treatement $E[Y_{1}|T=1] = E[Y_{1}]|T=0$. Then $$ATT = ATE$$, treatment effect on the treated is no different than all.\n\n**Exchangable, interchangable, independent** control and treatment groups refer all to the same assumptions. $(Y_{0}, Y_{1}) \\perp T$\n\nThis assumption means **treated and untreated** control and treatment groups are comparable and indistinguishable.\n\nIdentification is the process of making this assumption correct.\n\n\n# Chapter 2. Randomized Experiments and Stats Review\n\nMain assumption we need in casual inference is \"“Treatment & control groups are comparable regardless of treatment”\"\n\n**Key idea:** assigning the treatment randomly “brute-forces” this.\n\n\nRCT = Randomized Control Trials\n\n\n**Randomized vs Systematic Error**\n\nSystematic errors are consistent biases that affect all measurements in the same way, while random errors are unpredictable fluctuations in daa due to chance.\n\n\n\nIn the previous chapter we calculated $ATT$, $ATE$ estimates. How confident can we be in them? We need hypothesis testing, standdard error, CI, p-values caluations for this.\n\n\nCausual Inference two steps:\n\n1. Identation - going from unobservable causal quantities to observable statistical quantitites that you can estimate.\n2. Estimation - hypothesis testing.","n":0.032}}}]}
