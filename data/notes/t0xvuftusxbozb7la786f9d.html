<h1 id="distributed-computing">Distributed Computing<a aria-hidden="true" class="anchor-heading icon-link" href="#distributed-computing"></a></h1>
<h1 id="spark">Spark<a aria-hidden="true" class="anchor-heading icon-link" href="#spark"></a></h1>
<p><strong>Terminology</strong></p>
<ul>
<li>node = computer (VM, physical machine)</li>
<li>cluster = group of nodes</li>
<li>executor = process that runs on a node</li>
<li>driver = main process that coordinates the execution of tasks across the cluster</li>
</ul>
<p><strong>Abstractions</strong></p>
<ul>
<li>every spark application consists of a <strong>driver</strong> program that runs the user's main function and runs a set of <strong>executor</strong> processes in <em>parallel</em> on a cluster</li>
<li>RDD = resilient distributed dataset. This is the main <em>abstraction</em> that Spark provides. It is a collection of objects that can be ran in parrallel across a cluster</li>
<li>Users may ask Sprk to <strong>persis</strong> and RDD in memory</li>
<li>RDDs automatically recover from node failures</li>
<li>2nd <em>abstraction</em> that Spark provides is <strong>shared variables</strong> that can be used in parallel operations. There are 2 types of shared variables: <strong>broadcast variables</strong> (shared among all nodes) and <strong>accumulators</strong> (sums,counts)</li>
</ul>
<p><strong>RDD Operations</strong></p>
<ul>
<li>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. </li>
<li>All transformations in Spark are <strong>lazy</strong></li>
<li>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the <strong>persist (or cache)</strong> method</li>
<li><strong>Shuffle Write</strong> is the amount of data executors had to write to other executors so the other executor could read.</li>
<li><strong>Shuffle Read</strong> is the amount of data executors read that was provided by another executor</li>
</ul>
<p><strong>Shuffling</strong></p>
<ul>
<li>Shuffling is the process where data is redistributed across different executors (nodes/machines) in the Spark cluster — usually because it needs to group data differently than it was originally stored.</li>
<li>groupBy, distinct, reduceByKey, sortBy reaarange the data so lal records with the same key (join key) end up on the same partition/executor.</li>
<li>In ideal cases (no task failures, no data loss, perfect partitioning), <strong>Shuffle Write ≈ Shuffle Read.</strong></li>
</ul>
<h1 id="pyspark">PySpark<a aria-hidden="true" class="anchor-heading icon-link" href="#pyspark"></a></h1>
<h2 id="properties">Properties<a aria-hidden="true" class="anchor-heading icon-link" href="#properties"></a></h2>
<ul>
<li>To start a spark session you need to pass a SparkConf file in the SparkContext</li>
</ul>
<p><strong>Driver</strong></p>
<ul>
<li>spark.driver.memory = 16g Memory for the driver process (the "main program")</li>
<li>spark.driver.cores = 4 Number of CPU cores for the driver process</li>
</ul>
<p><strong>Executor</strong></p>
<ul>
<li>spark.executor.memory = 28g. Amount of memory for each executor process.</li>
<li>spark.executor.cores = 4 Number of CPU cores for each executor. Each core can handle up to 7g data in memory</li>
<li>spark.cores.max = 36. total number of cores in the cluster. So you can have 9 executors with 4 cores each</li>
<li>spark.executor.instances 0. Static executors to launch. 0 means not set—dynamic allocation will control.</li>
</ul>
<p><strong>Executor Dynamic Allocation</strong></p>
<ul>
<li>spark.dynamicAllocation.enabled = true. Let spark dynamically allocate executors based on workload</li>
<li>spark.dynamicAllocation.maxExecutors = 9 Max number of executors Spark will request dynamically.
spark.dynamicAllocation.executorAllocationRatio</li>
<li>spark.dynamicAllocation.executorAllocationRatio = 0.8. Allocate fraction of the estimated required executors. Controls aggressiveness to avoid overloading the cluster.
Say you have 100 tasks and an executer on averages executes 4 tasks. So we will need 25 tasks. If we set ratio 0.8, spark will allocate 0.8*25=20 executors</li>
</ul>
<p><strong>SQL</strong></p>
<ul>
<li>spark.sql.autoBroadcastJoinThreshold = 134217728 (128MB). If a table is smaller than this, Spark will broadcast it to avoid shuffles in joins.</li>
<li>spark.sql.shuffle.partitions = 200. After a shuffle operation (e.g join, groupby) Spark will output the data in chunks (number of output partitions)</li>
<li>Too low number, some executors will be overloaded with large partitions, leadint to slow or skewed jobs or even OOM errors</li>
<li>Too high number, will lead to scheduling overhead and many small tasks, increasing job time</li>
<li>A good value is usually 2–4 × total executor cores in your cluster, but it depends on <strong>data size and job characteristics.</strong></li>
</ul>
<p><a href="https://drive.google.com/file/d/1Dz5x9OPOYFs0nczzfeR7QBNY_tbB11v8/view?usp=drive_link">Notebook</a></p>
<p>PySpark is a Python API that allows you to use the power of Apache Spark - fast &#x26; scalable big data processing system.</p>
<ul>
<li>you write Python code</li>
<li>PySpark lets tat code run on many computers at once (a "cluster")</li>
<li>analyze, process and transform huge datasets that would not fit on a single computer's memory</li>
<li>distributed processing - distribute the data in multiple computers and splits up the work</li>
<li>supports Pandas like code + SQL queries</li>
</ul>
<h1 id="optimizations">Optimizations<a aria-hidden="true" class="anchor-heading icon-link" href="#optimizations"></a></h1>
<ul>
<li>cache initialized data. Whenever you reuse a dataframe in a for loop </li>
</ul>
<pre class="language-python"><code class="language-python"> 
</code></pre>
<h1 id="pyspark-1">PySpark<a aria-hidden="true" class="anchor-heading icon-link" href="#pyspark-1"></a></h1>
<p>Example spark configs:</p>
<pre><code>spark.executor.memory: 28g
spark.executor.memoryOverhead: 8g
spark.executor.cores: 4
spark.executor.instances: 256 # total number of executors, not needed if using dynamic allocation
spark.dynamicAllocation.initialExecutors: 64
spark.dynamicAllocation.minExecutors: 64
spark.dynamicAllocation.maxExecutors: 256
spark.driver.memory: 28g # DRIVER!
</code></pre>
<ul>
<li>spark is a distributed computing framework that allows you to work with large datasets across a cluster of machines/computers</li>
<li><strong>worker node</strong> is typically one physical or virtual computer in the cluster</li>
<li><strong>driver node</strong> is the main process that coordinates the execution of tasks across the cluster. It is responsible for creating the SparkContext, which is the entry point to using Spark.</li>
<li>spark executor is a <strong>process</strong> that runs on each worker node in the cluster and is responsible for executing tasks and managing resources. Like a separate Python interpreter that runs on the worker node.</li>
<li>node can have multiple executors running on it.</li>
<li>each executor has its own memory and CPU resources allocated to it, which are used to execute tasks in <strong>parallel</strong>.</li>
<li><strong>executor.cores</strong>: each executor can have multiple cores (i.e threads) that can execute tasks <strong>concurrently</strong>.</li>
<li><strong>executor instances (spark.executor.instances)</strong>: total number of executors (processes) launched on all worker nodes in the clust</li>
<li>If using dynamic allocation, leave spark.executor.instances unset or set min/max via:</li>
</ul>
<pre><code>spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors
spark.dynamicAllocation.maxExecutors
</code></pre>
<p><strong>Why to choose dynamic allocation?</strong></p>
<ul>
<li>Variable Input Size</li>
<li>Fluctuating Resource Needs - joins, groupby-s, aggregating and exploding data</li>
<li>shared, multi-user cluster : release executors when idle so others can use those resources</li>
<li>cost optimization, not paying for idle resources</li>
<li>long-running applications, your job scales up and down depending on activity</li>
</ul>
<p><strong>When NOT to Use Dynamic Allocation</strong></p>
<ul>
<li>Your resource needs are steady and predictable (and you’re on a dedicated cluster).</li>
<li>Very short jobs or jobs with very short "bursts" of high demand (executor startup delays can hurt performance).</li>
<li>You depend on RDD caching across all executors (since dynamic allocation can kill executors, losing cached data).</li>
</ul>
<p>Driver and Workers:</p>
<p><img src="/dendron-wiki/./assets/images/spark_driver_worker.png" alt="alt text"></p>
<h1 id="example-optimization">Example optimization<a aria-hidden="true" class="anchor-heading icon-link" href="#example-optimization"></a></h1>
<p>Hey! After some experiment runs I've updated and chose these spark parameters</p>
<pre><code>    spark.executor.memory: 28g
    spark.executor.memoryOverhead: 3g
    spark.executor.cores: 4
    spark.dynamicAllocation.initialExecutors: 8
    spark.dynamicAllocation.minExecutors: 8
    spark.dynamicAllocation.maxExecutors: 32
    spark.driver.memory: 28g
</code></pre>
<p>Run log</p>
<p>Run time 360 seconds, cost 1.41$ (previously it was 7$)</p>
<p>CPU average usage by the whole Pod is~30-40%. I won't decrease more the amount of executors since 8 to 32 dynamically allocated is already low compared to other jobs in this service.</p>
<p>Memory average usage by the whole Pod is 50-65% - this is ok since we want some room left</p>
<p>I use dynamic number of executors instead of fixed since CAPI has variable size input (as we onboard/churn clients num conversions can change a lot, matches data also depends a lot on the quality of the conversion client send to us which can vary over time). Also there are a few joins/groupbys in the DQS job that require variate amount of resources</p>
<p>Spark recommends memoryOverhead of executor to be about 6-10% of the container size = spark.executor.memory + spark.executor.memoryOverhead. I chose ~10% to be on the safe size</p>
<h1 id="spark-monitoring">Spark Monitoring<a aria-hidden="true" class="anchor-heading icon-link" href="#spark-monitoring"></a></h1>