<h1 id="loss-functions">Loss Functions<a aria-hidden="true" class="anchor-heading icon-link" href="#loss-functions"></a></h1>
<h1 id="huber-loss">Huber Loss<a aria-hidden="true" class="anchor-heading icon-link" href="#huber-loss"></a></h1>
<p><a href="https://en.wikipedia.org/wiki/Huber_loss#:~:text=absolute%20value%20function">Wikipedia</a>.-,Pseudo%2DHuber%20loss%20function,less%20steep%20for%20extreme%20values.)</p>
<p>Huber loss is a combination of L1 and L2 loss functions. It is less sensitive to outliers in data than the squared error loss.</p>
<p>pseudo huber loss allows you to control the smoothness and therefore you can specifically decide how much you penalise outliers by, whereas huber loss is either MSE or MAE</p>
<h2 id="how-to-choose-the-hyper-parameter-delta">How to choose the hyper parameter delta?<a aria-hidden="true" class="anchor-heading icon-link" href="#how-to-choose-the-hyper-parameter-delta"></a></h2>
<p>Huber loss will clip gradients to delta for residual (abs) values larger than delta. You want that when some part of your data points poorly fit the model and you would like to limit their influence. Also, clipping the grads is a common way to make optimization stable (not necessarily with huber).
<a href="https://stats.stackexchange.com/questions/465937/how-to-choose-delta-parameter-in-huber-loss-function">StackExchange</a></p>
<h1 id="l1-vs-l2-regularization">L1 vs L2 regularization<a aria-hidden="true" class="anchor-heading icon-link" href="#l1-vs-l2-regularization"></a></h1>
<p>The best advice in data science is always try both and see what produces better CV and LB.</p>
<p>I think L1 shines when we approach "the curse of dimensionality" (i.e. when the number of train rows is small compared with number of train columns). I think a rule of thumb is when number of features (i.e. columns) is anywhere near 1/10th (or larger) the number of training samples (i.e. rows) we approach the "the curse of dimensionality".</p>