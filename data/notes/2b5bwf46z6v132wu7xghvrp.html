<h1 id="machine-learning">Machine Learning<a aria-hidden="true" class="anchor-heading icon-link" href="#machine-learning"></a></h1>
<h1 id="articles">Articles<a aria-hidden="true" class="anchor-heading icon-link" href="#articles"></a></h1>
<p><a href="https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-for-2023-843dba11419c">ML algos</a></p>
<h1 id="kaggle">Kaggle<a aria-hidden="true" class="anchor-heading icon-link" href="#kaggle"></a></h1>
<ul>
<li><a href="https://github.com/ngocuong0105/kaggle">Repo</a></li>
<li><a href="/dendron-wiki/notes/ltvbyf1oqc8qx7khy58pby2">Kaggle Tricks</a></li>
</ul>
<p>Steps:</p>
<ol>
<li>EDA</li>
<li>Feature engineering</li>
<li>Build ML model</li>
<li>Model validation</li>
</ol>
<p><strong><a href="https://scikit-learn.org/stable/glossary.html#term-random_state:~:text=random_state-,%C2%B6,-%C2%B6">random_state parameter</a></strong></p>
<p>Many machine learning models allow some randomness in model training. Specifying a number for random_state ensures you get the same results in each run. You use any number, and model quality won't depend meaningfully on exactly what value you choose.</p>
<p><strong>Underfitting vs overfitting</strong></p>
<p>Control the balance between these two using max leaves and max tree depth parameters in decision trees.</p>
<h1 id="mlops">MLOps<a aria-hidden="true" class="anchor-heading icon-link" href="#mlops"></a></h1>
<p><a href="https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases/notebook">Weights and Biases</a></p>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/dendron-wiki/notes/ok7s53lhgu6s0eislk3pb0p">Casual Impact</a></li>
<li><a href="/dendron-wiki/notes/r70jjp3vympzneaerdgdoey">Code ML from scratch</a></li>
<li><a href="/dendron-wiki/notes/7bnhuaoqchmbkluk5lhldvs">Comprehensive maths behind ML</a></li>
<li><a href="/dendron-wiki/notes/ufrcqzs753ifx67ttk5u303">Cross Validation</a></li>
<li><a href="/dendron-wiki/notes/zfqnpk8iqhmawt2ecm57kfd">Curse of Large Dimensionality</a></li>
<li><a href="/dendron-wiki/notes/nkefr7ipopudoxe89h53oy4">DT RF GB</a></li>
<li><a href="/dendron-wiki/notes/bjd507r3q0ft6zvvkik31ok">Deep Learning</a></li>
<li><a href="/dendron-wiki/notes/jdlsh6qdvq2hyhchiak7nug">Diff and diff analysis</a></li>
<li><a href="/dendron-wiki/notes/x6ofql87nulq02kmlfrz81q">Experiments</a></li>
<li><a href="/dendron-wiki/notes/p7q8tnzclqd4j6ri9o0dpwh">Explainable AI - SHAP</a></li>
<li><a href="/dendron-wiki/notes/pc1mkj2gp1dfypmvprtqeo5">Feature Engineering</a></li>
<li><a href="/dendron-wiki/notes/619exdki1mbha7z20razdxm">Google Rules of Machine Learning</a></li>
<li><a href="/dendron-wiki/notes/tfv2mbp7pm72fjssn0eijec">Imbalanced dataset</a></li>
<li><a href="/dendron-wiki/notes/ltvbyf1oqc8qx7khy58pby2">Kaggle Tricks</a></li>
<li><a href="/dendron-wiki/notes/tggevcehu7fihdxxlhabv7x">Kaggle winning solutions</a></li>
<li><a href="/dendron-wiki/notes/h79jpatf2zw7a0qpq4asniw">Linear Regression</a></li>
<li><a href="/dendron-wiki/notes/1qig9jqzoq1t8nocwttq9qj">Logistic Regression</a></li>
<li><a href="/dendron-wiki/notes/gyqbfvh6v046yobrtn4rjh4">Loss Functions</a></li>
<li><a href="/dendron-wiki/notes/4to49a2jwhv8s6kap9e4jps">Metrics</a></li>
<li><a href="/dendron-wiki/notes/q06yegd2gxq72jqf5aib8em">Outliers</a></li>
<li><a href="/dendron-wiki/notes/emh1886qozogbsvvgwjf3ec">Time Series</a></li>
</ol>