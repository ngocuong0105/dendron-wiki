{"pageProps":{"note":{"id":"t0xvuftusxbozb7la786f9d","title":"Distributed Computing","desc":"","updated":1759744985124,"created":1752649602765,"custom":{},"fname":"engineering.Concepts.Distributed Computing","type":"note","vault":{"fsPath":"vault"},"contentHash":"9d34f4286087f55f5d5f009fbf968da2","links":[],"anchors":{"spark":{"type":"header","text":"Spark","value":"spark","line":8,"column":0,"depth":1},"pyspark":{"type":"header","text":"PySpark","value":"pyspark","line":40,"column":0,"depth":1},"properties":{"type":"header","text":"Properties","value":"properties","line":42,"column":0,"depth":2},"optimizations":{"type":"header","text":"Optimizations","value":"optimizations","line":80,"column":0,"depth":1},"pyspark-1":{"type":"header","text":"PySpark","value":"pyspark-1","line":90,"column":0,"depth":1},"example-optimization":{"type":"header","text":"Example optimization","value":"example-optimization","line":136,"column":0,"depth":1},"spark-monitoring":{"type":"header","text":"Spark Monitoring","value":"spark-monitoring","line":160,"column":0,"depth":1}},"children":[],"parent":"7khpigcji0ufpuptuqozcft","data":{}},"body":"<h1 id=\"distributed-computing\">Distributed Computing<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#distributed-computing\"></a></h1>\n<h1 id=\"spark\">Spark<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#spark\"></a></h1>\n<p><strong>Terminology</strong></p>\n<ul>\n<li>node = computer (VM, physical machine)</li>\n<li>cluster = group of nodes</li>\n<li>executor = process that runs on a node</li>\n<li>driver = main process that coordinates the execution of tasks across the cluster</li>\n</ul>\n<p><strong>Abstractions</strong></p>\n<ul>\n<li>every spark application consists of a <strong>driver</strong> program that runs the user's main function and runs a set of <strong>executor</strong> processes in <em>parallel</em> on a cluster</li>\n<li>RDD = resilient distributed dataset. This is the main <em>abstraction</em> that Spark provides. It is a collection of objects that can be ran in parrallel across a cluster</li>\n<li>Users may ask Sprk to <strong>persis</strong> and RDD in memory</li>\n<li>RDDs automatically recover from node failures</li>\n<li>2nd <em>abstraction</em> that Spark provides is <strong>shared variables</strong> that can be used in parallel operations. There are 2 types of shared variables: <strong>broadcast variables</strong> (shared among all nodes) and <strong>accumulators</strong> (sums,counts)</li>\n</ul>\n<p><strong>RDD Operations</strong></p>\n<ul>\n<li>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. </li>\n<li>All transformations in Spark are <strong>lazy</strong></li>\n<li>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the <strong>persist (or cache)</strong> method</li>\n<li><strong>Shuffle Write</strong> is the amount of data executors had to write to other executors so the other executor could read.</li>\n<li><strong>Shuffle Read</strong> is the amount of data executors read that was provided by another executor</li>\n</ul>\n<p><strong>Shuffling</strong></p>\n<ul>\n<li>Shuffling is the process where data is redistributed across different executors (nodes/machines) in the Spark cluster — usually because it needs to group data differently than it was originally stored.</li>\n<li>groupBy, distinct, reduceByKey, sortBy reaarange the data so lal records with the same key (join key) end up on the same partition/executor.</li>\n<li>In ideal cases (no task failures, no data loss, perfect partitioning), <strong>Shuffle Write ≈ Shuffle Read.</strong></li>\n</ul>\n<h1 id=\"pyspark\">PySpark<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pyspark\"></a></h1>\n<h2 id=\"properties\">Properties<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#properties\"></a></h2>\n<ul>\n<li>To start a spark session you need to pass a SparkConf file in the SparkContext</li>\n</ul>\n<p><strong>Driver</strong></p>\n<ul>\n<li>spark.driver.memory = 16g Memory for the driver process (the \"main program\")</li>\n<li>spark.driver.cores = 4 Number of CPU cores for the driver process</li>\n</ul>\n<p><strong>Executor</strong></p>\n<ul>\n<li>spark.executor.memory = 28g. Amount of memory for each executor process.</li>\n<li>spark.executor.cores = 4 Number of CPU cores for each executor. Each core can handle up to 7g data in memory</li>\n<li>spark.cores.max = 36. total number of cores in the cluster. So you can have 9 executors with 4 cores each</li>\n<li>spark.executor.instances 0. Static executors to launch. 0 means not set—dynamic allocation will control.</li>\n</ul>\n<p><strong>Executor Dynamic Allocation</strong></p>\n<ul>\n<li>spark.dynamicAllocation.enabled = true. Let spark dynamically allocate executors based on workload</li>\n<li>spark.dynamicAllocation.maxExecutors = 9 Max number of executors Spark will request dynamically.\nspark.dynamicAllocation.executorAllocationRatio</li>\n<li>spark.dynamicAllocation.executorAllocationRatio = 0.8. Allocate fraction of the estimated required executors. Controls aggressiveness to avoid overloading the cluster.\nSay you have 100 tasks and an executer on averages executes 4 tasks. So we will need 25 tasks. If we set ratio 0.8, spark will allocate 0.8*25=20 executors</li>\n</ul>\n<p><strong>SQL</strong></p>\n<ul>\n<li>spark.sql.autoBroadcastJoinThreshold = 134217728 (128MB). If a table is smaller than this, Spark will broadcast it to avoid shuffles in joins.</li>\n<li>spark.sql.shuffle.partitions = 200. After a shuffle operation (e.g join, groupby) Spark will output the data in chunks (number of output partitions)</li>\n<li>Too low number, some executors will be overloaded with large partitions, leadint to slow or skewed jobs or even OOM errors</li>\n<li>Too high number, will lead to scheduling overhead and many small tasks, increasing job time</li>\n<li>A good value is usually 2–4 × total executor cores in your cluster, but it depends on <strong>data size and job characteristics.</strong></li>\n</ul>\n<p><a href=\"https://drive.google.com/file/d/1Dz5x9OPOYFs0nczzfeR7QBNY_tbB11v8/view?usp=drive_link\">Notebook</a></p>\n<p>PySpark is a Python API that allows you to use the power of Apache Spark - fast &#x26; scalable big data processing system.</p>\n<ul>\n<li>you write Python code</li>\n<li>PySpark lets tat code run on many computers at once (a \"cluster\")</li>\n<li>analyze, process and transform huge datasets that would not fit on a single computer's memory</li>\n<li>distributed processing - distribute the data in multiple computers and splits up the work</li>\n<li>supports Pandas like code + SQL queries</li>\n</ul>\n<h1 id=\"optimizations\">Optimizations<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#optimizations\"></a></h1>\n<ul>\n<li>cache initialized data. Whenever you reuse a dataframe in a for loop </li>\n</ul>\n<pre class=\"language-python\"><code class=\"language-python\"> \n</code></pre>\n<h1 id=\"pyspark-1\">PySpark<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pyspark-1\"></a></h1>\n<p>Example spark configs:</p>\n<pre><code>spark.executor.memory: 28g\nspark.executor.memoryOverhead: 8g\nspark.executor.cores: 4\nspark.executor.instances: 256 # total number of executors, not needed if using dynamic allocation\nspark.dynamicAllocation.initialExecutors: 64\nspark.dynamicAllocation.minExecutors: 64\nspark.dynamicAllocation.maxExecutors: 256\nspark.driver.memory: 28g # DRIVER!\n</code></pre>\n<ul>\n<li>spark is a distributed computing framework that allows you to work with large datasets across a cluster of machines/computers</li>\n<li><strong>worker node</strong> is typically one physical or virtual computer in the cluster</li>\n<li><strong>driver node</strong> is the main process that coordinates the execution of tasks across the cluster. It is responsible for creating the SparkContext, which is the entry point to using Spark.</li>\n<li>spark executor is a <strong>process</strong> that runs on each worker node in the cluster and is responsible for executing tasks and managing resources. Like a separate Python interpreter that runs on the worker node.</li>\n<li>node can have multiple executors running on it.</li>\n<li>each executor has its own memory and CPU resources allocated to it, which are used to execute tasks in <strong>parallel</strong>.</li>\n<li><strong>executor.cores</strong>: each executor can have multiple cores (i.e threads) that can execute tasks <strong>concurrently</strong>.</li>\n<li><strong>executor instances (spark.executor.instances)</strong>: total number of executors (processes) launched on all worker nodes in the clust</li>\n<li>If using dynamic allocation, leave spark.executor.instances unset or set min/max via:</li>\n</ul>\n<pre><code>spark.dynamicAllocation.enabled=true\nspark.dynamicAllocation.minExecutors\nspark.dynamicAllocation.maxExecutors\n</code></pre>\n<p><strong>Why to choose dynamic allocation?</strong></p>\n<ul>\n<li>Variable Input Size</li>\n<li>Fluctuating Resource Needs - joins, groupby-s, aggregating and exploding data</li>\n<li>shared, multi-user cluster : release executors when idle so others can use those resources</li>\n<li>cost optimization, not paying for idle resources</li>\n<li>long-running applications, your job scales up and down depending on activity</li>\n</ul>\n<p><strong>When NOT to Use Dynamic Allocation</strong></p>\n<ul>\n<li>Your resource needs are steady and predictable (and you’re on a dedicated cluster).</li>\n<li>Very short jobs or jobs with very short \"bursts\" of high demand (executor startup delays can hurt performance).</li>\n<li>You depend on RDD caching across all executors (since dynamic allocation can kill executors, losing cached data).</li>\n</ul>\n<p>Driver and Workers:</p>\n<p><img src=\"/dendron-wiki/./assets/images/spark_driver_worker.png\" alt=\"alt text\"></p>\n<h1 id=\"example-optimization\">Example optimization<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#example-optimization\"></a></h1>\n<p>Hey! After some experiment runs I've updated and chose these spark parameters</p>\n<pre><code>    spark.executor.memory: 28g\n    spark.executor.memoryOverhead: 3g\n    spark.executor.cores: 4\n    spark.dynamicAllocation.initialExecutors: 8\n    spark.dynamicAllocation.minExecutors: 8\n    spark.dynamicAllocation.maxExecutors: 32\n    spark.driver.memory: 28g\n</code></pre>\n<p>Run log</p>\n<p>Run time 360 seconds, cost 1.41$ (previously it was 7$)</p>\n<p>CPU average usage by the whole Pod is~30-40%. I won't decrease more the amount of executors since 8 to 32 dynamically allocated is already low compared to other jobs in this service.</p>\n<p>Memory average usage by the whole Pod is 50-65% - this is ok since we want some room left</p>\n<p>I use dynamic number of executors instead of fixed since CAPI has variable size input (as we onboard/churn clients num conversions can change a lot, matches data also depends a lot on the quality of the conversion client send to us which can vary over time). Also there are a few joins/groupbys in the DQS job that require variate amount of resources</p>\n<p>Spark recommends memoryOverhead of executor to be about 6-10% of the container size = spark.executor.memory + spark.executor.memoryOverhead. I chose ~10% to be on the safe size</p>\n<h1 id=\"spark-monitoring\">Spark Monitoring<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#spark-monitoring\"></a></h1>","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1763992060664,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"dc095b340c158394d643b84f8585ff0c","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","lw1b4r6ykimvj9208nkgzps"],"parent":null,"data":{},"body":"\nZdr bebce kp ;)\n\nThis is my knowledge base, additionally I keep [daily journal](https://docs.google.com/document/d/1m8Npu0-t8RweyKiHCjLL1PPDYzbebqm3OZDHH8IVsb8/edit?tab=t.rj0kvkrm7zpr)."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}