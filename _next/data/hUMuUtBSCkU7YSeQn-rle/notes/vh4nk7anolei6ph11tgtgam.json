{"pageProps":{"note":{"id":"vh4nk7anolei6ph11tgtgam","title":"2025-04-15","desc":"","updated":1744836760355,"created":1744712302323,"traitIds":["journalNote"],"custom":{},"fname":"daily.journal.2025.04.15","type":"note","vault":{"fsPath":"vault"},"contentHash":"b50dcdf1606ee3ad2aac88c6660fb727","links":[{"type":"wiki","from":{"fname":"daily.journal.2025.04.15","id":"vh4nk7anolei6ph11tgtgam","vaultName":"vault"},"value":"tags.AI","alias":"#AI","position":{"start":{"line":3,"column":1,"offset":28},"end":{"line":3,"column":4,"offset":31},"indent":[]},"xvault":false,"to":{"fname":"tags.AI"}}],"anchors":{"ai-industry-developments":{"type":"header","text":"AI Industry Developments","value":"ai-industry-developments","line":9,"column":0,"depth":1},"reasoning-models":{"type":"header","text":"Reasoning Models","value":"reasoning-models","line":22,"column":0,"depth":2}},"children":[],"parent":"qygeaemip9zraaj3bxufhxz","data":{}},"body":"<h1 id=\"2025-04-15\">2025-04-15<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#2025-04-15\"></a></h1>\n<h1 id=\"ai-industry-developments\">AI Industry Developments<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ai-industry-developments\"></a></h1>\n<p><a class=\"color-tag\" style=\"--tag-color: #fff4f2;\" href=\"/dendron-wiki/notes/jcbadkzec97t7sa62s5q49c\">#AI</a></p>\n<ul>\n<li>Most LLMs are really just a fancy autocomplete. Given a set of words (tokens)\nthey are able to predict what the next word most likely is. They use billion of parameters to do this.</li>\n<li>In 2022 LLM practitioners, realized that if they ask the LLM to show its thought process, and use it as prompts, LLMs\ncan do much better in reasoning tasks <strong>chain of thought prompting</strong>.</li>\n<li>Open AI o1 wrapped this idea into a model and this model showed much better reasoning capabilities. <strong>chain of thoughts (CoTs)</strong>.\nThis new “reasoning model” goes through extensive internal reasoning in order to decompose problems, brainstorm ideas, check its work, iterate, etc.  Think of it like an internal monologue.</li>\n<li>[<strong>IMPORTANT</strong>] Reasoning models are best when we have verifiable tasks. That is why they are good for coding since we can verify if a code is correct by writing tests.</li>\n<li>One day LLMs reasoning models will be good at solving math problems too. Solutions are currently verifiable only by human, but once we are able to automate this process then models trained with chain of thoughts idea + self-verification will be able to solve hard math problems.</li>\n</ul>\n<h2 id=\"reasoning-models\">Reasoning Models<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#reasoning-models\"></a></h2>\n<p>As of today best reasoning models are o3 (OpenAI), Sonnet 3.7 (Anthropic backed by Amazon)</p>\n<p>Deep Research </p>\n<p>Operators, Computer-Using Agent. This is fucking awesome. It opens the browser and starts looking at the screen, moves the mouse and clicks...</p>\n<p>AI Agent, Assistant, Bot</p>\n<ul>\n<li>Agents perform complex operations, can reason on their own, and proactively make decisions. It has a \"self\" and is able to \"think\". Examples: Operators and Deep Research Products</li>\n<li>Assistants assist human when working on tasks. Best example is the vanilla chatGPT, VsCode Co-pilot</li>\n<li>Bot perform simple repetitive tasks, web scrappers, autocomplete, dial-call operators, classic call center support automation</li>\n</ul>\n<p><a href=\"https://openai.com/index/learning-to-reason-with-llms/\">o1 release, CoT</a></p>","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1744237493819,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"ea936af1aea818f3052610daac103a63","links":[],"anchors":{},"children":["dgcsvcwea8scgdegrk9tfni","xig93vo47ou1bkr7s4w1wb2","ro9bbyftsutm88mxw6r16p5","ypia1zmywsklpicmcgrzlz1","w7iitlako61ppm27mym400a","6bx5plramu4hksomqc1n55z","4nki3bedlvs3mnkixj3a07k","9r5ym61dkwap92fvbte1jkq","849u97nrsuyekmr4a1r92ux","2b5bwf46z6v132wu7xghvrp","dygp5h2vzw4mkromwmofynb","7h50s7ga5ziiyblmoctsqmw","pglaolxcge4xfvgoph3je89","uy9u1co5ih1fokind8tg0eq","jc23ggp8iiu92kpnzo721to","f1u2a47guuw70olv36bzf66","c1bs7wsjfbhb0zipaywqv1","2av385tcj2cbumxprsauff3","v77wdzobzcackzimz6a7crv"],"parent":null,"data":{},"body":"\nWelcome to my Knowledge Base! Here I write about my perception of life, document exciting things I've learned, debate (with myself) on controversial topics. If you know me you will not be surprised to find out that I write mostly about engineering and maths. Other topics I'm interested in are economics, politics, business, chess and poker."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/dendron-wiki","siteUrl":"https://ngocuong0105.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","ga":{"tracking":"G-W5DRRLQ1N7"},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}